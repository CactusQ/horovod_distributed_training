2021-12-03 20:49:10.770902: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 20:49:10.771005: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-12-03 20:49:10.792758: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 20:49:10.792941: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-12-03 20:49:14.742606: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 20:49:14.742722: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2021-12-03 20:49:14.742789: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (node-0005): /proc/driver/nvidia/version does not exist
2021-12-03 20:49:14.795142: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 20:49:14.795348: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2021-12-03 20:49:14.795421: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (node-0001): /proc/driver/nvidia/version does not exist
===========
rank: 0
Batch size: 16
Total number of batches: 4000
Total training examples: 64000
Num of workers: 2
===========
Step #0 (total examples = 0)	Loss: 2.333162
Step #10 (total examples = 160)	Loss: 0.745669
Step #20 (total examples = 320)	Loss: 0.387779
Step #30 (total examples = 480)	Loss: 0.950872
Step #40 (total examples = 640)	Loss: 0.832500
Step #50 (total examples = 800)	Loss: 0.295994
Step #60 (total examples = 960)	Loss: 0.156027
Step #70 (total examples = 1120)	Loss: 0.731732
Step #80 (total examples = 1280)	Loss: 0.328213
Step #90 (total examples = 1440)	Loss: 0.065515
Step #100 (total examples = 1600)	Loss: 0.107411
Step #110 (total examples = 1760)	Loss: 0.258793
Step #120 (total examples = 1920)	Loss: 0.349182
Step #130 (total examples = 2080)	Loss: 0.086481
Step #140 (total examples = 2240)	Loss: 0.506567
Step #150 (total examples = 2400)	Loss: 0.119837
Step #160 (total examples = 2560)	Loss: 0.106994
Step #170 (total examples = 2720)	Loss: 0.049533
Step #180 (total examples = 2880)	Loss: 0.014199
Step #190 (total examples = 3040)	Loss: 0.119652
Step #200 (total examples = 3200)	Loss: 0.067774
Step #210 (total examples = 3360)	Loss: 0.100573
Step #220 (total examples = 3520)	Loss: 0.135952
Step #230 (total examples = 3680)	Loss: 0.117674
Step #240 (total examples = 3840)	Loss: 0.024897
Step #250 (total examples = 4000)	Loss: 0.038644
Step #260 (total examples = 4160)	Loss: 0.042864
Step #270 (total examples = 4320)	Loss: 0.040788
Step #280 (total examples = 4480)	Loss: 0.034618
Step #290 (total examples = 4640)	Loss: 0.128909
Step #300 (total examples = 4800)	Loss: 0.256603
Step #310 (total examples = 4960)	Loss: 0.254074
Step #320 (total examples = 5120)	Loss: 0.114439
Step #330 (total examples = 5280)	Loss: 0.181502
Step #340 (total examples = 5440)	Loss: 0.045474
Step #350 (total examples = 5600)	Loss: 0.295214
Step #360 (total examples = 5760)	Loss: 0.145365
Step #370 (total examples = 5920)	Loss: 0.485117
Step #380 (total examples = 6080)	Loss: 0.020476
Step #390 (total examples = 6240)	Loss: 0.503439
Step #400 (total examples = 6400)	Loss: 0.332943
Step #410 (total examples = 6560)	Loss: 0.053590
Step #420 (total examples = 6720)	Loss: 0.016809
Step #430 (total examples = 6880)	Loss: 0.338548
Step #440 (total examples = 7040)	Loss: 0.056441
Step #450 (total examples = 7200)	Loss: 0.078215
Step #460 (total examples = 7360)	Loss: 0.212602
Step #470 (total examples = 7520)	Loss: 0.239531
Step #480 (total examples = 7680)	Loss: 0.236324
Step #490 (total examples = 7840)	Loss: 0.084497
Step #500 (total examples = 8000)	Loss: 0.014151
Step #510 (total examples = 8160)	Loss: 0.043019
Step #520 (total examples = 8320)	Loss: 0.188036
Step #530 (total examples = 8480)	Loss: 0.081841
Step #540 (total examples = 8640)	Loss: 0.020145
Step #550 (total examples = 8800)	Loss: 0.060225
Step #560 (total examples = 8960)	Loss: 0.584409
Step #570 (total examples = 9120)	Loss: 0.244473
Step #580 (total examples = 9280)	Loss: 0.344859
Step #590 (total examples = 9440)	Loss: 0.039355
Step #600 (total examples = 9600)	Loss: 0.030200
Step #610 (total examples = 9760)	Loss: 0.159753
Step #620 (total examples = 9920)	Loss: 0.012624
Step #630 (total examples = 10080)	Loss: 0.246766
Step #640 (total examples = 10240)	Loss: 0.030607
Step #650 (total examples = 10400)	Loss: 0.086799
Step #660 (total examples = 10560)	Loss: 0.045219
Step #670 (total examples = 10720)	Loss: 0.038161
Step #680 (total examples = 10880)	Loss: 0.029516
Step #690 (total examples = 11040)	Loss: 0.084663
Step #700 (total examples = 11200)	Loss: 0.042762
Step #710 (total examples = 11360)	Loss: 0.201944
Step #720 (total examples = 11520)	Loss: 0.053186
Step #730 (total examples = 11680)	Loss: 0.517501
Step #740 (total examples = 11840)	Loss: 0.013803
Step #750 (total examples = 12000)	Loss: 0.001598
Step #760 (total examples = 12160)	Loss: 0.144202
Step #770 (total examples = 12320)	Loss: 0.133486
Step #780 (total examples = 12480)	Loss: 0.030724
Step #790 (total examples = 12640)	Loss: 0.028715
Step #800 (total examples = 12800)	Loss: 0.008060
Step #810 (total examples = 12960)	Loss: 0.243932
Step #820 (total examples = 13120)	Loss: 0.181153
Step #830 (total examples = 13280)	Loss: 0.166076
Step #840 (total examples = 13440)	Loss: 0.014789
Step #850 (total examples = 13600)	Loss: 0.030261
Step #860 (total examples = 13760)	Loss: 0.161713
Step #870 (total examples = 13920)	Loss: 0.304229
Step #880 (total examples = 14080)	Loss: 0.011578
Step #890 (total examples = 14240)	Loss: 0.064032
Step #900 (total examples = 14400)	Loss: 0.233645
Step #910 (total examples = 14560)	Loss: 0.076718
Step #920 (total examples = 14720)	Loss: 0.040598
Step #930 (total examples = 14880)	Loss: 0.031199
Step #940 (total examples = 15040)	Loss: 0.013419
Step #950 (total examples = 15200)	Loss: 0.036168
Step #960 (total examples = 15360)	Loss: 0.112850
Step #970 (total examples = 15520)	Loss: 0.009023
Step #980 (total examples = 15680)	Loss: 0.032789
Step #990 (total examples = 15840)	Loss: 0.214919
Step #1000 (total examples = 16000)	Loss: 0.222538
Step #1010 (total examples = 16160)	Loss: 0.002418
Step #1020 (total examples = 16320)	Loss: 0.156216
Step #1030 (total examples = 16480)	Loss: 0.008826
Step #1040 (total examples = 16640)	Loss: 0.070177
Step #1050 (total examples = 16800)	Loss: 0.010872
Step #1060 (total examples = 16960)	Loss: 0.007010
Step #1070 (total examples = 17120)	Loss: 0.011610
Step #1080 (total examples = 17280)	Loss: 0.050464
Step #1090 (total examples = 17440)	Loss: 0.017980
Step #1100 (total examples = 17600)	Loss: 0.001720
Step #1110 (total examples = 17760)	Loss: 0.058851
Step #1120 (total examples = 17920)	Loss: 0.062865
Step #1130 (total examples = 18080)	Loss: 0.018550
Step #1140 (total examples = 18240)	Loss: 0.078077
Step #1150 (total examples = 18400)	Loss: 0.243273
Step #1160 (total examples = 18560)	Loss: 0.050528
Step #1170 (total examples = 18720)	Loss: 0.157605
Step #1180 (total examples = 18880)	Loss: 0.044461
Step #1190 (total examples = 19040)	Loss: 0.008821
Step #1200 (total examples = 19200)	Loss: 0.007445
Step #1210 (total examples = 19360)	Loss: 0.038735
Step #1220 (total examples = 19520)	Loss: 0.038817
Step #1230 (total examples = 19680)	Loss: 0.005303
Step #1240 (total examples = 19840)	Loss: 0.043144
Step #1250 (total examples = 20000)	Loss: 0.024815
Step #1260 (total examples = 20160)	Loss: 0.014854
Step #1270 (total examples = 20320)	Loss: 0.002842
Step #1280 (total examples = 20480)	Loss: 0.146159
Step #1290 (total examples = 20640)	Loss: 0.003614
Step #1300 (total examples = 20800)	Loss: 0.131745
Step #1310 (total examples = 20960)	Loss: 0.130785
Step #1320 (total examples = 21120)	Loss: 0.184080
Step #1330 (total examples = 21280)	Loss: 0.011601
Step #1340 (total examples = 21440)	Loss: 0.011148
Step #1350 (total examples = 21600)	Loss: 0.170801
Step #1360 (total examples = 21760)	Loss: 0.119503
Step #1370 (total examples = 21920)	Loss: 0.159022
Step #1380 (total examples = 22080)	Loss: 0.022346
Step #1390 (total examples = 22240)	Loss: 0.118378
Step #1400 (total examples = 22400)	Loss: 0.063025
Step #1410 (total examples = 22560)	Loss: 0.654530
Step #1420 (total examples = 22720)	Loss: 0.040491
Step #1430 (total examples = 22880)	Loss: 0.017871
Step #1440 (total examples = 23040)	Loss: 0.016608
Step #1450 (total examples = 23200)	Loss: 0.021213
Step #1460 (total examples = 23360)	Loss: 0.024447
Step #1470 (total examples = 23520)	Loss: 0.100820
Step #1480 (total examples = 23680)	Loss: 0.131712
Step #1490 (total examples = 23840)	Loss: 0.149695
Step #1500 (total examples = 24000)	Loss: 0.261064
Step #1510 (total examples = 24160)	Loss: 0.215624
Step #1520 (total examples = 24320)	Loss: 0.038737
Step #1530 (total examples = 24480)	Loss: 0.223162
Step #1540 (total examples = 24640)	Loss: 0.197319
Step #1550 (total examples = 24800)	Loss: 0.438244
Step #1560 (total examples = 24960)	Loss: 0.079455
Step #1570 (total examples = 25120)	Loss: 0.283554
Step #1580 (total examples = 25280)	Loss: 0.054203
Step #1590 (total examples = 25440)	Loss: 0.091278
Step #1600 (total examples = 25600)	Loss: 0.724944
Step #1610 (total examples = 25760)	Loss: 0.012647===========
rank: 1
===========
Step #0 (total examples = 0)	Loss: 2.347862
Step #10 (total examples = 160)	Loss: 1.206288
Step #20 (total examples = 320)	Loss: 0.759801
Step #30 (total examples = 480)	Loss: 0.327197
Step #40 (total examples = 640)	Loss: 0.896163
Step #50 (total examples = 800)	Loss: 0.228266
Step #60 (total examples = 960)	Loss: 0.210480
Step #70 (total examples = 1120)	Loss: 0.288489
Step #80 (total examples = 1280)	Loss: 0.909495
Step #90 (total examples = 1440)	Loss: 0.132465
Step #100 (total examples = 1600)	Loss: 0.599741
Step #110 (total examples = 1760)	Loss: 0.279968
Step #120 (total examples = 1920)	Loss: 0.145950
Step #130 (total examples = 2080)	Loss: 0.217098
Step #140 (total examples = 2240)	Loss: 0.180828
Step #150 (total examples = 2400)	Loss: 0.268729
Step #160 (total examples = 2560)	Loss: 0.113457
Step #170 (total examples = 2720)	Loss: 0.206650
Step #180 (total examples = 2880)	Loss: 0.128507
Step #190 (total examples = 3040)	Loss: 0.035513
Step #200 (total examples = 3200)	Loss: 0.148305
Step #210 (total examples = 3360)	Loss: 0.104681
Step #220 (total examples = 3520)	Loss: 0.278466
Step #230 (total examples = 3680)	Loss: 0.141831
Step #240 (total examples = 3840)	Loss: 0.222520
Step #250 (total examples = 4000)	Loss: 0.434683
Step #260 (total examples = 4160)	Loss: 0.112998
Step #270 (total examples = 4320)	Loss: 0.058828
Step #280 (total examples = 4480)	Loss: 0.060926
Step #290 (total examples = 4640)	Loss: 0.057250
Step #300 (total examples = 4800)	Loss: 0.118181
Step #310 (total examples = 4960)	Loss: 0.250312
Step #320 (total examples = 5120)	Loss: 0.136850
Step #330 (total examples = 5280)	Loss: 0.059036
Step #340 (total examples = 5440)	Loss: 0.096416
Step #350 (total examples = 5600)	Loss: 0.277130
Step #360 (total examples = 5760)	Loss: 0.164712
Step #370 (total examples = 5920)	Loss: 0.412194
Step #380 (total examples = 6080)	Loss: 0.099135
Step #390 (total examples = 6240)	Loss: 0.330048
Step #400 (total examples = 6400)	Loss: 0.282764
Step #410 (total examples = 6560)	Loss: 0.017100
Step #420 (total examples = 6720)	Loss: 0.016793
Step #430 (total examples = 6880)	Loss: 0.123324
Step #440 (total examples = 7040)	Loss: 0.170038
Step #450 (total examples = 7200)	Loss: 0.348109
Step #460 (total examples = 7360)	Loss: 0.496049
Step #470 (total examples = 7520)	Loss: 0.052097
Step #480 (total examples = 7680)	Loss: 0.459101
Step #490 (total examples = 7840)	Loss: 0.151429
Step #500 (total examples = 8000)	Loss: 0.345992
Step #510 (total examples = 8160)	Loss: 0.005885
Step #520 (total examples = 8320)	Loss: 0.357295
Step #530 (total examples = 8480)	Loss: 0.040827
Step #540 (total examples = 8640)	Loss: 0.298094
Step #550 (total examples = 8800)	Loss: 0.705688
Step #560 (total examples = 8960)	Loss: 0.022425
Step #570 (total examples = 9120)	Loss: 0.060947
Step #580 (total examples = 9280)	Loss: 0.046166
Step #590 (total examples = 9440)	Loss: 0.179089
Step #600 (total examples = 9600)	Loss: 0.040439
Step #610 (total examples = 9760)	Loss: 0.073031
Step #620 (total examples = 9920)	Loss: 0.251330
Step #630 (total examples = 10080)	Loss: 0.026689
Step #640 (total examples = 10240)	Loss: 0.387182
Step #650 (total examples = 10400)	Loss: 0.078682
Step #660 (total examples = 10560)	Loss: 0.108054
Step #670 (total examples = 10720)	Loss: 0.123731
Step #680 (total examples = 10880)	Loss: 0.393236
Step #690 (total examples = 11040)	Loss: 0.028206
Step #700 (total examples = 11200)	Loss: 0.092939
Step #710 (total examples = 11360)	Loss: 0.049703
Step #720 (total examples = 11520)	Loss: 0.065839
Step #730 (total examples = 11680)	Loss: 0.031936
Step #740 (total examples = 11840)	Loss: 0.060004
Step #750 (total examples = 12000)	Loss: 0.015931
Step #760 (total examples = 12160)	Loss: 0.272533
Step #770 (total examples = 12320)	Loss: 0.436983
Step #780 (total examples = 12480)	Loss: 0.022335
Step #790 (total examples = 12640)	Loss: 0.122259
Step #800 (total examples = 12800)	Loss: 0.069609
Step #810 (total examples = 12960)	Loss: 0.009020
Step #820 (total examples = 13120)	Loss: 0.177254
Step #830 (total examples = 13280)	Loss: 0.167672
Step #840 (total examples = 13440)	Loss: 0.086295
Step #850 (total examples = 13600)	Loss: 0.037862
Step #860 (total examples = 13760)	Loss: 0.392691
Step #870 (total examples = 13920)	Loss: 0.057628
Step #880 (total examples = 14080)	Loss: 0.201020
Step #890 (total examples = 14240)	Loss: 0.021628
Step #900 (total examples = 14400)	Loss: 0.002247
Step #910 (total examples = 14560)	Loss: 0.018210
Step #920 (total examples = 14720)	Loss: 0.007446
Step #930 (total examples = 14880)	Loss: 0.098115
Step #940 (total examples = 15040)	Loss: 0.190791
Step #950 (total examples = 15200)	Loss: 0.011084
Step #960 (total examples = 15360)	Loss: 0.012966
Step #970 (total examples = 15520)	Loss: 0.237314
Step #980 (total examples = 15680)	Loss: 0.029199
Step #990 (total examples = 15840)	Loss: 0.039421
Step #1000 (total examples = 16000)	Loss: 0.017651
Step #1010 (total examples = 16160)	Loss: 0.055690
Step #1020 (total examples = 16320)	Loss: 0.059488
Step #1030 (total examples = 16480)	Loss: 0.083389
Step #1040 (total examples = 16640)	Loss: 0.008337
Step #1050 (total examples = 16800)	Loss: 0.244805
Step #1060 (total examples = 16960)	Loss: 0.012484
Step #1070 (total examples = 17120)	Loss: 0.478087
Step #1080 (total examples = 17280)	Loss: 0.027981
Step #1090 (total examples = 17440)	Loss: 0.004312
Step #1100 (total examples = 17600)	Loss: 0.020241
Step #1110 (total examples = 17760)	Loss: 0.052544
Step #1120 (total examples = 17920)	Loss: 0.110431
Step #1130 (total examples = 18080)	Loss: 0.113316
Step #1140 (total examples = 18240)	Loss: 0.105235
Step #1150 (total examples = 18400)	Loss: 0.112997
Step #1160 (total examples = 18560)	Loss: 0.112987
Step #1170 (total examples = 18720)	Loss: 0.116515
Step #1180 (total examples = 18880)	Loss: 0.024095
Step #1190 (total examples = 19040)	Loss: 0.024059
Step #1200 (total examples = 19200)	Loss: 0.006052
Step #1210 (total examples = 19360)	Loss: 0.059130
Step #1220 (total examples = 19520)	Loss: 0.562299
Step #1230 (total examples = 19680)	Loss: 0.100878
Step #1240 (total examples = 19840)	Loss: 0.067665
Step #1250 (total examples = 20000)	Loss: 0.087486
Step #1260 (total examples = 20160)	Loss: 0.074888
Step #1270 (total examples = 20320)	Loss: 0.077442
Step #1280 (total examples = 20480)	Loss: 0.005185
Step #1290 (total examples = 20640)	Loss: 0.301124
Step #1300 (total examples = 20800)	Loss: 0.045582
Step #1310 (total examples = 20960)	Loss: 0.005608
Step #1320 (total examples = 21120)	Loss: 0.001568
Step #1330 (total examples = 21280)	Loss: 0.067398
Step #1340 (total examples = 21440)	Loss: 0.081784
Step #1350 (total examples = 21600)	Loss: 0.012665
Step #1360 (total examples = 21760)	Loss: 0.098108
Step #1370 (total examples = 21920)	Loss: 0.055619
Step #1380 (total examples = 22080)	Loss: 0.116660
Step #1390 (total examples = 22240)	Loss: 0.032415
Step #1400 (total examples = 22400)	Loss: 0.020904
Step #1410 (total examples = 22560)	Loss: 0.002585
Step #1420 (total examples = 22720)	Loss: 0.001897
Step #1430 (total examples = 22880)	Loss: 0.032148
Step #1440 (total examples = 23040)	Loss: 0.020918
Step #1450 (total examples = 23200)	Loss: 0.081040
Step #1460 (total examples = 23360)	Loss: 0.172562
Step #1470 (total examples = 23520)	Loss: 0.007589
Step #1480 (total examples = 23680)	Loss: 0.003334
Step #1490 (total examples = 23840)	Loss: 0.006563
Step #1500 (total examples = 24000)	Loss: 0.013288
Step #1510 (total examples = 24160)	Loss: 0.013143
Step #1520 (total examples = 24320)	Loss: 0.038545
Step #1530 (total examples = 24480)	Loss: 0.012341
Step #1540 (total examples = 24640)	Loss: 0.000710
Step #1550 (total examples = 24800)	Loss: 0.011284
Step #1560 (total examples = 24960)	Loss: 0.007990
Step #1570 (total examples = 25120)	Loss: 0.219091
Step #1580 (total examples = 25280)	Loss: 0.050377
Step #1590 (total examples = 25440)	Loss: 0.074047
Step #1600 (total examples = 25600)	Loss: 0.004223
Step #1610 (total examples = 25760)	Loss: 0.004455
Step #1620 (total examples = 25920)	Loss: 0.003911
Step #1630 (total examples = 26080)	Loss: 0.006024
Step #1640 (total examples = 26240)	Loss: 0.023537
Step #1650 (total examples = 26400)	Loss: 0.006821
Step #1660 (total examples = 26560)	Loss: 0.004923
Step #1670 (total examples = 26720)	Loss: 0.095093
Step #1680 (total examples = 26880)	Loss: 0.015482
Step #1690 (total examples = 27040)	Loss: 0.035488
Step #1700 (total examples = 27200)	Loss: 0.005716
Step #1710 (total examples = 27360)	Loss: 0.040363
Step #1720 (total examples = 27520)	Loss: 0.035519
Step #1730 (total examples = 27680)	Loss: 0.014394
Step #1740 (total examples = 27840)	Loss: 0.000709
Step #1750 (total examples = 28000)	Loss: 0.003236
Step #1760 (total examples = 28160)	Loss: 0.152196
Step #1770 (total examples = 28320)	Loss: 0.104203
Step #1780 (total examples = 28480)	Loss: 0.013553
Step #1790 (total examples = 28640)	Loss: 0.047272
Step #1800 (total examples = 28800)	Loss: 0.561446
Step #1810 (total examples = 28960)	Loss: 0.003177
Step #1820 (total examples = 29120)	Loss: 0.005976
Step #1830 (total examples = 29280)	Loss: 0.001577
Step #1840 (total examples = 29440)	Loss: 0.004751
Step #1850 (total examples = 29600)	Loss: 0.000569
Step #1860 (total examples = 29760)	Loss: 0.005032
Step #1870 (total examples = 29920)	Loss: 0.006194
Step #1880 (total examples = 30080)	Loss: 0.015081
Step #1890 (total examples = 30240)	Loss: 0.007275
Step #1900 (total examples = 30400)	Loss: 0.011644
Step #1910 (total examples = 30560)	Loss: 0.003657
Step #1920 (total examples = 30720)	Loss: 0.050423
Step #1930 (total examples = 30880)	Loss: 0.081922
Step #1940 (total examples = 31040)	Loss: 0.048263
Step #1950 (total examples = 31200)	Loss: 0.031482
Step #1960 (total examples = 31360)	Loss: 0.428056
Step #1970 (total examples = 31520)	Loss: 0.003562
Step #1980 (total examples = 31680)	Loss: 0.269549
Step #1990 (total examples = 31840)	Loss: 0.000865

Step #1620 (total examples = 25920)	Loss: 0.243080
Step #1630 (total examples = 26080)	Loss: 0.010047
Step #1640 (total examples = 26240)	Loss: 0.069993
Step #1650 (total examples = 26400)	Loss: 0.001545
Step #1660 (total examples = 26560)	Loss: 0.138632
Step #1670 (total examples = 26720)	Loss: 0.097630
Step #1680 (total examples = 26880)	Loss: 0.009714
Step #1690 (total examples = 27040)	Loss: 0.121670
Step #1700 (total examples = 27200)	Loss: 0.004765
Step #1710 (total examples = 27360)	Loss: 0.121318
Step #1720 (total examples = 27520)	Loss: 0.026983
Step #1730 (total examples = 27680)	Loss: 0.020715
Step #1740 (total examples = 27840)	Loss: 0.145381
Step #1750 (total examples = 28000)	Loss: 0.027231
Step #1760 (total examples = 28160)	Loss: 0.053676
Step #1770 (total examples = 28320)	Loss: 0.013528
Step #1780 (total examples = 28480)	Loss: 0.004809
Step #1790 (total examples = 28640)	Loss: 0.004329
Step #1800 (total examples = 28800)	Loss: 0.019776
Step #1810 (total examples = 28960)	Loss: 0.081451
Step #1820 (total examples = 29120)	Loss: 0.064794
Step #1830 (total examples = 29280)	Loss: 0.035043
Step #1840 (total examples = 29440)	Loss: 0.000349
Step #1850 (total examples = 29600)	Loss: 0.003152
Step #1860 (total examples = 29760)	Loss: 0.005124
Step #1870 (total examples = 29920)	Loss: 0.003302
Step #1880 (total examples = 30080)	Loss: 0.257230
Step #1890 (total examples = 30240)	Loss: 0.064357
Step #1900 (total examples = 30400)	Loss: 0.125389
Step #1910 (total examples = 30560)	Loss: 0.005503
Step #1920 (total examples = 30720)	Loss: 0.053759
Step #1930 (total examples = 30880)	Loss: 0.006961
Step #1940 (total examples = 31040)	Loss: 0.062343
Step #1950 (total examples = 31200)	Loss: 0.010991
Step #1960 (total examples = 31360)	Loss: 0.419719
Step #1970 (total examples = 31520)	Loss: 0.156928
Step #1980 (total examples = 31680)	Loss: 0.033320
Step #1990 (total examples = 31840)	Loss: 0.427517
  1/313 [..............................] - ETA: 2:20 - loss: 13.5481 - accuracy: 0.9375  3/313 [..............................] - ETA: 11s - loss: 8.5372 - accuracy: 0.9688    5/313 [..............................] - ETA: 11s - loss: 6.8083 - accuracy: 0.9750  7/313 [..............................] - ETA: 11s - loss: 4.8631 - accuracy: 0.9821  8/313 [..............................] - ETA: 13s - loss: 4.2552 - accuracy: 0.9844  9/313 [..............................] - ETA: 14s - loss: 4.7920 - accuracy: 0.9792 10/313 [..............................] - ETA: 15s - loss: 4.3128 - accuracy: 0.9812 11/313 [>.............................] - ETA: 15s - loss: 9.1349 - accuracy: 0.9744 12/313 [>.............................] - ETA: 16s - loss: 8.3737 - accuracy: 0.9766 13/313 [>.............................] - ETA: 16s - loss: 7.7718 - accuracy: 0.9760 14/313 [>.............................] - ETA: 17s - loss: 7.2167 - accuracy: 0.9777 15/313 [>.............................] - ETA: 17s - loss: 8.2851 - accuracy: 0.9750 16/313 [>.............................] - ETA: 17s - loss: 7.7672 - accuracy: 0.9766 17/313 [>.............................] - ETA: 17s - loss: 7.3103 - accuracy: 0.9779 18/313 [>.............................] - ETA: 17s - loss: 6.9042 - accuracy: 0.9792 19/313 [>.............................] - ETA: 17s - loss: 7.2732 - accuracy: 0.9786 20/313 [>.............................] - ETA: 18s - loss: 7.5467 - accuracy: 0.9781 21/313 [=>............................] - ETA: 18s - loss: 8.2838 - accuracy: 0.9762 23/313 [=>............................] - ETA: 17s - loss: 9.4601 - accuracy: 0.9742 25/313 [=>............................] - ETA: 16s - loss: 9.7552 - accuracy: 0.9750 27/313 [=>............................] - ETA: 16s - loss: 9.2721 - accuracy: 0.9757 29/313 [=>............................] - ETA: 15s - loss: 9.1952 - accuracy: 0.9763 31/313 [=>............................] - ETA: 15s - loss: 9.2378 - accuracy: 0.9748 33/313 [==>...........................] - ETA: 14s - loss: 9.9461 - accuracy: 0.9725 35/313 [==>...........................] - ETA: 14s - loss: 10.2059 - accuracy: 0.9723 37/313 [==>...........................] - ETA: 13s - loss: 9.7271 - accuracy: 0.9730  39/313 [==>...........................] - ETA: 13s - loss: 11.9066 - accuracy: 0.9712 41/313 [==>...........................] - ETA: 13s - loss: 12.2709 - accuracy: 0.9703 43/313 [===>..........................] - ETA: 13s - loss: 11.7876 - accuracy: 0.9709 45/313 [===>..........................] - ETA: 12s - loss: 11.7761 - accuracy: 0.9708 47/313 [===>..........................] - ETA: 12s - loss: 11.2750 - accuracy: 0.9721 49/313 [===>..........................] - ETA: 12s - loss: 11.0548 - accuracy: 0.9719 51/313 [===>..........................] - ETA: 12s - loss: 10.6883 - accuracy: 0.9724 53/313 [====>.........................] - ETA: 12s - loss: 11.4732 - accuracy: 0.9723 55/313 [====>.........................] - ETA: 11s - loss: 12.6315 - accuracy: 0.9710 57/313 [====>.........................] - ETA: 11s - loss: 12.1883 - accuracy: 0.9720 59/313 [====>.........................] - ETA: 11s - loss: 12.2672 - accuracy: 0.9719 61/313 [====>.........................] - ETA: 11s - loss: 12.5385 - accuracy: 0.9718 63/313 [=====>........................] - ETA: 11s - loss: 12.1404 - accuracy: 0.9727 65/313 [=====>........................] - ETA: 11s - loss: 12.9996 - accuracy: 0.9721 67/313 [=====>........................] - ETA: 10s - loss: 16.5466 - accuracy: 0.9697 69/313 [=====>........................] - ETA: 10s - loss: 16.2177 - accuracy: 0.9701 71/313 [=====>........................] - ETA: 10s - loss: 16.1308 - accuracy: 0.9701 73/313 [=====>........................] - ETA: 10s - loss: 16.1854 - accuracy: 0.9700 75/313 [======>.......................] - ETA: 10s - loss: 16.5063 - accuracy: 0.9696 77/313 [======>.......................] - ETA: 10s - loss: 17.6620 - accuracy: 0.9692 79/313 [======>.......................] - ETA: 10s - loss: 17.4310 - accuracy: 0.9695 81/313 [======>.......................] - ETA: 10s - loss: 17.1131 - accuracy: 0.9699 83/313 [======>.......................] - ETA: 9s - loss: 18.0641 - accuracy: 0.9699  85/313 [=======>......................] - ETA: 9s - loss: 17.6391 - accuracy: 0.9706 87/313 [=======>......................] - ETA: 9s - loss: 17.8734 - accuracy: 0.9698 89/313 [=======>......................] - ETA: 9s - loss: 17.5021 - accuracy: 0.9702 91/313 [=======>......................] - ETA: 9s - loss: 17.7278 - accuracy: 0.9705 93/313 [=======>......................] - ETA: 9s - loss: 17.6048 - accuracy: 0.9701 95/313 [========>.....................] - ETA: 9s - loss: 18.3361 - accuracy: 0.9697 97/313 [========>.....................] - ETA: 9s - loss: 18.1165 - accuracy: 0.9700 99/313 [========>.....................] - ETA: 8s - loss: 17.7506 - accuracy: 0.9706101/313 [========>.....................] - ETA: 8s - loss: 17.4744 - accuracy: 0.9709103/313 [========>.....................] - ETA: 8s - loss: 17.1351 - accuracy: 0.9715105/313 [=========>....................] - ETA: 8s - loss: 16.8087 - accuracy: 0.9720107/313 [=========>....................] - ETA: 8s - loss: 16.6312 - accuracy: 0.9720109/313 [=========>....................] - ETA: 8s - loss: 16.3260 - accuracy: 0.9725111/313 [=========>....................] - ETA: 8s - loss: 16.9383 - accuracy: 0.9721113/313 [=========>....................] - ETA: 8s - loss: 17.2398 - accuracy: 0.9721115/313 [==========>...................] - ETA: 8s - loss: 16.9400 - accuracy: 0.9726117/313 [==========>...................] - ETA: 8s - loss: 16.6505 - accuracy: 0.9730119/313 [==========>...................] - ETA: 7s - loss: 16.9243 - accuracy: 0.9722121/313 [==========>...................] - ETA: 7s - loss: 17.2574 - accuracy: 0.9716123/313 [==========>...................] - ETA: 7s - loss: 17.1541 - accuracy: 0.9715125/313 [==========>...................] - ETA: 7s - loss: 17.1115 - accuracy: 0.9715127/313 [===========>..................] - ETA: 7s - loss: 16.8420 - accuracy: 0.9719129/313 [===========>..................] - ETA: 7s - loss: 16.7888 - accuracy: 0.9719131/313 [===========>..................] - ETA: 7s - loss: 16.8439 - accuracy: 0.9719133/313 [===========>..................] - ETA: 7s - loss: 17.1339 - accuracy: 0.9718135/313 [===========>..................] - ETA: 7s - loss: 17.1188 - accuracy: 0.9715137/313 [============>.................] - ETA: 7s - loss: 16.8958 - accuracy: 0.9717139/313 [============>.................] - ETA: 7s - loss: 16.6793 - accuracy: 0.9719141/313 [============>.................] - ETA: 6s - loss: 16.9161 - accuracy: 0.9716143/313 [============>.................] - ETA: 6s - loss: 16.9836 - accuracy: 0.9716145/313 [============>.................] - ETA: 6s - loss: 16.7584 - accuracy: 0.9718147/313 [=============>................] - ETA: 6s - loss: 16.6656 - accuracy: 0.9719149/313 [=============>................] - ETA: 6s - loss: 16.9331 - accuracy: 0.9719151/313 [=============>................] - ETA: 6s - loss: 17.0778 - accuracy: 0.9719153/313 [=============>................] - ETA: 6s - loss: 17.1194 - accuracy: 0.9712155/313 [=============>................] - ETA: 6s - loss: 17.0345 - accuracy: 0.9714157/313 [==============>...............] - ETA: 6s - loss: 16.8175 - accuracy: 0.9717159/313 [==============>...............] - ETA: 6s - loss: 16.6060 - accuracy: 0.9721161/313 [==============>...............] - ETA: 6s - loss: 16.3997 - accuracy: 0.9724163/313 [==============>...............] - ETA: 5s - loss: 16.1985 - accuracy: 0.9728165/313 [==============>...............] - ETA: 5s - loss: 16.0923 - accuracy: 0.9727167/313 [===============>..............] - ETA: 5s - loss: 15.8995 - accuracy: 0.9731169/313 [===============>..............] - ETA: 5s - loss: 15.7114 - accuracy: 0.9734171/313 [===============>..............] - ETA: 5s - loss: 15.5276 - accuracy: 0.9737173/313 [===============>..............] - ETA: 5s - loss: 15.3481 - accuracy: 0.9740175/313 [===============>..............] - ETA: 5s - loss: 15.2230 - accuracy: 0.9741177/313 [===============>..............] - ETA: 5s - loss: 15.0510 - accuracy: 0.9744179/313 [================>.............] - ETA: 5s - loss: 14.8829 - accuracy: 0.9747181/313 [================>.............] - ETA: 5s - loss: 14.7184 - accuracy: 0.9750183/313 [================>.............] - ETA: 5s - loss: 14.5575 - accuracy: 0.9752185/313 [================>.............] - ETA: 5s - loss: 14.4454 - accuracy: 0.9753187/313 [================>.............] - ETA: 4s - loss: 14.5809 - accuracy: 0.9751189/313 [=================>............] - ETA: 4s - loss: 14.5385 - accuracy: 0.9750191/313 [=================>............] - ETA: 4s - loss: 14.5677 - accuracy: 0.9750193/313 [=================>............] - ETA: 4s - loss: 14.5552 - accuracy: 0.9747195/313 [=================>............] - ETA: 4s - loss: 14.4060 - accuracy: 0.9750197/313 [=================>............] - ETA: 4s - loss: 14.2597 - accuracy: 0.9753199/313 [==================>...........] - ETA: 4s - loss: 14.1164 - accuracy: 0.9755201/313 [==================>...........] - ETA: 4s - loss: 13.9759 - accuracy: 0.9757203/313 [==================>...........] - ETA: 4s - loss: 13.8382 - accuracy: 0.9760205/313 [==================>...........] - ETA: 4s - loss: 13.8624 - accuracy: 0.9761207/313 [==================>...........] - ETA: 4s - loss: 13.8853 - accuracy: 0.9760209/313 [===================>..........] - ETA: 4s - loss: 13.8506 - accuracy: 0.9761211/313 [===================>..........] - ETA: 4s - loss: 13.7375 - accuracy: 0.9760213/313 [===================>..........] - ETA: 3s - loss: 13.6086 - accuracy: 0.9762215/313 [===================>..........] - ETA: 3s - loss: 13.5244 - accuracy: 0.9763217/313 [===================>..........] - ETA: 3s - loss: 13.3997 - accuracy: 0.9765219/313 [===================>..........] - ETA: 3s - loss: 13.2774 - accuracy: 0.9767221/313 [====================>.........] - ETA: 3s - loss: 13.1572 - accuracy: 0.9770223/313 [====================>.........] - ETA: 3s - loss: 13.0392 - accuracy: 0.9772225/313 [====================>.........] - ETA: 3s - loss: 12.9233 - accuracy: 0.9774227/313 [====================>.........] - ETA: 3s - loss: 12.8094 - accuracy: 0.9776229/313 [====================>.........] - ETA: 3s - loss: 12.6976 - accuracy: 0.9778231/313 [=====================>........] - ETA: 3s - loss: 12.5876 - accuracy: 0.9779233/313 [=====================>........] - ETA: 3s - loss: 12.5508 - accuracy: 0.9780235/313 [=====================>........] - ETA: 3s - loss: 12.4462 - accuracy: 0.9781237/313 [=====================>........] - ETA: 2s - loss: 12.3412 - accuracy: 0.9782239/313 [=====================>........] - ETA: 2s - loss: 12.2379 - accuracy: 0.9784241/313 [======================>.......] - ETA: 2s - loss: 12.1363 - accuracy: 0.9786243/313 [======================>.......] - ETA: 2s - loss: 12.1400 - accuracy: 0.9787245/313 [======================>.......] - ETA: 2s - loss: 12.0409 - accuracy: 0.9788247/313 [======================>.......] - ETA: 2s - loss: 12.0794 - accuracy: 0.9787249/313 [======================>.......] - ETA: 2s - loss: 12.0077 - accuracy: 0.9787251/313 [=======================>......] - ETA: 2s - loss: 11.9120 - accuracy: 0.9788253/313 [=======================>......] - ETA: 2s - loss: 11.9040 - accuracy: 0.9786255/313 [=======================>......] - ETA: 2s - loss: 11.8107 - accuracy: 0.9788257/313 [=======================>......] - ETA: 2s - loss: 11.7188 - accuracy: 0.9790259/313 [=======================>......] - ETA: 2s - loss: 11.6283 - accuracy: 0.9791261/313 [========================>.....] - ETA: 2s - loss: 11.5392 - accuracy: 0.9793263/313 [========================>.....] - ETA: 1s - loss: 11.5009 - accuracy: 0.9793265/313 [========================>.....] - ETA: 1s - loss: 11.4141 - accuracy: 0.9795267/313 [========================>.....] - ETA: 1s - loss: 11.3286 - accuracy: 0.9796269/313 [========================>.....] - ETA: 1s - loss: 11.2443 - accuracy: 0.9798271/313 [========================>.....] - ETA: 1s - loss: 11.1613 - accuracy: 0.9799273/313 [=========================>....] - ETA: 1s - loss: 11.0796 - accuracy: 0.9801275/313 [=========================>....] - ETA: 1s - loss: 10.9990 - accuracy: 0.9802277/313 [=========================>....] - ETA: 1s - loss: 10.9196 - accuracy: 0.9804279/313 [=========================>....] - ETA: 1s - loss: 10.8413 - accuracy: 0.9805281/313 [=========================>....] - ETA: 1s - loss: 10.7641 - accuracy: 0.9806283/313 [==========================>...] - ETA: 1s - loss: 11.0523 - accuracy: 0.9805285/313 [==========================>...] - ETA: 1s - loss: 10.9748 - accuracy: 0.9806287/313 [==========================>...] - ETA: 1s - loss: 10.8983 - accuracy: 0.9807289/313 [==========================>...] - ETA: 0s - loss: 10.8229 - accuracy: 0.9809291/313 [==========================>...] - ETA: 0s - loss: 10.7485 - accuracy: 0.9810293/313 [===========================>..] - ETA: 0s - loss: 10.6751 - accuracy: 0.9811295/313 [===========================>..] - ETA: 0s - loss: 10.6027 - accuracy: 0.9812297/313 [===========================>..] - ETA: 0s - loss: 10.5313 - accuracy: 0.9814299/313 [===========================>..] - ETA: 0s - loss: 10.5895 - accuracy: 0.9814301/313 [===========================>..] - ETA: 0s - loss: 10.5867 - accuracy: 0.9814303/313 [============================>.] - ETA: 0s - loss: 10.6313 - accuracy: 0.9813305/313 [============================>.] - ETA: 0s - loss: 10.7656 - accuracy: 0.9814307/313 [============================>.] - ETA: 0s - loss: 10.9898 - accuracy: 0.9813309/313 [============================>.] - ETA: 0s - loss: 10.9897 - accuracy: 0.9812311/313 [============================>.] - ETA: 0s - loss: 10.9598 - accuracy: 0.9811313/313 [==============================] - ETA: 0s - loss: 11.0074 - accuracy: 0.9811313/313 [==============================] - 12s 39ms/step - loss: 11.0074 - accuracy: 0.9811
Final model accuracy: 0.981100
Total training time: 236.026375
2021-12-03 20:53:25.271880: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 20:53:25.271987: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-12-03 20:53:25.296618: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 20:53:25.296801: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-12-03 20:53:29.291267: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 20:53:29.291372: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2021-12-03 20:53:29.291432: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (node-0005): /proc/driver/nvidia/version does not exist
2021-12-03 20:53:29.370627: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 20:53:29.370851: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2021-12-03 20:53:29.370928: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (node-0001): /proc/driver/nvidia/version does not exist
===========
rank: 0
Batch size: 16
Total number of batches: 4000
Total training examples: 64000
Num of workers: 2
===========
Step #0 (total examples = 0)	Loss: 2.290984
Step #10 (total examples = 160)	Loss: 0.928529
Step #20 (total examples = 320)	Loss: 0.676717
Step #30 (total examples = 480)	Loss: 0.626298
Step #40 (total examples = 640)	Loss: 0.620076
Step #50 (total examples = 800)	Loss: 0.272781
Step #60 (total examples = 960)	Loss: 0.226972
Step #70 (total examples = 1120)	Loss: 0.520234
Step #80 (total examples = 1280)	Loss: 0.451886
Step #90 (total examples = 1440)	Loss: 0.110931
Step #100 (total examples = 1600)	Loss: 0.087163
Step #110 (total examples = 1760)	Loss: 0.269286
Step #120 (total examples = 1920)	Loss: 0.249202
Step #130 (total examples = 2080)	Loss: 0.084512
Step #140 (total examples = 2240)	Loss: 0.354125
Step #150 (total examples = 2400)	Loss: 0.147606
Step #160 (total examples = 2560)	Loss: 0.441216
Step #170 (total examples = 2720)	Loss: 0.075848
Step #180 (total examples = 2880)	Loss: 0.009695
Step #190 (total examples = 3040)	Loss: 0.175803
Step #200 (total examples = 3200)	Loss: 0.028369
Step #210 (total examples = 3360)	Loss: 0.075714
Step #220 (total examples = 3520)	Loss: 0.240599
Step #230 (total examples = 3680)	Loss: 0.105695
Step #240 (total examples = 3840)	Loss: 0.075030
Step #250 (total examples = 4000)	Loss: 0.010031
Step #260 (total examples = 4160)	Loss: 0.062979
Step #270 (total examples = 4320)	Loss: 0.062155
Step #280 (total examples = 4480)	Loss: 0.042026
Step #290 (total examples = 4640)	Loss: 0.148962
Step #300 (total examples = 4800)	Loss: 0.153291
Step #310 (total examples = 4960)	Loss: 0.574996
Step #320 (total examples = 5120)	Loss: 0.380737
Step #330 (total examples = 5280)	Loss: 0.025698
Step #340 (total examples = 5440)	Loss: 0.033721
Step #350 (total examples = 5600)	Loss: 0.196002
Step #360 (total examples = 5760)	Loss: 0.029252
Step #370 (total examples = 5920)	Loss: 0.476792
Step #380 (total examples = 6080)	Loss: 0.031998
Step #390 (total examples = 6240)	Loss: 0.251364
Step #400 (total examples = 6400)	Loss: 0.162141
Step #410 (total examples = 6560)	Loss: 0.050744
Step #420 (total examples = 6720)	Loss: 0.025175
Step #430 (total examples = 6880)	Loss: 0.385833
Step #440 (total examples = 7040)	Loss: 0.349741
Step #450 (total examples = 7200)	Loss: 0.102335
Step #460 (total examples = 7360)	Loss: 0.058066
Step #470 (total examples = 7520)	Loss: 0.359141
Step #480 (total examples = 7680)	Loss: 0.244342
Step #490 (total examples = 7840)	Loss: 0.020565
Step #500 (total examples = 8000)	Loss: 0.080869
Step #510 (total examples = 8160)	Loss: 0.032524
Step #520 (total examples = 8320)	Loss: 0.038114
Step #530 (total examples = 8480)	Loss: 0.082562
Step #540 (total examples = 8640)	Loss: 0.318661
Step #550 (total examples = 8800)	Loss: 0.138766
Step #560 (total examples = 8960)	Loss: 0.624742
Step #570 (total examples = 9120)	Loss: 0.059884
Step #580 (total examples = 9280)	Loss: 0.008779
Step #590 (total examples = 9440)	Loss: 0.047145
Step #600 (total examples = 9600)	Loss: 0.034458
Step #610 (total examples = 9760)	Loss: 0.036711
Step #620 (total examples = 9920)	Loss: 0.014682
Step #630 (total examples = 10080)	Loss: 0.101572
Step #640 (total examples = 10240)	Loss: 0.176928
Step #650 (total examples = 10400)	Loss: 0.211139
Step #660 (total examples = 10560)	Loss: 0.055414
Step #670 (total examples = 10720)	Loss: 0.017746
Step #680 (total examples = 10880)	Loss: 0.043365
Step #690 (total examples = 11040)	Loss: 0.143094
Step #700 (total examples = 11200)	Loss: 0.052172
Step #710 (total examples = 11360)	Loss: 0.205911
Step #720 (total examples = 11520)	Loss: 0.067760
Step #730 (total examples = 11680)	Loss: 0.626851
Step #740 (total examples = 11840)	Loss: 0.025332
Step #750 (total examples = 12000)	Loss: 0.002474
Step #760 (total examples = 12160)	Loss: 0.025621
Step #770 (total examples = 12320)	Loss: 0.045668
Step #780 (total examples = 12480)	Loss: 0.042487
Step #790 (total examples = 12640)	Loss: 0.050332
Step #800 (total examples = 12800)	Loss: 0.000940
Step #810 (total examples = 12960)	Loss: 0.238932
Step #820 (total examples = 13120)	Loss: 0.113161
Step #830 (total examples = 13280)	Loss: 0.116694
Step #840 (total examples = 13440)	Loss: 0.005947
Step #850 (total examples = 13600)	Loss: 0.147119
Step #860 (total examples = 13760)	Loss: 0.195824
Step #870 (total examples = 13920)	Loss: 0.181719
Step #880 (total examples = 14080)	Loss: 0.005359
Step #890 (total examples = 14240)	Loss: 0.047171
Step #900 (total examples = 14400)	Loss: 0.208349
Step #910 (total examples = 14560)	Loss: 0.063511
Step #920 (total examples = 14720)	Loss: 0.057189
Step #930 (total examples = 14880)	Loss: 0.073762
Step #940 (total examples = 15040)	Loss: 0.005346
Step #950 (total examples = 15200)	Loss: 0.171763
Step #960 (total examples = 15360)	Loss: 0.016002
Step #970 (total examples = 15520)	Loss: 0.066268
Step #980 (total examples = 15680)	Loss: 0.037974
Step #990 (total examples = 15840)	Loss: 0.145158
Step #1000 (total examples = 16000)	Loss: 0.025604
Step #1010 (total examples = 16160)	Loss: 0.002666
Step #1020 (total examples = 16320)	Loss: 0.131449
Step #1030 (total examples = 16480)	Loss: 0.121288
Step #1040 (total examples = 16640)	Loss: 0.004863
Step #1050 (total examples = 16800)	Loss: 0.003109
Step #1060 (total examples = 16960)	Loss: 0.034598
Step #1070 (total examples = 17120)	Loss: 0.002503
Step #1080 (total examples = 17280)	Loss: 0.135561
Step #1090 (total examples = 17440)	Loss: 0.001293
Step #1100 (total examples = 17600)	Loss: 0.002936
Step #1110 (total examples = 17760)	Loss: 0.013367
Step #1120 (total examples = 17920)	Loss: 0.007929
Step #1130 (total examples = 18080)	Loss: 0.006771
Step #1140 (total examples = 18240)	Loss: 0.067282
Step #1150 (total examples = 18400)	Loss: 0.016783
Step #1160 (total examples = 18560)	Loss: 0.042462
Step #1170 (total examples = 18720)	Loss: 0.250956
Step #1180 (total examples = 18880)	Loss: 0.041143
Step #1190 (total examples = 19040)	Loss: 0.025823
Step #1200 (total examples = 19200)	Loss: 0.020209
Step #1210 (total examples = 19360)	Loss: 0.114281
Step #1220 (total examples = 19520)	Loss: 0.148349
Step #1230 (total examples = 19680)	Loss: 0.010643
Step #1240 (total examples = 19840)	Loss: 0.006033
Step #1250 (total examples = 20000)	Loss: 0.178747
Step #1260 (total examples = 20160)	Loss: 0.050207
Step #1270 (total examples = 20320)	Loss: 0.001531
Step #1280 (total examples = 20480)	Loss: 0.227029
Step #1290 (total examples = 20640)	Loss: 0.005156
Step #1300 (total examples = 20800)	Loss: 0.144246
Step #1310 (total examples = 20960)	Loss: 0.105240
Step #1320 (total examples = 21120)	Loss: 0.181126
Step #1330 (total examples = 21280)	Loss: 0.022893
Step #1340 (total examples = 21440)	Loss: 0.069572
Step #1350 (total examples = 21600)	Loss: 0.123895
Step #1360 (total examples = 21760)	Loss: 0.076313
Step #1370 (total examples = 21920)	Loss: 0.441442
Step #1380 (total examples = 22080)	Loss: 0.058832
Step #1390 (total examples = 22240)	Loss: 0.026880
Step #1400 (total examples = 22400)	Loss: 0.016467
Step #1410 (total examples = 22560)	Loss: 0.376091
Step #1420 (total examples = 22720)	Loss: 0.073673
Step #1430 (total examples = 22880)	Loss: 0.019204
Step #1440 (total examples = 23040)	Loss: 0.010276
Step #1450 (total examples = 23200)	Loss: 0.103672
Step #1460 (total examples = 23360)	Loss: 0.045462
Step #1470 (total examples = 23520)	Loss: 0.021191
Step #1480 (total examples = 23680)	Loss: 0.069690
Step #1490 (total examples = 23840)	Loss: 0.004380
Step #1500 (total examples = 24000)	Loss: 0.693208
Step #1510 (total examples = 24160)	Loss: 0.131499
Step #1520 (total examples = 24320)	Loss: 0.008283
Step #1530 (total examples = 24480)	Loss: 0.163416
Step #1540 (total examples = 24640)	Loss: 0.273092
Step #1550 (total examples = 24800)	Loss: 0.235167
Step #1560 (total examples = 24960)	Loss: 0.014657
Step #1570 (total examples = 25120)	Loss: 0.012831
Step #1580 (total examples = 25280)	Loss: 0.059325
Step #1590 (total examples = 25440)	Loss: 0.017503
Step #1600 (total examples = 25600)	Loss: 0.450754
Step #1610 (total examples = 25760)	Loss: 0.013016===========
rank: 1
===========
Step #0 (total examples = 0)	Loss: 2.342448
Step #10 (total examples = 160)	Loss: 1.314881
Step #20 (total examples = 320)	Loss: 1.487767
Step #30 (total examples = 480)	Loss: 0.474088
Step #40 (total examples = 640)	Loss: 0.844174
Step #50 (total examples = 800)	Loss: 0.130959
Step #60 (total examples = 960)	Loss: 0.308684
Step #70 (total examples = 1120)	Loss: 0.218928
Step #80 (total examples = 1280)	Loss: 1.567313
Step #90 (total examples = 1440)	Loss: 0.196017
Step #100 (total examples = 1600)	Loss: 0.204389
Step #110 (total examples = 1760)	Loss: 0.415351
Step #120 (total examples = 1920)	Loss: 0.104911
Step #130 (total examples = 2080)	Loss: 0.588254
Step #140 (total examples = 2240)	Loss: 0.130361
Step #150 (total examples = 2400)	Loss: 0.287180
Step #160 (total examples = 2560)	Loss: 0.082318
Step #170 (total examples = 2720)	Loss: 0.220514
Step #180 (total examples = 2880)	Loss: 0.156641
Step #190 (total examples = 3040)	Loss: 0.040991
Step #200 (total examples = 3200)	Loss: 0.322071
Step #210 (total examples = 3360)	Loss: 0.043720
Step #220 (total examples = 3520)	Loss: 0.227862
Step #230 (total examples = 3680)	Loss: 0.293609
Step #240 (total examples = 3840)	Loss: 0.081803
Step #250 (total examples = 4000)	Loss: 0.952354
Step #260 (total examples = 4160)	Loss: 0.088768
Step #270 (total examples = 4320)	Loss: 0.063214
Step #280 (total examples = 4480)	Loss: 0.097145
Step #290 (total examples = 4640)	Loss: 0.148862
Step #300 (total examples = 4800)	Loss: 0.216151
Step #310 (total examples = 4960)	Loss: 0.168810
Step #320 (total examples = 5120)	Loss: 0.026823
Step #330 (total examples = 5280)	Loss: 0.048596
Step #340 (total examples = 5440)	Loss: 0.065236
Step #350 (total examples = 5600)	Loss: 0.563547
Step #360 (total examples = 5760)	Loss: 0.038365
Step #370 (total examples = 5920)	Loss: 0.252722
Step #380 (total examples = 6080)	Loss: 0.173306
Step #390 (total examples = 6240)	Loss: 0.370404
Step #400 (total examples = 6400)	Loss: 0.161478
Step #410 (total examples = 6560)	Loss: 0.015289
Step #420 (total examples = 6720)	Loss: 0.106815
Step #430 (total examples = 6880)	Loss: 0.085579
Step #440 (total examples = 7040)	Loss: 0.062009
Step #450 (total examples = 7200)	Loss: 0.178834
Step #460 (total examples = 7360)	Loss: 0.305649
Step #470 (total examples = 7520)	Loss: 0.044441
Step #480 (total examples = 7680)	Loss: 0.256098
Step #490 (total examples = 7840)	Loss: 0.108721
Step #500 (total examples = 8000)	Loss: 0.284381
Step #510 (total examples = 8160)	Loss: 0.002274
Step #520 (total examples = 8320)	Loss: 0.300001
Step #530 (total examples = 8480)	Loss: 0.122420
Step #540 (total examples = 8640)	Loss: 0.064787
Step #550 (total examples = 8800)	Loss: 0.137531
Step #560 (total examples = 8960)	Loss: 0.030235
Step #570 (total examples = 9120)	Loss: 0.076296
Step #580 (total examples = 9280)	Loss: 0.012262
Step #590 (total examples = 9440)	Loss: 0.080829
Step #600 (total examples = 9600)	Loss: 0.034014
Step #610 (total examples = 9760)	Loss: 0.016509
Step #620 (total examples = 9920)	Loss: 0.227164
Step #630 (total examples = 10080)	Loss: 0.029476
Step #640 (total examples = 10240)	Loss: 0.144901
Step #650 (total examples = 10400)	Loss: 0.079824
Step #660 (total examples = 10560)	Loss: 0.423231
Step #670 (total examples = 10720)	Loss: 0.318275
Step #680 (total examples = 10880)	Loss: 0.182628
Step #690 (total examples = 11040)	Loss: 0.025459
Step #700 (total examples = 11200)	Loss: 0.074234
Step #710 (total examples = 11360)	Loss: 0.218734
Step #720 (total examples = 11520)	Loss: 0.256565
Step #730 (total examples = 11680)	Loss: 0.085990
Step #740 (total examples = 11840)	Loss: 0.079431
Step #750 (total examples = 12000)	Loss: 0.294477
Step #760 (total examples = 12160)	Loss: 0.045588
Step #770 (total examples = 12320)	Loss: 0.512498
Step #780 (total examples = 12480)	Loss: 0.028234
Step #790 (total examples = 12640)	Loss: 0.291395
Step #800 (total examples = 12800)	Loss: 0.037624
Step #810 (total examples = 12960)	Loss: 0.003486
Step #820 (total examples = 13120)	Loss: 0.076830
Step #830 (total examples = 13280)	Loss: 0.073719
Step #840 (total examples = 13440)	Loss: 0.290218
Step #850 (total examples = 13600)	Loss: 0.130190
Step #860 (total examples = 13760)	Loss: 0.031412
Step #870 (total examples = 13920)	Loss: 0.003842
Step #880 (total examples = 14080)	Loss: 0.032179
Step #890 (total examples = 14240)	Loss: 0.012975
Step #900 (total examples = 14400)	Loss: 0.010640
Step #910 (total examples = 14560)	Loss: 0.075008
Step #920 (total examples = 14720)	Loss: 0.145719
Step #930 (total examples = 14880)	Loss: 0.023131
Step #940 (total examples = 15040)	Loss: 0.093600
Step #950 (total examples = 15200)	Loss: 0.102047
Step #960 (total examples = 15360)	Loss: 0.005905
Step #970 (total examples = 15520)	Loss: 0.099683
Step #980 (total examples = 15680)	Loss: 0.377534
Step #990 (total examples = 15840)	Loss: 0.032678
Step #1000 (total examples = 16000)	Loss: 0.067907
Step #1010 (total examples = 16160)	Loss: 0.098870
Step #1020 (total examples = 16320)	Loss: 0.308351
Step #1030 (total examples = 16480)	Loss: 0.011279
Step #1040 (total examples = 16640)	Loss: 0.006060
Step #1050 (total examples = 16800)	Loss: 0.000888
Step #1060 (total examples = 16960)	Loss: 0.017692
Step #1070 (total examples = 17120)	Loss: 0.172632
Step #1080 (total examples = 17280)	Loss: 0.368952
Step #1090 (total examples = 17440)	Loss: 0.009318
Step #1100 (total examples = 17600)	Loss: 0.002537
Step #1110 (total examples = 17760)	Loss: 0.019966
Step #1120 (total examples = 17920)	Loss: 0.171592
Step #1130 (total examples = 18080)	Loss: 0.008667
Step #1140 (total examples = 18240)	Loss: 0.022460
Step #1150 (total examples = 18400)	Loss: 0.019093
Step #1160 (total examples = 18560)	Loss: 0.072125
Step #1170 (total examples = 18720)	Loss: 0.162594
Step #1180 (total examples = 18880)	Loss: 0.021061
Step #1190 (total examples = 19040)	Loss: 0.036015
Step #1200 (total examples = 19200)	Loss: 0.043885
Step #1210 (total examples = 19360)	Loss: 0.428367
Step #1220 (total examples = 19520)	Loss: 0.500150
Step #1230 (total examples = 19680)	Loss: 0.007683
Step #1240 (total examples = 19840)	Loss: 0.023161
Step #1250 (total examples = 20000)	Loss: 0.088629
Step #1260 (total examples = 20160)	Loss: 0.070929
Step #1270 (total examples = 20320)	Loss: 0.066066
Step #1280 (total examples = 20480)	Loss: 0.023712
Step #1290 (total examples = 20640)	Loss: 0.199721
Step #1300 (total examples = 20800)	Loss: 0.070514
Step #1310 (total examples = 20960)	Loss: 0.005928
Step #1320 (total examples = 21120)	Loss: 0.016779
Step #1330 (total examples = 21280)	Loss: 0.274686
Step #1340 (total examples = 21440)	Loss: 0.062266
Step #1350 (total examples = 21600)	Loss: 0.004372
Step #1360 (total examples = 21760)	Loss: 0.172267
Step #1370 (total examples = 21920)	Loss: 0.049133
Step #1380 (total examples = 22080)	Loss: 0.036574
Step #1390 (total examples = 22240)	Loss: 0.170249
Step #1400 (total examples = 22400)	Loss: 0.004354
Step #1410 (total examples = 22560)	Loss: 0.159492
Step #1420 (total examples = 22720)	Loss: 0.033687
Step #1430 (total examples = 22880)	Loss: 0.134049
Step #1440 (total examples = 23040)	Loss: 0.014964
Step #1450 (total examples = 23200)	Loss: 0.014619
Step #1460 (total examples = 23360)	Loss: 0.184356
Step #1470 (total examples = 23520)	Loss: 0.006159
Step #1480 (total examples = 23680)	Loss: 0.008220
Step #1490 (total examples = 23840)	Loss: 0.001958
Step #1500 (total examples = 24000)	Loss: 0.013317
Step #1510 (total examples = 24160)	Loss: 0.032973
Step #1520 (total examples = 24320)	Loss: 0.088206
Step #1530 (total examples = 24480)	Loss: 0.036895
Step #1540 (total examples = 24640)	Loss: 0.009099
Step #1550 (total examples = 24800)	Loss: 0.029293
Step #1560 (total examples = 24960)	Loss: 0.017846
Step #1570 (total examples = 25120)	Loss: 0.008308
Step #1580 (total examples = 25280)	Loss: 0.086808
Step #1590 (total examples = 25440)	Loss: 0.126691
Step #1600 (total examples = 25600)	Loss: 0.098152
Step #1610 (total examples = 25760)	Loss: 0.015083
Step #1620 (total examples = 25920)	Loss: 0.004475
Step #1630 (total examples = 26080)	Loss: 0.002481
Step #1640 (total examples = 26240)	Loss: 0.015975
Step #1650 (total examples = 26400)	Loss: 0.023808
Step #1660 (total examples = 26560)	Loss: 0.007614
Step #1670 (total examples = 26720)	Loss: 0.032303
Step #1680 (total examples = 26880)	Loss: 0.076096
Step #1690 (total examples = 27040)	Loss: 0.034311
Step #1700 (total examples = 27200)	Loss: 0.002906
Step #1710 (total examples = 27360)	Loss: 0.024098
Step #1720 (total examples = 27520)	Loss: 0.034482
Step #1730 (total examples = 27680)	Loss: 0.003262
Step #1740 (total examples = 27840)	Loss: 0.071229
Step #1750 (total examples = 28000)	Loss: 0.000431
Step #1760 (total examples = 28160)	Loss: 0.101441
Step #1770 (total examples = 28320)	Loss: 0.098597
Step #1780 (total examples = 28480)	Loss: 0.106347
Step #1790 (total examples = 28640)	Loss: 0.015941
Step #1800 (total examples = 28800)	Loss: 0.177595
Step #1810 (total examples = 28960)	Loss: 0.009963
Step #1820 (total examples = 29120)	Loss: 0.005429
Step #1830 (total examples = 29280)	Loss: 0.001153
Step #1840 (total examples = 29440)	Loss: 0.012154
Step #1850 (total examples = 29600)	Loss: 0.012477
Step #1860 (total examples = 29760)	Loss: 0.162842
Step #1870 (total examples = 29920)	Loss: 0.000980
Step #1880 (total examples = 30080)	Loss: 0.001345
Step #1890 (total examples = 30240)	Loss: 0.000537
Step #1900 (total examples = 30400)	Loss: 0.003713
Step #1910 (total examples = 30560)	Loss: 0.004757
Step #1920 (total examples = 30720)	Loss: 0.073873
Step #1930 (total examples = 30880)	Loss: 0.089158
Step #1940 (total examples = 31040)	Loss: 0.015619
Step #1950 (total examples = 31200)	Loss: 0.002616
Step #1960 (total examples = 31360)	Loss: 0.480638
Step #1970 (total examples = 31520)	Loss: 0.017393
Step #1980 (total examples = 31680)	Loss: 0.310807
Step #1990 (total examples = 31840)	Loss: 0.011018

Step #1620 (total examples = 25920)	Loss: 0.223487
Step #1630 (total examples = 26080)	Loss: 0.197935
Step #1640 (total examples = 26240)	Loss: 0.028543
Step #1650 (total examples = 26400)	Loss: 0.004794
Step #1660 (total examples = 26560)	Loss: 0.038320
Step #1670 (total examples = 26720)	Loss: 0.023177
Step #1680 (total examples = 26880)	Loss: 0.003931
Step #1690 (total examples = 27040)	Loss: 0.014069
Step #1700 (total examples = 27200)	Loss: 0.034468
Step #1710 (total examples = 27360)	Loss: 0.008743
Step #1720 (total examples = 27520)	Loss: 0.091249
Step #1730 (total examples = 27680)	Loss: 0.011012
Step #1740 (total examples = 27840)	Loss: 0.040594
Step #1750 (total examples = 28000)	Loss: 0.017260
Step #1760 (total examples = 28160)	Loss: 0.023528
Step #1770 (total examples = 28320)	Loss: 0.009372
Step #1780 (total examples = 28480)	Loss: 0.002025
Step #1790 (total examples = 28640)	Loss: 0.008258
Step #1800 (total examples = 28800)	Loss: 0.050730
Step #1810 (total examples = 28960)	Loss: 0.117939
Step #1820 (total examples = 29120)	Loss: 0.023153
Step #1830 (total examples = 29280)	Loss: 0.017087
Step #1840 (total examples = 29440)	Loss: 0.018566
Step #1850 (total examples = 29600)	Loss: 0.261763
Step #1860 (total examples = 29760)	Loss: 0.001339
Step #1870 (total examples = 29920)	Loss: 0.003537
Step #1880 (total examples = 30080)	Loss: 0.084730
Step #1890 (total examples = 30240)	Loss: 0.006377
Step #1900 (total examples = 30400)	Loss: 0.144241
Step #1910 (total examples = 30560)	Loss: 0.080663
Step #1920 (total examples = 30720)	Loss: 0.059555
Step #1930 (total examples = 30880)	Loss: 0.055868
Step #1940 (total examples = 31040)	Loss: 0.007972
Step #1950 (total examples = 31200)	Loss: 0.001666
Step #1960 (total examples = 31360)	Loss: 0.793584
Step #1970 (total examples = 31520)	Loss: 0.063374
Step #1980 (total examples = 31680)	Loss: 0.157620
Step #1990 (total examples = 31840)	Loss: 0.129082
  1/313 [..............................] - ETA: 2:20 - loss: 32.4302 - accuracy: 0.9375  3/313 [..............................] - ETA: 11s - loss: 10.8101 - accuracy: 0.9792   5/313 [..............................] - ETA: 11s - loss: 7.0607 - accuracy: 0.9812   7/313 [..............................] - ETA: 11s - loss: 5.0433 - accuracy: 0.9866  8/313 [..............................] - ETA: 13s - loss: 4.4129 - accuracy: 0.9883  9/313 [..............................] - ETA: 14s - loss: 6.0212 - accuracy: 0.9861 10/313 [..............................] - ETA: 14s - loss: 5.4191 - accuracy: 0.9875 11/313 [>.............................] - ETA: 15s - loss: 6.5355 - accuracy: 0.9801 12/313 [>.............................] - ETA: 16s - loss: 5.9908 - accuracy: 0.9818 13/313 [>.............................] - ETA: 16s - loss: 5.5300 - accuracy: 0.9832 14/313 [>.............................] - ETA: 16s - loss: 6.8892 - accuracy: 0.9821 15/313 [>.............................] - ETA: 17s - loss: 7.5335 - accuracy: 0.9812 16/313 [>.............................] - ETA: 17s - loss: 7.0626 - accuracy: 0.9824 17/313 [>.............................] - ETA: 17s - loss: 6.6472 - accuracy: 0.9835 18/313 [>.............................] - ETA: 17s - loss: 6.2779 - accuracy: 0.9844 19/313 [>.............................] - ETA: 18s - loss: 8.5528 - accuracy: 0.9819 20/313 [>.............................] - ETA: 18s - loss: 8.4368 - accuracy: 0.9812 21/313 [=>............................] - ETA: 18s - loss: 8.5745 - accuracy: 0.9807 23/313 [=>............................] - ETA: 17s - loss: 9.1117 - accuracy: 0.9783 25/313 [=>............................] - ETA: 16s - loss: 10.2573 - accuracy: 0.9775 27/313 [=>............................] - ETA: 16s - loss: 9.4975 - accuracy: 0.9792  29/313 [=>............................] - ETA: 15s - loss: 9.2857 - accuracy: 0.9795 31/313 [=>............................] - ETA: 15s - loss: 9.1990 - accuracy: 0.9788 33/313 [==>...........................] - ETA: 14s - loss: 9.3846 - accuracy: 0.9773 35/313 [==>...........................] - ETA: 14s - loss: 9.5266 - accuracy: 0.9777 37/313 [==>...........................] - ETA: 13s - loss: 9.0116 - accuracy: 0.9789 39/313 [==>...........................] - ETA: 13s - loss: 10.0857 - accuracy: 0.9776 41/313 [==>...........................] - ETA: 13s - loss: 9.9279 - accuracy: 0.9771  43/313 [===>..........................] - ETA: 13s - loss: 9.4661 - accuracy: 0.9782 45/313 [===>..........................] - ETA: 12s - loss: 9.1349 - accuracy: 0.9785 47/313 [===>..........................] - ETA: 12s - loss: 8.7462 - accuracy: 0.9794 49/313 [===>..........................] - ETA: 12s - loss: 9.1213 - accuracy: 0.9783 51/313 [===>..........................] - ETA: 12s - loss: 9.1181 - accuracy: 0.9786 53/313 [====>.........................] - ETA: 12s - loss: 9.4325 - accuracy: 0.9782 55/313 [====>.........................] - ETA: 11s - loss: 10.1815 - accuracy: 0.9773 57/313 [====>.........................] - ETA: 11s - loss: 10.0599 - accuracy: 0.9775 59/313 [====>.........................] - ETA: 11s - loss: 10.3400 - accuracy: 0.9772 61/313 [====>.........................] - ETA: 11s - loss: 10.8696 - accuracy: 0.9775 63/313 [=====>........................] - ETA: 11s - loss: 10.5245 - accuracy: 0.9782 65/313 [=====>........................] - ETA: 11s - loss: 11.8376 - accuracy: 0.9769 67/313 [=====>........................] - ETA: 10s - loss: 14.4950 - accuracy: 0.9748 69/313 [=====>........................] - ETA: 10s - loss: 14.4172 - accuracy: 0.9742 71/313 [=====>........................] - ETA: 10s - loss: 14.2032 - accuracy: 0.9745 73/313 [=====>........................] - ETA: 10s - loss: 14.0026 - accuracy: 0.9747 75/313 [======>.......................] - ETA: 10s - loss: 13.8230 - accuracy: 0.9746 77/313 [======>.......................] - ETA: 10s - loss: 14.6448 - accuracy: 0.9732 79/313 [======>.......................] - ETA: 10s - loss: 14.2741 - accuracy: 0.9739 81/313 [======>.......................] - ETA: 10s - loss: 13.9989 - accuracy: 0.9742 83/313 [======>.......................] - ETA: 9s - loss: 14.9176 - accuracy: 0.9740  85/313 [=======>......................] - ETA: 9s - loss: 14.6521 - accuracy: 0.9743 87/313 [=======>......................] - ETA: 9s - loss: 14.4545 - accuracy: 0.9741 89/313 [=======>......................] - ETA: 9s - loss: 14.1297 - accuracy: 0.9747 91/313 [=======>......................] - ETA: 9s - loss: 14.2046 - accuracy: 0.9749 93/313 [=======>......................] - ETA: 9s - loss: 14.1716 - accuracy: 0.9745 95/313 [========>.....................] - ETA: 9s - loss: 14.6771 - accuracy: 0.9740 97/313 [========>.....................] - ETA: 9s - loss: 14.3853 - accuracy: 0.9742 99/313 [========>.....................] - ETA: 8s - loss: 14.0947 - accuracy: 0.9747101/313 [========>.....................] - ETA: 8s - loss: 13.8156 - accuracy: 0.9752103/313 [========>.....................] - ETA: 8s - loss: 13.5473 - accuracy: 0.9757105/313 [=========>....................] - ETA: 8s - loss: 13.2893 - accuracy: 0.9762107/313 [=========>....................] - ETA: 8s - loss: 13.2546 - accuracy: 0.9761109/313 [=========>....................] - ETA: 8s - loss: 13.0114 - accuracy: 0.9765111/313 [=========>....................] - ETA: 8s - loss: 13.6088 - accuracy: 0.9761113/313 [=========>....................] - ETA: 8s - loss: 13.7219 - accuracy: 0.9762115/313 [==========>...................] - ETA: 8s - loss: 13.4832 - accuracy: 0.9766117/313 [==========>...................] - ETA: 8s - loss: 13.3574 - accuracy: 0.9768119/313 [==========>...................] - ETA: 7s - loss: 13.5060 - accuracy: 0.9761121/313 [==========>...................] - ETA: 7s - loss: 13.9543 - accuracy: 0.9755123/313 [==========>...................] - ETA: 7s - loss: 13.9330 - accuracy: 0.9756125/313 [==========>...................] - ETA: 7s - loss: 13.8286 - accuracy: 0.9755127/313 [===========>..................] - ETA: 7s - loss: 13.8325 - accuracy: 0.9754129/313 [===========>..................] - ETA: 7s - loss: 13.7421 - accuracy: 0.9753131/313 [===========>..................] - ETA: 7s - loss: 13.9146 - accuracy: 0.9754133/313 [===========>..................] - ETA: 7s - loss: 14.0212 - accuracy: 0.9751135/313 [===========>..................] - ETA: 7s - loss: 13.9664 - accuracy: 0.9752137/313 [============>.................] - ETA: 7s - loss: 13.8389 - accuracy: 0.9754139/313 [============>.................] - ETA: 7s - loss: 13.6397 - accuracy: 0.9757141/313 [============>.................] - ETA: 6s - loss: 13.6039 - accuracy: 0.9756143/313 [============>.................] - ETA: 6s - loss: 13.4586 - accuracy: 0.9757145/313 [============>.................] - ETA: 6s - loss: 13.2863 - accuracy: 0.9759147/313 [=============>................] - ETA: 6s - loss: 13.1056 - accuracy: 0.9762149/313 [=============>................] - ETA: 6s - loss: 13.3123 - accuracy: 0.9761151/313 [=============>................] - ETA: 6s - loss: 13.6666 - accuracy: 0.9758153/313 [=============>................] - ETA: 6s - loss: 13.6280 - accuracy: 0.9759155/313 [=============>................] - ETA: 6s - loss: 13.5005 - accuracy: 0.9760157/313 [==============>...............] - ETA: 6s - loss: 13.3285 - accuracy: 0.9763159/313 [==============>...............] - ETA: 6s - loss: 13.1608 - accuracy: 0.9766161/313 [==============>...............] - ETA: 6s - loss: 12.9974 - accuracy: 0.9769163/313 [==============>...............] - ETA: 6s - loss: 12.8379 - accuracy: 0.9772165/313 [==============>...............] - ETA: 5s - loss: 12.6861 - accuracy: 0.9773167/313 [===============>..............] - ETA: 5s - loss: 12.5342 - accuracy: 0.9775169/313 [===============>..............] - ETA: 5s - loss: 12.3858 - accuracy: 0.9778171/313 [===============>..............] - ETA: 5s - loss: 12.2410 - accuracy: 0.9781173/313 [===============>..............] - ETA: 5s - loss: 12.0995 - accuracy: 0.9783175/313 [===============>..............] - ETA: 5s - loss: 11.9612 - accuracy: 0.9786177/313 [===============>..............] - ETA: 5s - loss: 11.8260 - accuracy: 0.9788179/313 [================>.............] - ETA: 5s - loss: 11.6939 - accuracy: 0.9791181/313 [================>.............] - ETA: 5s - loss: 11.5647 - accuracy: 0.9793183/313 [================>.............] - ETA: 5s - loss: 11.4383 - accuracy: 0.9795185/313 [================>.............] - ETA: 5s - loss: 11.3591 - accuracy: 0.9796187/313 [================>.............] - ETA: 4s - loss: 11.8642 - accuracy: 0.9789189/313 [=================>............] - ETA: 4s - loss: 11.8477 - accuracy: 0.9788191/313 [=================>............] - ETA: 4s - loss: 11.8397 - accuracy: 0.9787193/313 [=================>............] - ETA: 4s - loss: 11.7171 - accuracy: 0.9790195/313 [=================>............] - ETA: 4s - loss: 11.5969 - accuracy: 0.9792197/313 [=================>............] - ETA: 4s - loss: 11.4791 - accuracy: 0.9794199/313 [==================>...........] - ETA: 4s - loss: 11.3638 - accuracy: 0.9796201/313 [==================>...........] - ETA: 4s - loss: 11.2507 - accuracy: 0.9798203/313 [==================>...........] - ETA: 4s - loss: 11.1399 - accuracy: 0.9800205/313 [==================>...........] - ETA: 4s - loss: 11.1705 - accuracy: 0.9800207/313 [==================>...........] - ETA: 4s - loss: 11.1927 - accuracy: 0.9799209/313 [===================>..........] - ETA: 4s - loss: 11.1168 - accuracy: 0.9800211/313 [===================>..........] - ETA: 4s - loss: 11.0115 - accuracy: 0.9802213/313 [===================>..........] - ETA: 3s - loss: 10.9306 - accuracy: 0.9802215/313 [===================>..........] - ETA: 3s - loss: 10.8289 - accuracy: 0.9804217/313 [===================>..........] - ETA: 3s - loss: 10.7291 - accuracy: 0.9806219/313 [===================>..........] - ETA: 3s - loss: 10.6311 - accuracy: 0.9807221/313 [====================>.........] - ETA: 3s - loss: 10.5349 - accuracy: 0.9809223/313 [====================>.........] - ETA: 3s - loss: 10.4404 - accuracy: 0.9811225/313 [====================>.........] - ETA: 3s - loss: 10.3476 - accuracy: 0.9812227/313 [====================>.........] - ETA: 3s - loss: 10.2583 - accuracy: 0.9813229/313 [====================>.........] - ETA: 3s - loss: 10.1688 - accuracy: 0.9814231/313 [=====================>........] - ETA: 3s - loss: 10.0807 - accuracy: 0.9816233/313 [=====================>........] - ETA: 3s - loss: 9.9942 - accuracy: 0.9818 235/313 [=====================>........] - ETA: 3s - loss: 9.9091 - accuracy: 0.9819237/313 [=====================>........] - ETA: 2s - loss: 9.8255 - accuracy: 0.9821239/313 [=====================>........] - ETA: 2s - loss: 9.7433 - accuracy: 0.9822241/313 [======================>.......] - ETA: 2s - loss: 9.6624 - accuracy: 0.9824243/313 [======================>.......] - ETA: 2s - loss: 9.5829 - accuracy: 0.9825245/313 [======================>.......] - ETA: 2s - loss: 9.5047 - accuracy: 0.9827247/313 [======================>.......] - ETA: 2s - loss: 9.5974 - accuracy: 0.9825249/313 [======================>.......] - ETA: 2s - loss: 9.5302 - accuracy: 0.9826251/313 [=======================>......] - ETA: 2s - loss: 9.4543 - accuracy: 0.9827253/313 [=======================>......] - ETA: 2s - loss: 9.5336 - accuracy: 0.9825255/313 [=======================>......] - ETA: 2s - loss: 9.4589 - accuracy: 0.9826257/313 [=======================>......] - ETA: 2s - loss: 9.3853 - accuracy: 0.9827259/313 [=======================>......] - ETA: 2s - loss: 9.3128 - accuracy: 0.9829261/313 [========================>.....] - ETA: 2s - loss: 9.2414 - accuracy: 0.9830263/313 [========================>.....] - ETA: 1s - loss: 9.1711 - accuracy: 0.9831265/313 [========================>.....] - ETA: 1s - loss: 9.1019 - accuracy: 0.9833267/313 [========================>.....] - ETA: 1s - loss: 9.0751 - accuracy: 0.9833269/313 [========================>.....] - ETA: 1s - loss: 9.0076 - accuracy: 0.9834271/313 [========================>.....] - ETA: 1s - loss: 8.9411 - accuracy: 0.9835273/313 [=========================>....] - ETA: 1s - loss: 8.8756 - accuracy: 0.9836275/313 [=========================>....] - ETA: 1s - loss: 8.8111 - accuracy: 0.9837277/313 [=========================>....] - ETA: 1s - loss: 8.7474 - accuracy: 0.9839279/313 [=========================>....] - ETA: 1s - loss: 8.6847 - accuracy: 0.9840281/313 [=========================>....] - ETA: 1s - loss: 8.6229 - accuracy: 0.9841283/313 [==========================>...] - ETA: 1s - loss: 8.7694 - accuracy: 0.9839285/313 [==========================>...] - ETA: 1s - loss: 8.7078 - accuracy: 0.9840287/313 [==========================>...] - ETA: 1s - loss: 8.6472 - accuracy: 0.9841289/313 [==========================>...] - ETA: 0s - loss: 8.5873 - accuracy: 0.9842291/313 [==========================>...] - ETA: 0s - loss: 8.5283 - accuracy: 0.9843293/313 [===========================>..] - ETA: 0s - loss: 8.4701 - accuracy: 0.9844295/313 [===========================>..] - ETA: 0s - loss: 8.4127 - accuracy: 0.9845297/313 [===========================>..] - ETA: 0s - loss: 8.3832 - accuracy: 0.9845299/313 [===========================>..] - ETA: 0s - loss: 8.4211 - accuracy: 0.9845301/313 [===========================>..] - ETA: 0s - loss: 8.4165 - accuracy: 0.9845303/313 [============================>.] - ETA: 0s - loss: 8.4552 - accuracy: 0.9844305/313 [============================>.] - ETA: 0s - loss: 8.5615 - accuracy: 0.9844307/313 [============================>.] - ETA: 0s - loss: 8.7886 - accuracy: 0.9842309/313 [============================>.] - ETA: 0s - loss: 8.7440 - accuracy: 0.9842311/313 [============================>.] - ETA: 0s - loss: 8.7366 - accuracy: 0.9841313/313 [==============================] - ETA: 0s - loss: 8.7810 - accuracy: 0.9841313/313 [==============================] - 12s 39ms/step - loss: 8.7810 - accuracy: 0.9841
Final model accuracy: 0.984100
Total training time: 236.514789
2021-12-03 20:57:40.492554: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 20:57:40.492747: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-12-03 20:57:40.502797: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 20:57:40.502894: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-12-03 20:57:44.351422: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 20:57:44.351588: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2021-12-03 20:57:44.351655: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (node-0005): /proc/driver/nvidia/version does not exist
2021-12-03 20:57:44.359657: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 20:57:44.359855: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2021-12-03 20:57:44.359926: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (node-0001): /proc/driver/nvidia/version does not exist
===========
rank: 0
Batch size: 16
Total number of batches: 4000
Total training examples: 64000
Num of workers: 2
===========
Step #0 (total examples = 0)	Loss: 2.316864
Step #10 (total examples = 160)	Loss: 1.103957
Step #20 (total examples = 320)	Loss: 0.563896
Step #30 (total examples = 480)	Loss: 0.702094
Step #40 (total examples = 640)	Loss: 0.584640
Step #50 (total examples = 800)	Loss: 0.408869
Step #60 (total examples = 960)	Loss: 0.125477
Step #70 (total examples = 1120)	Loss: 0.689004
Step #80 (total examples = 1280)	Loss: 0.276808
Step #90 (total examples = 1440)	Loss: 0.168253
Step #100 (total examples = 1600)	Loss: 0.192625
Step #110 (total examples = 1760)	Loss: 0.217654
Step #120 (total examples = 1920)	Loss: 0.374341
Step #130 (total examples = 2080)	Loss: 0.129807
Step #140 (total examples = 2240)	Loss: 0.364997
Step #150 (total examples = 2400)	Loss: 0.114219
Step #160 (total examples = 2560)	Loss: 0.339832
Step #170 (total examples = 2720)	Loss: 0.096574
Step #180 (total examples = 2880)	Loss: 0.015211
Step #190 (total examples = 3040)	Loss: 0.170111
Step #200 (total examples = 3200)	Loss: 0.058739
Step #210 (total examples = 3360)	Loss: 0.019366
Step #220 (total examples = 3520)	Loss: 0.068627
Step #230 (total examples = 3680)	Loss: 0.152227
Step #240 (total examples = 3840)	Loss: 0.043441
Step #250 (total examples = 4000)	Loss: 0.011740
Step #260 (total examples = 4160)	Loss: 0.194535
Step #270 (total examples = 4320)	Loss: 0.058439
Step #280 (total examples = 4480)	Loss: 0.042113
Step #290 (total examples = 4640)	Loss: 0.141349
Step #300 (total examples = 4800)	Loss: 0.358804
Step #310 (total examples = 4960)	Loss: 0.502379
Step #320 (total examples = 5120)	Loss: 0.191083
Step #330 (total examples = 5280)	Loss: 0.297479
Step #340 (total examples = 5440)	Loss: 0.124332
Step #350 (total examples = 5600)	Loss: 0.741360
Step #360 (total examples = 5760)	Loss: 0.019669
Step #370 (total examples = 5920)	Loss: 0.524082
Step #380 (total examples = 6080)	Loss: 0.014973
Step #390 (total examples = 6240)	Loss: 0.218237
Step #400 (total examples = 6400)	Loss: 0.278279
Step #410 (total examples = 6560)	Loss: 0.025362
Step #420 (total examples = 6720)	Loss: 0.007416
Step #430 (total examples = 6880)	Loss: 0.590195
Step #440 (total examples = 7040)	Loss: 0.096041
Step #450 (total examples = 7200)	Loss: 0.040488
Step #460 (total examples = 7360)	Loss: 0.050317
Step #470 (total examples = 7520)	Loss: 0.507458
Step #480 (total examples = 7680)	Loss: 0.268011
Step #490 (total examples = 7840)	Loss: 0.042273
Step #500 (total examples = 8000)	Loss: 0.041119
Step #510 (total examples = 8160)	Loss: 0.035298
Step #520 (total examples = 8320)	Loss: 0.094268
Step #530 (total examples = 8480)	Loss: 0.121660
Step #540 (total examples = 8640)	Loss: 0.266674
Step #550 (total examples = 8800)	Loss: 0.080808
Step #560 (total examples = 8960)	Loss: 0.279625
Step #570 (total examples = 9120)	Loss: 0.019690
Step #580 (total examples = 9280)	Loss: 0.112392
Step #590 (total examples = 9440)	Loss: 0.024084
Step #600 (total examples = 9600)	Loss: 0.026308
Step #610 (total examples = 9760)	Loss: 0.015587
Step #620 (total examples = 9920)	Loss: 0.044978
Step #630 (total examples = 10080)	Loss: 0.031425
Step #640 (total examples = 10240)	Loss: 0.002930
Step #650 (total examples = 10400)	Loss: 0.161023
Step #660 (total examples = 10560)	Loss: 0.058023
Step #670 (total examples = 10720)	Loss: 0.012614
Step #680 (total examples = 10880)	Loss: 0.013973
Step #690 (total examples = 11040)	Loss: 0.093453
Step #700 (total examples = 11200)	Loss: 0.004635
Step #710 (total examples = 11360)	Loss: 0.325184
Step #720 (total examples = 11520)	Loss: 0.043096
Step #730 (total examples = 11680)	Loss: 0.543466
Step #740 (total examples = 11840)	Loss: 0.009531
Step #750 (total examples = 12000)	Loss: 0.006359
Step #760 (total examples = 12160)	Loss: 0.043670
Step #770 (total examples = 12320)	Loss: 0.093281
Step #780 (total examples = 12480)	Loss: 0.024903
Step #790 (total examples = 12640)	Loss: 0.092170
Step #800 (total examples = 12800)	Loss: 0.015106
Step #810 (total examples = 12960)	Loss: 0.013819
Step #820 (total examples = 13120)	Loss: 0.053692
Step #830 (total examples = 13280)	Loss: 0.190968
Step #840 (total examples = 13440)	Loss: 0.006152
Step #850 (total examples = 13600)	Loss: 0.053548
Step #860 (total examples = 13760)	Loss: 0.112941
Step #870 (total examples = 13920)	Loss: 0.250980
Step #880 (total examples = 14080)	Loss: 0.012136
Step #890 (total examples = 14240)	Loss: 0.028166
Step #900 (total examples = 14400)	Loss: 0.080371
Step #910 (total examples = 14560)	Loss: 0.042767
Step #920 (total examples = 14720)	Loss: 0.038480
Step #930 (total examples = 14880)	Loss: 0.143216
Step #940 (total examples = 15040)	Loss: 0.019565
Step #950 (total examples = 15200)	Loss: 0.272201
Step #960 (total examples = 15360)	Loss: 0.037579
Step #970 (total examples = 15520)	Loss: 0.006370
Step #980 (total examples = 15680)	Loss: 0.073982
Step #990 (total examples = 15840)	Loss: 0.202942
Step #1000 (total examples = 16000)	Loss: 0.158081
Step #1010 (total examples = 16160)	Loss: 0.032156
Step #1020 (total examples = 16320)	Loss: 0.124612
Step #1030 (total examples = 16480)	Loss: 0.008061
Step #1040 (total examples = 16640)	Loss: 0.010175
Step #1050 (total examples = 16800)	Loss: 0.018819
Step #1060 (total examples = 16960)	Loss: 0.057333
Step #1070 (total examples = 17120)	Loss: 0.002284
Step #1080 (total examples = 17280)	Loss: 0.178711
Step #1090 (total examples = 17440)	Loss: 0.082443
Step #1100 (total examples = 17600)	Loss: 0.004001
Step #1110 (total examples = 17760)	Loss: 0.021093
Step #1120 (total examples = 17920)	Loss: 0.009848
Step #1130 (total examples = 18080)	Loss: 0.004455
Step #1140 (total examples = 18240)	Loss: 0.005835
Step #1150 (total examples = 18400)	Loss: 0.028777
Step #1160 (total examples = 18560)	Loss: 0.031873
Step #1170 (total examples = 18720)	Loss: 0.224308
Step #1180 (total examples = 18880)	Loss: 0.039055
Step #1190 (total examples = 19040)	Loss: 0.010473
Step #1200 (total examples = 19200)	Loss: 0.003588
Step #1210 (total examples = 19360)	Loss: 0.028711
Step #1220 (total examples = 19520)	Loss: 0.025158
Step #1230 (total examples = 19680)	Loss: 0.000870
Step #1240 (total examples = 19840)	Loss: 0.010599
Step #1250 (total examples = 20000)	Loss: 0.028914
Step #1260 (total examples = 20160)	Loss: 0.002137
Step #1270 (total examples = 20320)	Loss: 0.002839
Step #1280 (total examples = 20480)	Loss: 0.086392
Step #1290 (total examples = 20640)	Loss: 0.021394
Step #1300 (total examples = 20800)	Loss: 0.236473
Step #1310 (total examples = 20960)	Loss: 0.098390
Step #1320 (total examples = 21120)	Loss: 0.034949
Step #1330 (total examples = 21280)	Loss: 0.077263
Step #1340 (total examples = 21440)	Loss: 0.006388
Step #1350 (total examples = 21600)	Loss: 0.118969
Step #1360 (total examples = 21760)	Loss: 0.032200
Step #1370 (total examples = 21920)	Loss: 0.185453
Step #1380 (total examples = 22080)	Loss: 0.150997
Step #1390 (total examples = 22240)	Loss: 0.009235
Step #1400 (total examples = 22400)	Loss: 0.003883
Step #1410 (total examples = 22560)	Loss: 0.180259
Step #1420 (total examples = 22720)	Loss: 0.139088
Step #1430 (total examples = 22880)	Loss: 0.131737
Step #1440 (total examples = 23040)	Loss: 0.074407
Step #1450 (total examples = 23200)	Loss: 0.105539
Step #1460 (total examples = 23360)	Loss: 0.212715
Step #1470 (total examples = 23520)	Loss: 0.019569
Step #1480 (total examples = 23680)	Loss: 0.026463
Step #1490 (total examples = 23840)	Loss: 0.003616
Step #1500 (total examples = 24000)	Loss: 0.289609
Step #1510 (total examples = 24160)	Loss: 0.236300
Step #1520 (total examples = 24320)	Loss: 0.016739
Step #1530 (total examples = 24480)	Loss: 0.220822
Step #1540 (total examples = 24640)	Loss: 0.055214
Step #1550 (total examples = 24800)	Loss: 0.435907
Step #1560 (total examples = 24960)	Loss: 0.012921
Step #1570 (total examples = 25120)	Loss: 0.024427
Step #1580 (total examples = 25280)	Loss: 0.078107
Step #1590 (total examples = 25440)	Loss: 0.017696
Step #1600 (total examples = 25600)	Loss: 0.797638
Step #1610 (total examples = 25760)	Loss: 0.032409===========
rank: 1
===========
Step #0 (total examples = 0)	Loss: 2.286841
Step #10 (total examples = 160)	Loss: 1.446761
Step #20 (total examples = 320)	Loss: 1.161109
Step #30 (total examples = 480)	Loss: 0.519095
Step #40 (total examples = 640)	Loss: 1.382727
Step #50 (total examples = 800)	Loss: 0.187125
Step #60 (total examples = 960)	Loss: 0.578321
Step #70 (total examples = 1120)	Loss: 0.081392
Step #80 (total examples = 1280)	Loss: 1.118590
Step #90 (total examples = 1440)	Loss: 0.382549
Step #100 (total examples = 1600)	Loss: 0.272341
Step #110 (total examples = 1760)	Loss: 0.219610
Step #120 (total examples = 1920)	Loss: 0.268527
Step #130 (total examples = 2080)	Loss: 0.432145
Step #140 (total examples = 2240)	Loss: 0.138874
Step #150 (total examples = 2400)	Loss: 0.221438
Step #160 (total examples = 2560)	Loss: 0.152131
Step #170 (total examples = 2720)	Loss: 0.071275
Step #180 (total examples = 2880)	Loss: 0.066138
Step #190 (total examples = 3040)	Loss: 0.035990
Step #200 (total examples = 3200)	Loss: 0.276781
Step #210 (total examples = 3360)	Loss: 0.124068
Step #220 (total examples = 3520)	Loss: 0.278797
Step #230 (total examples = 3680)	Loss: 0.399139
Step #240 (total examples = 3840)	Loss: 0.158296
Step #250 (total examples = 4000)	Loss: 0.391353
Step #260 (total examples = 4160)	Loss: 0.032300
Step #270 (total examples = 4320)	Loss: 0.022231
Step #280 (total examples = 4480)	Loss: 0.056052
Step #290 (total examples = 4640)	Loss: 0.130255
Step #300 (total examples = 4800)	Loss: 0.089287
Step #310 (total examples = 4960)	Loss: 0.360109
Step #320 (total examples = 5120)	Loss: 0.080973
Step #330 (total examples = 5280)	Loss: 0.038322
Step #340 (total examples = 5440)	Loss: 0.016668
Step #350 (total examples = 5600)	Loss: 0.282441
Step #360 (total examples = 5760)	Loss: 0.030250
Step #370 (total examples = 5920)	Loss: 0.475891
Step #380 (total examples = 6080)	Loss: 0.354285
Step #390 (total examples = 6240)	Loss: 0.230794
Step #400 (total examples = 6400)	Loss: 0.251936
Step #410 (total examples = 6560)	Loss: 0.006552
Step #420 (total examples = 6720)	Loss: 0.165333
Step #430 (total examples = 6880)	Loss: 0.027212
Step #440 (total examples = 7040)	Loss: 0.012027
Step #450 (total examples = 7200)	Loss: 0.061959
Step #460 (total examples = 7360)	Loss: 0.302093
Step #470 (total examples = 7520)	Loss: 0.153669
Step #480 (total examples = 7680)	Loss: 0.731325
Step #490 (total examples = 7840)	Loss: 0.221203
Step #500 (total examples = 8000)	Loss: 0.193733
Step #510 (total examples = 8160)	Loss: 0.009020
Step #520 (total examples = 8320)	Loss: 0.104055
Step #530 (total examples = 8480)	Loss: 0.194282
Step #540 (total examples = 8640)	Loss: 0.038160
Step #550 (total examples = 8800)	Loss: 0.123333
Step #560 (total examples = 8960)	Loss: 0.050674
Step #570 (total examples = 9120)	Loss: 0.148403
Step #580 (total examples = 9280)	Loss: 0.044249
Step #590 (total examples = 9440)	Loss: 0.042885
Step #600 (total examples = 9600)	Loss: 0.021477
Step #610 (total examples = 9760)	Loss: 0.023518
Step #620 (total examples = 9920)	Loss: 0.335741
Step #630 (total examples = 10080)	Loss: 0.040754
Step #640 (total examples = 10240)	Loss: 0.174265
Step #650 (total examples = 10400)	Loss: 0.098881
Step #660 (total examples = 10560)	Loss: 0.071099
Step #670 (total examples = 10720)	Loss: 0.032784
Step #680 (total examples = 10880)	Loss: 0.113359
Step #690 (total examples = 11040)	Loss: 0.006599
Step #700 (total examples = 11200)	Loss: 0.136291
Step #710 (total examples = 11360)	Loss: 0.546816
Step #720 (total examples = 11520)	Loss: 0.384102
Step #730 (total examples = 11680)	Loss: 0.104169
Step #740 (total examples = 11840)	Loss: 0.032011
Step #750 (total examples = 12000)	Loss: 0.523299
Step #760 (total examples = 12160)	Loss: 0.187399
Step #770 (total examples = 12320)	Loss: 0.557292
Step #780 (total examples = 12480)	Loss: 0.017158
Step #790 (total examples = 12640)	Loss: 0.134334
Step #800 (total examples = 12800)	Loss: 0.123018
Step #810 (total examples = 12960)	Loss: 0.007000
Step #820 (total examples = 13120)	Loss: 0.181861
Step #830 (total examples = 13280)	Loss: 0.044288
Step #840 (total examples = 13440)	Loss: 0.057519
Step #850 (total examples = 13600)	Loss: 0.011389
Step #860 (total examples = 13760)	Loss: 0.082370
Step #870 (total examples = 13920)	Loss: 0.047566
Step #880 (total examples = 14080)	Loss: 0.047981
Step #890 (total examples = 14240)	Loss: 0.012259
Step #900 (total examples = 14400)	Loss: 0.021691
Step #910 (total examples = 14560)	Loss: 0.032526
Step #920 (total examples = 14720)	Loss: 0.042834
Step #930 (total examples = 14880)	Loss: 0.026826
Step #940 (total examples = 15040)	Loss: 0.086869
Step #950 (total examples = 15200)	Loss: 0.336428
Step #960 (total examples = 15360)	Loss: 0.015027
Step #970 (total examples = 15520)	Loss: 0.030731
Step #980 (total examples = 15680)	Loss: 0.279095
Step #990 (total examples = 15840)	Loss: 0.095897
Step #1000 (total examples = 16000)	Loss: 0.037209
Step #1010 (total examples = 16160)	Loss: 0.116191
Step #1020 (total examples = 16320)	Loss: 0.029487
Step #1030 (total examples = 16480)	Loss: 0.022530
Step #1040 (total examples = 16640)	Loss: 0.035269
Step #1050 (total examples = 16800)	Loss: 0.079354
Step #1060 (total examples = 16960)	Loss: 0.011212
Step #1070 (total examples = 17120)	Loss: 0.062399
Step #1080 (total examples = 17280)	Loss: 0.125513
Step #1090 (total examples = 17440)	Loss: 0.005104
Step #1100 (total examples = 17600)	Loss: 0.031762
Step #1110 (total examples = 17760)	Loss: 0.003947
Step #1120 (total examples = 17920)	Loss: 0.140491
Step #1130 (total examples = 18080)	Loss: 0.016877
Step #1140 (total examples = 18240)	Loss: 0.087651
Step #1150 (total examples = 18400)	Loss: 0.212584
Step #1160 (total examples = 18560)	Loss: 0.118189
Step #1170 (total examples = 18720)	Loss: 0.094872
Step #1180 (total examples = 18880)	Loss: 0.015547
Step #1190 (total examples = 19040)	Loss: 0.057429
Step #1200 (total examples = 19200)	Loss: 0.037558
Step #1210 (total examples = 19360)	Loss: 0.016431
Step #1220 (total examples = 19520)	Loss: 0.857531
Step #1230 (total examples = 19680)	Loss: 0.114144
Step #1240 (total examples = 19840)	Loss: 0.050378
Step #1250 (total examples = 20000)	Loss: 0.213249
Step #1260 (total examples = 20160)	Loss: 0.484085
Step #1270 (total examples = 20320)	Loss: 0.028763
Step #1280 (total examples = 20480)	Loss: 0.052876
Step #1290 (total examples = 20640)	Loss: 0.071905
Step #1300 (total examples = 20800)	Loss: 0.050155
Step #1310 (total examples = 20960)	Loss: 0.037799
Step #1320 (total examples = 21120)	Loss: 0.004084
Step #1330 (total examples = 21280)	Loss: 0.105283
Step #1340 (total examples = 21440)	Loss: 0.109936
Step #1350 (total examples = 21600)	Loss: 0.016532
Step #1360 (total examples = 21760)	Loss: 0.060715
Step #1370 (total examples = 21920)	Loss: 0.011777
Step #1380 (total examples = 22080)	Loss: 0.000506
Step #1390 (total examples = 22240)	Loss: 0.049852
Step #1400 (total examples = 22400)	Loss: 0.008301
Step #1410 (total examples = 22560)	Loss: 0.044916
Step #1420 (total examples = 22720)	Loss: 0.004556
Step #1430 (total examples = 22880)	Loss: 0.017610
Step #1440 (total examples = 23040)	Loss: 0.026262
Step #1450 (total examples = 23200)	Loss: 0.055470
Step #1460 (total examples = 23360)	Loss: 0.258799
Step #1470 (total examples = 23520)	Loss: 0.000890
Step #1480 (total examples = 23680)	Loss: 0.060625
Step #1490 (total examples = 23840)	Loss: 0.003923
Step #1500 (total examples = 24000)	Loss: 0.002340
Step #1510 (total examples = 24160)	Loss: 0.020238
Step #1520 (total examples = 24320)	Loss: 0.357117
Step #1530 (total examples = 24480)	Loss: 0.033524
Step #1540 (total examples = 24640)	Loss: 0.083103
Step #1550 (total examples = 24800)	Loss: 0.043332
Step #1560 (total examples = 24960)	Loss: 0.010848
Step #1570 (total examples = 25120)	Loss: 0.077282
Step #1580 (total examples = 25280)	Loss: 0.081750
Step #1590 (total examples = 25440)	Loss: 0.189708
Step #1600 (total examples = 25600)	Loss: 0.018818
Step #1610 (total examples = 25760)	Loss: 0.030984
Step #1620 (total examples = 25920)	Loss: 0.008904
Step #1630 (total examples = 26080)	Loss: 0.002345
Step #1640 (total examples = 26240)	Loss: 0.026829
Step #1650 (total examples = 26400)	Loss: 0.006272
Step #1660 (total examples = 26560)	Loss: 0.011112
Step #1670 (total examples = 26720)	Loss: 0.017814
Step #1680 (total examples = 26880)	Loss: 0.018217
Step #1690 (total examples = 27040)	Loss: 0.035969
Step #1700 (total examples = 27200)	Loss: 0.225778
Step #1710 (total examples = 27360)	Loss: 0.077063
Step #1720 (total examples = 27520)	Loss: 0.065669
Step #1730 (total examples = 27680)	Loss: 0.002500
Step #1740 (total examples = 27840)	Loss: 0.007202
Step #1750 (total examples = 28000)	Loss: 0.001474
Step #1760 (total examples = 28160)	Loss: 0.076492
Step #1770 (total examples = 28320)	Loss: 0.179283
Step #1780 (total examples = 28480)	Loss: 0.024967
Step #1790 (total examples = 28640)	Loss: 0.050013
Step #1800 (total examples = 28800)	Loss: 0.051712
Step #1810 (total examples = 28960)	Loss: 0.010648
Step #1820 (total examples = 29120)	Loss: 0.018236
Step #1830 (total examples = 29280)	Loss: 0.002840
Step #1840 (total examples = 29440)	Loss: 0.095155
Step #1850 (total examples = 29600)	Loss: 0.034538
Step #1860 (total examples = 29760)	Loss: 0.019502
Step #1870 (total examples = 29920)	Loss: 0.025599
Step #1880 (total examples = 30080)	Loss: 0.004157
Step #1890 (total examples = 30240)	Loss: 0.003895
Step #1900 (total examples = 30400)	Loss: 0.001006
Step #1910 (total examples = 30560)	Loss: 0.001652
Step #1920 (total examples = 30720)	Loss: 0.186329
Step #1930 (total examples = 30880)	Loss: 0.111699
Step #1940 (total examples = 31040)	Loss: 0.092785
Step #1950 (total examples = 31200)	Loss: 0.003356
Step #1960 (total examples = 31360)	Loss: 0.543301
Step #1970 (total examples = 31520)	Loss: 0.035507
Step #1980 (total examples = 31680)	Loss: 0.190372
Step #1990 (total examples = 31840)	Loss: 0.021330

Step #1620 (total examples = 25920)	Loss: 0.042869
Step #1630 (total examples = 26080)	Loss: 0.006056
Step #1640 (total examples = 26240)	Loss: 0.013487
Step #1650 (total examples = 26400)	Loss: 0.006892
Step #1660 (total examples = 26560)	Loss: 0.119900
Step #1670 (total examples = 26720)	Loss: 0.004674
Step #1680 (total examples = 26880)	Loss: 0.065800
Step #1690 (total examples = 27040)	Loss: 0.012021
Step #1700 (total examples = 27200)	Loss: 0.009218
Step #1710 (total examples = 27360)	Loss: 0.023829
Step #1720 (total examples = 27520)	Loss: 0.013858
Step #1730 (total examples = 27680)	Loss: 0.068190
Step #1740 (total examples = 27840)	Loss: 0.075782
Step #1750 (total examples = 28000)	Loss: 0.146218
Step #1760 (total examples = 28160)	Loss: 0.052309
Step #1770 (total examples = 28320)	Loss: 0.025474
Step #1780 (total examples = 28480)	Loss: 0.151508
Step #1790 (total examples = 28640)	Loss: 0.004684
Step #1800 (total examples = 28800)	Loss: 0.072964
Step #1810 (total examples = 28960)	Loss: 0.040146
Step #1820 (total examples = 29120)	Loss: 0.235481
Step #1830 (total examples = 29280)	Loss: 0.212635
Step #1840 (total examples = 29440)	Loss: 0.005321
Step #1850 (total examples = 29600)	Loss: 0.004779
Step #1860 (total examples = 29760)	Loss: 0.014406
Step #1870 (total examples = 29920)	Loss: 0.195955
Step #1880 (total examples = 30080)	Loss: 0.367960
Step #1890 (total examples = 30240)	Loss: 0.045534
Step #1900 (total examples = 30400)	Loss: 0.267842
Step #1910 (total examples = 30560)	Loss: 0.004306
Step #1920 (total examples = 30720)	Loss: 0.018081
Step #1930 (total examples = 30880)	Loss: 0.028755
Step #1940 (total examples = 31040)	Loss: 0.012151
Step #1950 (total examples = 31200)	Loss: 0.043420
Step #1960 (total examples = 31360)	Loss: 0.858565
Step #1970 (total examples = 31520)	Loss: 0.093479
Step #1980 (total examples = 31680)	Loss: 0.089664
Step #1990 (total examples = 31840)	Loss: 0.092455
  1/313 [..............................] - ETA: 2:22 - loss: 0.0000e+00 - accuracy: 1.0000  3/313 [..............................] - ETA: 11s - loss: 0.0000e+00 - accuracy: 1.0000   5/313 [..............................] - ETA: 11s - loss: 1.2051 - accuracy: 0.9937      7/313 [..............................] - ETA: 12s - loss: 0.8608 - accuracy: 0.9955  8/313 [..............................] - ETA: 14s - loss: 0.7532 - accuracy: 0.9961  9/313 [..............................] - ETA: 15s - loss: 0.6695 - accuracy: 0.9965 10/313 [..............................] - ETA: 16s - loss: 0.6026 - accuracy: 0.9969 11/313 [>.............................] - ETA: 16s - loss: 4.7606 - accuracy: 0.9886 12/313 [>.............................] - ETA: 17s - loss: 4.3639 - accuracy: 0.9896 13/313 [>.............................] - ETA: 17s - loss: 4.0282 - accuracy: 0.9904 14/313 [>.............................] - ETA: 17s - loss: 4.3169 - accuracy: 0.9866 15/313 [>.............................] - ETA: 17s - loss: 4.5284 - accuracy: 0.9854 16/313 [>.............................] - ETA: 18s - loss: 4.2453 - accuracy: 0.9863 17/313 [>.............................] - ETA: 18s - loss: 3.9956 - accuracy: 0.9871 18/313 [>.............................] - ETA: 18s - loss: 3.7736 - accuracy: 0.9878 19/313 [>.............................] - ETA: 18s - loss: 4.4737 - accuracy: 0.9868 20/313 [>.............................] - ETA: 18s - loss: 4.2500 - accuracy: 0.9875 22/313 [=>............................] - ETA: 17s - loss: 4.1931 - accuracy: 0.9872 24/313 [=>............................] - ETA: 16s - loss: 6.3728 - accuracy: 0.9831 26/313 [=>............................] - ETA: 16s - loss: 6.5541 - accuracy: 0.9832 28/313 [=>............................] - ETA: 15s - loss: 6.6321 - accuracy: 0.9833 30/313 [=>............................] - ETA: 15s - loss: 6.6423 - accuracy: 0.9812 32/313 [==>...........................] - ETA: 14s - loss: 7.6112 - accuracy: 0.9805 34/313 [==>...........................] - ETA: 14s - loss: 7.1639 - accuracy: 0.9816 36/313 [==>...........................] - ETA: 14s - loss: 7.1492 - accuracy: 0.9809 38/313 [==>...........................] - ETA: 13s - loss: 7.1338 - accuracy: 0.9803 40/313 [==>...........................] - ETA: 13s - loss: 7.6898 - accuracy: 0.9797 42/313 [===>..........................] - ETA: 13s - loss: 7.9581 - accuracy: 0.9784 44/313 [===>..........................] - ETA: 13s - loss: 7.8370 - accuracy: 0.9787 46/313 [===>..........................] - ETA: 12s - loss: 7.6828 - accuracy: 0.9789 48/313 [===>..........................] - ETA: 12s - loss: 7.6916 - accuracy: 0.9792 50/313 [===>..........................] - ETA: 12s - loss: 7.3968 - accuracy: 0.9794 52/313 [===>..........................] - ETA: 12s - loss: 7.6073 - accuracy: 0.9790 54/313 [====>.........................] - ETA: 12s - loss: 8.1346 - accuracy: 0.9780 56/313 [====>.........................] - ETA: 11s - loss: 8.0262 - accuracy: 0.9782 58/313 [====>.........................] - ETA: 11s - loss: 8.1807 - accuracy: 0.9784 60/313 [====>.........................] - ETA: 11s - loss: 8.3690 - accuracy: 0.9781 62/313 [====>.........................] - ETA: 11s - loss: 8.2062 - accuracy: 0.9783 64/313 [=====>........................] - ETA: 11s - loss: 9.1812 - accuracy: 0.9775 66/313 [=====>........................] - ETA: 11s - loss: 9.7383 - accuracy: 0.9768 68/313 [=====>........................] - ETA: 10s - loss: 11.5284 - accuracy: 0.9756 70/313 [=====>........................] - ETA: 10s - loss: 11.4036 - accuracy: 0.9750 72/313 [=====>........................] - ETA: 10s - loss: 11.4051 - accuracy: 0.9753 74/313 [======>.......................] - ETA: 10s - loss: 11.0969 - accuracy: 0.9759 76/313 [======>.......................] - ETA: 10s - loss: 11.3536 - accuracy: 0.9749 78/313 [======>.......................] - ETA: 10s - loss: 12.2326 - accuracy: 0.9744 80/313 [======>.......................] - ETA: 10s - loss: 11.9267 - accuracy: 0.9750 82/313 [======>.......................] - ETA: 10s - loss: 11.7476 - accuracy: 0.9748 84/313 [=======>......................] - ETA: 9s - loss: 12.3356 - accuracy: 0.9743  86/313 [=======>......................] - ETA: 9s - loss: 12.0656 - accuracy: 0.9746 88/313 [=======>......................] - ETA: 9s - loss: 11.9396 - accuracy: 0.9741 90/313 [=======>......................] - ETA: 9s - loss: 11.9228 - accuracy: 0.9743 92/313 [=======>......................] - ETA: 9s - loss: 12.4243 - accuracy: 0.9738 94/313 [========>.....................] - ETA: 9s - loss: 12.5210 - accuracy: 0.9734 96/313 [========>.....................] - ETA: 9s - loss: 12.3944 - accuracy: 0.9736 98/313 [========>.....................] - ETA: 9s - loss: 12.3379 - accuracy: 0.9739100/313 [========>.....................] - ETA: 8s - loss: 12.0911 - accuracy: 0.9744102/313 [========>.....................] - ETA: 8s - loss: 11.8540 - accuracy: 0.9749104/313 [========>.....................] - ETA: 8s - loss: 11.7713 - accuracy: 0.9748106/313 [=========>....................] - ETA: 8s - loss: 11.5492 - accuracy: 0.9752108/313 [=========>....................] - ETA: 8s - loss: 11.4517 - accuracy: 0.9754110/313 [=========>....................] - ETA: 8s - loss: 11.4992 - accuracy: 0.9756112/313 [=========>....................] - ETA: 8s - loss: 12.1785 - accuracy: 0.9752114/313 [=========>....................] - ETA: 8s - loss: 11.9648 - accuracy: 0.9756116/313 [==========>...................] - ETA: 8s - loss: 11.7585 - accuracy: 0.9760118/313 [==========>...................] - ETA: 8s - loss: 12.2134 - accuracy: 0.9756120/313 [==========>...................] - ETA: 7s - loss: 12.5498 - accuracy: 0.9755122/313 [==========>...................] - ETA: 7s - loss: 12.4666 - accuracy: 0.9757124/313 [==========>...................] - ETA: 7s - loss: 12.3422 - accuracy: 0.9756126/313 [===========>..................] - ETA: 7s - loss: 12.3403 - accuracy: 0.9757128/313 [===========>..................] - ETA: 7s - loss: 12.3825 - accuracy: 0.9756130/313 [===========>..................] - ETA: 7s - loss: 12.1920 - accuracy: 0.9760132/313 [===========>..................] - ETA: 7s - loss: 12.2247 - accuracy: 0.9756134/313 [===========>..................] - ETA: 7s - loss: 12.4186 - accuracy: 0.9755136/313 [============>.................] - ETA: 7s - loss: 12.2360 - accuracy: 0.9759138/313 [============>.................] - ETA: 7s - loss: 12.2750 - accuracy: 0.9758140/313 [============>.................] - ETA: 7s - loss: 12.0997 - accuracy: 0.9761142/313 [============>.................] - ETA: 6s - loss: 12.1403 - accuracy: 0.9760144/313 [============>.................] - ETA: 6s - loss: 12.0056 - accuracy: 0.9761146/313 [============>.................] - ETA: 6s - loss: 11.8486 - accuracy: 0.9762148/313 [=============>................] - ETA: 6s - loss: 11.6885 - accuracy: 0.9766150/313 [=============>................] - ETA: 6s - loss: 11.8813 - accuracy: 0.9762152/313 [=============>................] - ETA: 6s - loss: 12.0822 - accuracy: 0.9762154/313 [=============>................] - ETA: 6s - loss: 11.9253 - accuracy: 0.9765156/313 [=============>................] - ETA: 6s - loss: 11.7727 - accuracy: 0.9766158/313 [==============>...............] - ETA: 6s - loss: 11.6237 - accuracy: 0.9769160/313 [==============>...............] - ETA: 6s - loss: 11.4784 - accuracy: 0.9771162/313 [==============>...............] - ETA: 6s - loss: 11.3366 - accuracy: 0.9774164/313 [==============>...............] - ETA: 5s - loss: 11.1984 - accuracy: 0.9777166/313 [==============>...............] - ETA: 5s - loss: 11.0635 - accuracy: 0.9780168/313 [===============>..............] - ETA: 5s - loss: 10.9318 - accuracy: 0.9782170/313 [===============>..............] - ETA: 5s - loss: 10.8032 - accuracy: 0.9785172/313 [===============>..............] - ETA: 5s - loss: 10.6775 - accuracy: 0.9787174/313 [===============>..............] - ETA: 5s - loss: 10.5548 - accuracy: 0.9790176/313 [===============>..............] - ETA: 5s - loss: 10.4349 - accuracy: 0.9792178/313 [================>.............] - ETA: 5s - loss: 10.3176 - accuracy: 0.9795180/313 [================>.............] - ETA: 5s - loss: 10.2030 - accuracy: 0.9797182/313 [================>.............] - ETA: 5s - loss: 10.0909 - accuracy: 0.9799184/313 [================>.............] - ETA: 5s - loss: 9.9812 - accuracy: 0.9801 186/313 [================>.............] - ETA: 5s - loss: 10.1141 - accuracy: 0.9800188/313 [=================>............] - ETA: 4s - loss: 10.7580 - accuracy: 0.9794190/313 [=================>............] - ETA: 4s - loss: 10.8145 - accuracy: 0.9793192/313 [=================>............] - ETA: 4s - loss: 10.9258 - accuracy: 0.9792194/313 [=================>............] - ETA: 4s - loss: 11.0291 - accuracy: 0.9787196/313 [=================>............] - ETA: 4s - loss: 10.9166 - accuracy: 0.9790198/313 [=================>............] - ETA: 4s - loss: 10.8063 - accuracy: 0.9792200/313 [==================>...........] - ETA: 4s - loss: 10.6983 - accuracy: 0.9794202/313 [==================>...........] - ETA: 4s - loss: 10.5923 - accuracy: 0.9796204/313 [==================>...........] - ETA: 4s - loss: 10.6974 - accuracy: 0.9796206/313 [==================>...........] - ETA: 4s - loss: 10.5936 - accuracy: 0.9798208/313 [==================>...........] - ETA: 4s - loss: 10.7231 - accuracy: 0.9797210/313 [===================>..........] - ETA: 4s - loss: 10.6210 - accuracy: 0.9799212/313 [===================>..........] - ETA: 3s - loss: 10.5821 - accuracy: 0.9800214/313 [===================>..........] - ETA: 3s - loss: 10.4832 - accuracy: 0.9801216/313 [===================>..........] - ETA: 3s - loss: 10.3861 - accuracy: 0.9803218/313 [===================>..........] - ETA: 3s - loss: 10.2908 - accuracy: 0.9805220/313 [====================>.........] - ETA: 3s - loss: 10.1973 - accuracy: 0.9807222/313 [====================>.........] - ETA: 3s - loss: 10.1054 - accuracy: 0.9809224/313 [====================>.........] - ETA: 3s - loss: 10.0152 - accuracy: 0.9810226/313 [====================>.........] - ETA: 3s - loss: 9.9266 - accuracy: 0.9812 228/313 [====================>.........] - ETA: 3s - loss: 9.8395 - accuracy: 0.9814230/313 [=====================>........] - ETA: 3s - loss: 9.7539 - accuracy: 0.9815232/313 [=====================>........] - ETA: 3s - loss: 9.6698 - accuracy: 0.9817234/313 [=====================>........] - ETA: 3s - loss: 9.5872 - accuracy: 0.9818236/313 [=====================>........] - ETA: 3s - loss: 9.5059 - accuracy: 0.9820238/313 [=====================>........] - ETA: 2s - loss: 9.4261 - accuracy: 0.9821240/313 [======================>.......] - ETA: 2s - loss: 9.3475 - accuracy: 0.9823242/313 [======================>.......] - ETA: 2s - loss: 9.3084 - accuracy: 0.9823244/313 [======================>.......] - ETA: 2s - loss: 9.2321 - accuracy: 0.9825246/313 [======================>.......] - ETA: 2s - loss: 9.2912 - accuracy: 0.9823248/313 [======================>.......] - ETA: 2s - loss: 9.4812 - accuracy: 0.9822250/313 [======================>.......] - ETA: 2s - loss: 9.4416 - accuracy: 0.9822252/313 [=======================>......] - ETA: 2s - loss: 9.3666 - accuracy: 0.9824254/313 [=======================>......] - ETA: 2s - loss: 9.4400 - accuracy: 0.9824256/313 [=======================>......] - ETA: 2s - loss: 9.3663 - accuracy: 0.9825258/313 [=======================>......] - ETA: 2s - loss: 9.2937 - accuracy: 0.9827260/313 [=======================>......] - ETA: 2s - loss: 9.2276 - accuracy: 0.9827262/313 [========================>.....] - ETA: 1s - loss: 9.1572 - accuracy: 0.9828264/313 [========================>.....] - ETA: 1s - loss: 9.1179 - accuracy: 0.9828266/313 [========================>.....] - ETA: 1s - loss: 9.0493 - accuracy: 0.9830268/313 [========================>.....] - ETA: 1s - loss: 9.0309 - accuracy: 0.9829270/313 [========================>.....] - ETA: 1s - loss: 8.9640 - accuracy: 0.9830272/313 [=========================>....] - ETA: 1s - loss: 8.8981 - accuracy: 0.9831274/313 [=========================>....] - ETA: 1s - loss: 8.8332 - accuracy: 0.9832276/313 [=========================>....] - ETA: 1s - loss: 8.7692 - accuracy: 0.9834278/313 [=========================>....] - ETA: 1s - loss: 8.7061 - accuracy: 0.9835280/313 [=========================>....] - ETA: 1s - loss: 8.6439 - accuracy: 0.9836282/313 [==========================>...] - ETA: 1s - loss: 8.9355 - accuracy: 0.9834284/313 [==========================>...] - ETA: 1s - loss: 8.8726 - accuracy: 0.9835286/313 [==========================>...] - ETA: 1s - loss: 8.8105 - accuracy: 0.9836288/313 [==========================>...] - ETA: 0s - loss: 8.7494 - accuracy: 0.9837290/313 [==========================>...] - ETA: 0s - loss: 8.6890 - accuracy: 0.9838292/313 [==========================>...] - ETA: 0s - loss: 8.6295 - accuracy: 0.9839294/313 [===========================>..] - ETA: 0s - loss: 8.5708 - accuracy: 0.9841296/313 [===========================>..] - ETA: 0s - loss: 8.5129 - accuracy: 0.9842298/313 [===========================>..] - ETA: 0s - loss: 8.6085 - accuracy: 0.9842300/313 [===========================>..] - ETA: 0s - loss: 8.5511 - accuracy: 0.9843302/313 [===========================>..] - ETA: 0s - loss: 8.4974 - accuracy: 0.9843304/313 [============================>.] - ETA: 0s - loss: 8.5248 - accuracy: 0.9841306/313 [============================>.] - ETA: 0s - loss: 8.7482 - accuracy: 0.9838308/313 [============================>.] - ETA: 0s - loss: 8.7920 - accuracy: 0.9837310/313 [============================>.] - ETA: 0s - loss: 8.7502 - accuracy: 0.9837312/313 [============================>.] - ETA: 0s - loss: 8.7736 - accuracy: 0.9837313/313 [==============================] - 13s 39ms/step - loss: 8.7596 - accuracy: 0.9837
Final model accuracy: 0.983700
Total training time: 241.708049
2021-12-03 21:02:00.860684: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 21:02:00.860876: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-12-03 21:02:00.870656: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 21:02:00.870758: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-12-03 21:02:04.848087: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 21:02:04.848216: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2021-12-03 21:02:04.848276: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (node-0005): /proc/driver/nvidia/version does not exist
2021-12-03 21:02:04.893421: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 21:02:04.893601: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2021-12-03 21:02:04.893669: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (node-0001): /proc/driver/nvidia/version does not exist
===========
rank: 0
Batch size: 16
Total number of batches: 4000
Total training examples: 64000
Num of workers: 2
===========
Step #0 (total examples = 0)	Loss: 2.313693
Step #10 (total examples = 160)	Loss: 0.823512
Step #20 (total examples = 320)	Loss: 0.698384
Step #30 (total examples = 480)	Loss: 0.645328
Step #40 (total examples = 640)	Loss: 0.615902
Step #50 (total examples = 800)	Loss: 0.317968
Step #60 (total examples = 960)	Loss: 0.281355
Step #70 (total examples = 1120)	Loss: 0.895291
Step #80 (total examples = 1280)	Loss: 0.238913
Step #90 (total examples = 1440)	Loss: 0.100060
Step #100 (total examples = 1600)	Loss: 0.140693
Step #110 (total examples = 1760)	Loss: 0.305524
Step #120 (total examples = 1920)	Loss: 0.617761
Step #130 (total examples = 2080)	Loss: 0.045391
Step #140 (total examples = 2240)	Loss: 0.219116
Step #150 (total examples = 2400)	Loss: 0.079352
Step #160 (total examples = 2560)	Loss: 0.305233
Step #170 (total examples = 2720)	Loss: 0.076524
Step #180 (total examples = 2880)	Loss: 0.011474
Step #190 (total examples = 3040)	Loss: 0.065006
Step #200 (total examples = 3200)	Loss: 0.123597
Step #210 (total examples = 3360)	Loss: 0.126355
Step #220 (total examples = 3520)	Loss: 0.075319
Step #230 (total examples = 3680)	Loss: 0.071574
Step #240 (total examples = 3840)	Loss: 0.014849
Step #250 (total examples = 4000)	Loss: 0.009723
Step #260 (total examples = 4160)	Loss: 0.062769
Step #270 (total examples = 4320)	Loss: 0.045350
Step #280 (total examples = 4480)	Loss: 0.040251
Step #290 (total examples = 4640)	Loss: 0.182532
Step #300 (total examples = 4800)	Loss: 0.137726
Step #310 (total examples = 4960)	Loss: 0.289243
Step #320 (total examples = 5120)	Loss: 0.300194
Step #330 (total examples = 5280)	Loss: 0.015565
Step #340 (total examples = 5440)	Loss: 0.057816
Step #350 (total examples = 5600)	Loss: 0.343072
Step #360 (total examples = 5760)	Loss: 0.060386
Step #370 (total examples = 5920)	Loss: 0.347820
Step #380 (total examples = 6080)	Loss: 0.009271
Step #390 (total examples = 6240)	Loss: 0.219401
Step #400 (total examples = 6400)	Loss: 0.093390
Step #410 (total examples = 6560)	Loss: 0.104833
Step #420 (total examples = 6720)	Loss: 0.016965
Step #430 (total examples = 6880)	Loss: 0.628253
Step #440 (total examples = 7040)	Loss: 0.184900
Step #450 (total examples = 7200)	Loss: 0.052454
Step #460 (total examples = 7360)	Loss: 0.047219
Step #470 (total examples = 7520)	Loss: 0.056789
Step #480 (total examples = 7680)	Loss: 0.151710
Step #490 (total examples = 7840)	Loss: 0.091739
Step #500 (total examples = 8000)	Loss: 0.011858
Step #510 (total examples = 8160)	Loss: 0.004529
Step #520 (total examples = 8320)	Loss: 0.104026
Step #530 (total examples = 8480)	Loss: 0.109239
Step #540 (total examples = 8640)	Loss: 0.100604
Step #550 (total examples = 8800)	Loss: 0.098148
Step #560 (total examples = 8960)	Loss: 0.502674
Step #570 (total examples = 9120)	Loss: 0.060881
Step #580 (total examples = 9280)	Loss: 0.113136
Step #590 (total examples = 9440)	Loss: 0.231075
Step #600 (total examples = 9600)	Loss: 0.146280
Step #610 (total examples = 9760)	Loss: 0.025690
Step #620 (total examples = 9920)	Loss: 0.007021
Step #630 (total examples = 10080)	Loss: 0.056637
Step #640 (total examples = 10240)	Loss: 0.011423
Step #650 (total examples = 10400)	Loss: 0.086139
Step #660 (total examples = 10560)	Loss: 0.113602
Step #670 (total examples = 10720)	Loss: 0.040421
Step #680 (total examples = 10880)	Loss: 0.154754
Step #690 (total examples = 11040)	Loss: 0.065356
Step #700 (total examples = 11200)	Loss: 0.072238
Step #710 (total examples = 11360)	Loss: 0.277198
Step #720 (total examples = 11520)	Loss: 0.052309
Step #730 (total examples = 11680)	Loss: 0.785151
Step #740 (total examples = 11840)	Loss: 0.015985
Step #750 (total examples = 12000)	Loss: 0.018069
Step #760 (total examples = 12160)	Loss: 0.080860
Step #770 (total examples = 12320)	Loss: 0.098465
Step #780 (total examples = 12480)	Loss: 0.031050
Step #790 (total examples = 12640)	Loss: 0.160556
Step #800 (total examples = 12800)	Loss: 0.057671
Step #810 (total examples = 12960)	Loss: 0.239197
Step #820 (total examples = 13120)	Loss: 0.237066
Step #830 (total examples = 13280)	Loss: 0.067183
Step #840 (total examples = 13440)	Loss: 0.016656
Step #850 (total examples = 13600)	Loss: 0.001621
Step #860 (total examples = 13760)	Loss: 0.391005
Step #870 (total examples = 13920)	Loss: 0.146101
Step #880 (total examples = 14080)	Loss: 0.113451
Step #890 (total examples = 14240)	Loss: 0.025212
Step #900 (total examples = 14400)	Loss: 0.099187
Step #910 (total examples = 14560)	Loss: 0.006106
Step #920 (total examples = 14720)	Loss: 0.035231
Step #930 (total examples = 14880)	Loss: 0.148417
Step #940 (total examples = 15040)	Loss: 0.048238
Step #950 (total examples = 15200)	Loss: 0.220665
Step #960 (total examples = 15360)	Loss: 0.106104
Step #970 (total examples = 15520)	Loss: 0.010775
Step #980 (total examples = 15680)	Loss: 0.146109
Step #990 (total examples = 15840)	Loss: 0.251559
Step #1000 (total examples = 16000)	Loss: 0.016180
Step #1010 (total examples = 16160)	Loss: 0.000863
Step #1020 (total examples = 16320)	Loss: 0.393966
Step #1030 (total examples = 16480)	Loss: 0.033233
Step #1040 (total examples = 16640)	Loss: 0.006870
Step #1050 (total examples = 16800)	Loss: 0.107794
Step #1060 (total examples = 16960)	Loss: 0.017339
Step #1070 (total examples = 17120)	Loss: 0.006155
Step #1080 (total examples = 17280)	Loss: 0.526092
Step #1090 (total examples = 17440)	Loss: 0.124487
Step #1100 (total examples = 17600)	Loss: 0.009352
Step #1110 (total examples = 17760)	Loss: 0.003806
Step #1120 (total examples = 17920)	Loss: 0.020356
Step #1130 (total examples = 18080)	Loss: 0.105112
Step #1140 (total examples = 18240)	Loss: 0.021860
Step #1150 (total examples = 18400)	Loss: 0.039756
Step #1160 (total examples = 18560)	Loss: 0.325629
Step #1170 (total examples = 18720)	Loss: 0.047057
Step #1180 (total examples = 18880)	Loss: 0.001155
Step #1190 (total examples = 19040)	Loss: 0.016631
Step #1200 (total examples = 19200)	Loss: 0.171359
Step #1210 (total examples = 19360)	Loss: 0.024984
Step #1220 (total examples = 19520)	Loss: 0.007586
Step #1230 (total examples = 19680)	Loss: 0.007100
Step #1240 (total examples = 19840)	Loss: 0.032097
Step #1250 (total examples = 20000)	Loss: 0.021332
Step #1260 (total examples = 20160)	Loss: 0.013610
Step #1270 (total examples = 20320)	Loss: 0.010207
Step #1280 (total examples = 20480)	Loss: 0.141117
Step #1290 (total examples = 20640)	Loss: 0.011165
Step #1300 (total examples = 20800)	Loss: 0.127425
Step #1310 (total examples = 20960)	Loss: 0.019083
Step #1320 (total examples = 21120)	Loss: 0.033711
Step #1330 (total examples = 21280)	Loss: 0.011815
Step #1340 (total examples = 21440)	Loss: 0.033093
Step #1350 (total examples = 21600)	Loss: 0.080702
Step #1360 (total examples = 21760)	Loss: 0.155684
Step #1370 (total examples = 21920)	Loss: 0.186294
Step #1380 (total examples = 22080)	Loss: 0.027699
Step #1390 (total examples = 22240)	Loss: 0.069719
Step #1400 (total examples = 22400)	Loss: 0.010799
Step #1410 (total examples = 22560)	Loss: 0.094184
Step #1420 (total examples = 22720)	Loss: 0.044466
Step #1430 (total examples = 22880)	Loss: 0.051791
Step #1440 (total examples = 23040)	Loss: 0.125486
Step #1450 (total examples = 23200)	Loss: 0.013242
Step #1460 (total examples = 23360)	Loss: 0.064698
Step #1470 (total examples = 23520)	Loss: 0.011074
Step #1480 (total examples = 23680)	Loss: 0.239221
Step #1490 (total examples = 23840)	Loss: 0.016023
Step #1500 (total examples = 24000)	Loss: 0.210020
Step #1510 (total examples = 24160)	Loss: 0.247266
Step #1520 (total examples = 24320)	Loss: 0.017699
Step #1530 (total examples = 24480)	Loss: 0.143336
Step #1540 (total examples = 24640)	Loss: 0.039365
Step #1550 (total examples = 24800)	Loss: 0.162476
Step #1560 (total examples = 24960)	Loss: 0.007515
Step #1570 (total examples = 25120)	Loss: 0.014334
Step #1580 (total examples = 25280)	Loss: 0.023307
Step #1590 (total examples = 25440)	Loss: 0.016461
Step #1600 (total examples = 25600)	Loss: 0.511343
Step #1610 (total examples = 25760)	Loss: 0.016373===========
rank: 1
===========
Step #0 (total examples = 0)	Loss: 2.307378
Step #10 (total examples = 160)	Loss: 1.243929
Step #20 (total examples = 320)	Loss: 0.981905
Step #30 (total examples = 480)	Loss: 0.286918
Step #40 (total examples = 640)	Loss: 1.277239
Step #50 (total examples = 800)	Loss: 0.223876
Step #60 (total examples = 960)	Loss: 0.521932
Step #70 (total examples = 1120)	Loss: 0.331197
Step #80 (total examples = 1280)	Loss: 1.670006
Step #90 (total examples = 1440)	Loss: 0.177958
Step #100 (total examples = 1600)	Loss: 0.196135
Step #110 (total examples = 1760)	Loss: 0.408297
Step #120 (total examples = 1920)	Loss: 0.088930
Step #130 (total examples = 2080)	Loss: 0.216823
Step #140 (total examples = 2240)	Loss: 0.093539
Step #150 (total examples = 2400)	Loss: 0.269491
Step #160 (total examples = 2560)	Loss: 0.067413
Step #170 (total examples = 2720)	Loss: 0.155028
Step #180 (total examples = 2880)	Loss: 0.120750
Step #190 (total examples = 3040)	Loss: 0.021810
Step #200 (total examples = 3200)	Loss: 0.195808
Step #210 (total examples = 3360)	Loss: 0.034078
Step #220 (total examples = 3520)	Loss: 0.282276
Step #230 (total examples = 3680)	Loss: 0.402203
Step #240 (total examples = 3840)	Loss: 0.202981
Step #250 (total examples = 4000)	Loss: 0.748790
Step #260 (total examples = 4160)	Loss: 0.039118
Step #270 (total examples = 4320)	Loss: 0.008929
Step #280 (total examples = 4480)	Loss: 0.159189
Step #290 (total examples = 4640)	Loss: 0.115811
Step #300 (total examples = 4800)	Loss: 0.133849
Step #310 (total examples = 4960)	Loss: 0.127058
Step #320 (total examples = 5120)	Loss: 0.034808
Step #330 (total examples = 5280)	Loss: 0.128564
Step #340 (total examples = 5440)	Loss: 0.082356
Step #350 (total examples = 5600)	Loss: 0.372680
Step #360 (total examples = 5760)	Loss: 0.029272
Step #370 (total examples = 5920)	Loss: 0.339473
Step #380 (total examples = 6080)	Loss: 0.290053
Step #390 (total examples = 6240)	Loss: 0.281189
Step #400 (total examples = 6400)	Loss: 0.243049
Step #410 (total examples = 6560)	Loss: 0.186483
Step #420 (total examples = 6720)	Loss: 0.060041
Step #430 (total examples = 6880)	Loss: 0.150192
Step #440 (total examples = 7040)	Loss: 0.031644
Step #450 (total examples = 7200)	Loss: 0.096733
Step #460 (total examples = 7360)	Loss: 0.290688
Step #470 (total examples = 7520)	Loss: 0.034831
Step #480 (total examples = 7680)	Loss: 0.548756
Step #490 (total examples = 7840)	Loss: 0.349609
Step #500 (total examples = 8000)	Loss: 0.225633
Step #510 (total examples = 8160)	Loss: 0.002721
Step #520 (total examples = 8320)	Loss: 0.186812
Step #530 (total examples = 8480)	Loss: 0.029026
Step #540 (total examples = 8640)	Loss: 0.052801
Step #550 (total examples = 8800)	Loss: 0.406751
Step #560 (total examples = 8960)	Loss: 0.011907
Step #570 (total examples = 9120)	Loss: 0.335943
Step #580 (total examples = 9280)	Loss: 0.011034
Step #590 (total examples = 9440)	Loss: 0.120145
Step #600 (total examples = 9600)	Loss: 0.059997
Step #610 (total examples = 9760)	Loss: 0.016454
Step #620 (total examples = 9920)	Loss: 0.187200
Step #630 (total examples = 10080)	Loss: 0.066968
Step #640 (total examples = 10240)	Loss: 0.238513
Step #650 (total examples = 10400)	Loss: 0.025036
Step #660 (total examples = 10560)	Loss: 0.108346
Step #670 (total examples = 10720)	Loss: 0.083371
Step #680 (total examples = 10880)	Loss: 0.406846
Step #690 (total examples = 11040)	Loss: 0.041984
Step #700 (total examples = 11200)	Loss: 0.144510
Step #710 (total examples = 11360)	Loss: 0.097654
Step #720 (total examples = 11520)	Loss: 0.101762
Step #730 (total examples = 11680)	Loss: 0.215118
Step #740 (total examples = 11840)	Loss: 0.058067
Step #750 (total examples = 12000)	Loss: 0.554370
Step #760 (total examples = 12160)	Loss: 0.231380
Step #770 (total examples = 12320)	Loss: 0.446083
Step #780 (total examples = 12480)	Loss: 0.069698
Step #790 (total examples = 12640)	Loss: 0.106984
Step #800 (total examples = 12800)	Loss: 0.195294
Step #810 (total examples = 12960)	Loss: 0.013474
Step #820 (total examples = 13120)	Loss: 0.129913
Step #830 (total examples = 13280)	Loss: 0.038502
Step #840 (total examples = 13440)	Loss: 0.072371
Step #850 (total examples = 13600)	Loss: 0.009346
Step #860 (total examples = 13760)	Loss: 0.348690
Step #870 (total examples = 13920)	Loss: 0.045873
Step #880 (total examples = 14080)	Loss: 0.079880
Step #890 (total examples = 14240)	Loss: 0.008270
Step #900 (total examples = 14400)	Loss: 0.021580
Step #910 (total examples = 14560)	Loss: 0.024395
Step #920 (total examples = 14720)	Loss: 0.147217
Step #930 (total examples = 14880)	Loss: 0.147127
Step #940 (total examples = 15040)	Loss: 0.122518
Step #950 (total examples = 15200)	Loss: 0.099677
Step #960 (total examples = 15360)	Loss: 0.025069
Step #970 (total examples = 15520)	Loss: 0.451133
Step #980 (total examples = 15680)	Loss: 0.024536
Step #990 (total examples = 15840)	Loss: 0.150050
Step #1000 (total examples = 16000)	Loss: 0.063458
Step #1010 (total examples = 16160)	Loss: 0.213463
Step #1020 (total examples = 16320)	Loss: 0.059851
Step #1030 (total examples = 16480)	Loss: 0.083251
Step #1040 (total examples = 16640)	Loss: 0.004733
Step #1050 (total examples = 16800)	Loss: 0.009202
Step #1060 (total examples = 16960)	Loss: 0.003648
Step #1070 (total examples = 17120)	Loss: 0.248059
Step #1080 (total examples = 17280)	Loss: 0.011377
Step #1090 (total examples = 17440)	Loss: 0.003339
Step #1100 (total examples = 17600)	Loss: 0.023101
Step #1110 (total examples = 17760)	Loss: 0.021765
Step #1120 (total examples = 17920)	Loss: 0.148931
Step #1130 (total examples = 18080)	Loss: 0.101696
Step #1140 (total examples = 18240)	Loss: 0.067267
Step #1150 (total examples = 18400)	Loss: 0.010382
Step #1160 (total examples = 18560)	Loss: 0.097352
Step #1170 (total examples = 18720)	Loss: 0.043094
Step #1180 (total examples = 18880)	Loss: 0.061244
Step #1190 (total examples = 19040)	Loss: 0.021049
Step #1200 (total examples = 19200)	Loss: 0.010967
Step #1210 (total examples = 19360)	Loss: 0.054295
Step #1220 (total examples = 19520)	Loss: 0.639702
Step #1230 (total examples = 19680)	Loss: 0.006807
Step #1240 (total examples = 19840)	Loss: 0.179324
Step #1250 (total examples = 20000)	Loss: 0.198083
Step #1260 (total examples = 20160)	Loss: 0.330871
Step #1270 (total examples = 20320)	Loss: 0.052239
Step #1280 (total examples = 20480)	Loss: 0.026802
Step #1290 (total examples = 20640)	Loss: 0.043719
Step #1300 (total examples = 20800)	Loss: 0.097230
Step #1310 (total examples = 20960)	Loss: 0.003812
Step #1320 (total examples = 21120)	Loss: 0.000757
Step #1330 (total examples = 21280)	Loss: 0.294268
Step #1340 (total examples = 21440)	Loss: 0.068209
Step #1350 (total examples = 21600)	Loss: 0.002841
Step #1360 (total examples = 21760)	Loss: 0.078330
Step #1370 (total examples = 21920)	Loss: 0.062511
Step #1380 (total examples = 22080)	Loss: 0.009554
Step #1390 (total examples = 22240)	Loss: 0.021875
Step #1400 (total examples = 22400)	Loss: 0.001762
Step #1410 (total examples = 22560)	Loss: 0.014923
Step #1420 (total examples = 22720)	Loss: 0.003296
Step #1430 (total examples = 22880)	Loss: 0.033262
Step #1440 (total examples = 23040)	Loss: 0.046462
Step #1450 (total examples = 23200)	Loss: 0.045589
Step #1460 (total examples = 23360)	Loss: 0.259050
Step #1470 (total examples = 23520)	Loss: 0.013442
Step #1480 (total examples = 23680)	Loss: 0.013791
Step #1490 (total examples = 23840)	Loss: 0.000761
Step #1500 (total examples = 24000)	Loss: 0.016347
Step #1510 (total examples = 24160)	Loss: 0.118237
Step #1520 (total examples = 24320)	Loss: 0.023841
Step #1530 (total examples = 24480)	Loss: 0.107065
Step #1540 (total examples = 24640)	Loss: 0.014262
Step #1550 (total examples = 24800)	Loss: 0.005140
Step #1560 (total examples = 24960)	Loss: 0.109115
Step #1570 (total examples = 25120)	Loss: 0.007427
Step #1580 (total examples = 25280)	Loss: 0.025146
Step #1590 (total examples = 25440)	Loss: 0.244138
Step #1600 (total examples = 25600)	Loss: 0.156007
Step #1610 (total examples = 25760)	Loss: 0.002027
Step #1620 (total examples = 25920)	Loss: 0.001832
Step #1630 (total examples = 26080)	Loss: 0.008817
Step #1640 (total examples = 26240)	Loss: 0.020384
Step #1650 (total examples = 26400)	Loss: 0.001984
Step #1660 (total examples = 26560)	Loss: 0.009602
Step #1670 (total examples = 26720)	Loss: 0.008308
Step #1680 (total examples = 26880)	Loss: 0.209275
Step #1690 (total examples = 27040)	Loss: 0.009306
Step #1700 (total examples = 27200)	Loss: 0.015837
Step #1710 (total examples = 27360)	Loss: 0.003110
Step #1720 (total examples = 27520)	Loss: 0.101542
Step #1730 (total examples = 27680)	Loss: 0.003392
Step #1740 (total examples = 27840)	Loss: 0.011210
Step #1750 (total examples = 28000)	Loss: 0.006056
Step #1760 (total examples = 28160)	Loss: 0.099647
Step #1770 (total examples = 28320)	Loss: 0.193874
Step #1780 (total examples = 28480)	Loss: 0.061537
Step #1790 (total examples = 28640)	Loss: 0.024990
Step #1800 (total examples = 28800)	Loss: 0.048863
Step #1810 (total examples = 28960)	Loss: 0.005688
Step #1820 (total examples = 29120)	Loss: 0.025415
Step #1830 (total examples = 29280)	Loss: 0.004504
Step #1840 (total examples = 29440)	Loss: 0.008412
Step #1850 (total examples = 29600)	Loss: 0.007428
Step #1860 (total examples = 29760)	Loss: 0.002703
Step #1870 (total examples = 29920)	Loss: 0.000359
Step #1880 (total examples = 30080)	Loss: 0.010615
Step #1890 (total examples = 30240)	Loss: 0.020368
Step #1900 (total examples = 30400)	Loss: 0.005706
Step #1910 (total examples = 30560)	Loss: 0.004891
Step #1920 (total examples = 30720)	Loss: 0.082537
Step #1930 (total examples = 30880)	Loss: 0.064023
Step #1940 (total examples = 31040)	Loss: 0.025329
Step #1950 (total examples = 31200)	Loss: 0.001623
Step #1960 (total examples = 31360)	Loss: 0.478962
Step #1970 (total examples = 31520)	Loss: 0.082631
Step #1980 (total examples = 31680)	Loss: 0.518335
Step #1990 (total examples = 31840)	Loss: 0.173754

Step #1620 (total examples = 25920)	Loss: 0.074700
Step #1630 (total examples = 26080)	Loss: 0.037494
Step #1640 (total examples = 26240)	Loss: 0.032972
Step #1650 (total examples = 26400)	Loss: 0.054423
Step #1660 (total examples = 26560)	Loss: 0.103487
Step #1670 (total examples = 26720)	Loss: 0.031199
Step #1680 (total examples = 26880)	Loss: 0.093848
Step #1690 (total examples = 27040)	Loss: 0.059717
Step #1700 (total examples = 27200)	Loss: 0.006714
Step #1710 (total examples = 27360)	Loss: 0.058853
Step #1720 (total examples = 27520)	Loss: 0.016553
Step #1730 (total examples = 27680)	Loss: 0.012262
Step #1740 (total examples = 27840)	Loss: 0.033791
Step #1750 (total examples = 28000)	Loss: 0.035067
Step #1760 (total examples = 28160)	Loss: 0.029471
Step #1770 (total examples = 28320)	Loss: 0.003798
Step #1780 (total examples = 28480)	Loss: 0.001307
Step #1790 (total examples = 28640)	Loss: 0.000612
Step #1800 (total examples = 28800)	Loss: 0.250432
Step #1810 (total examples = 28960)	Loss: 0.125736
Step #1820 (total examples = 29120)	Loss: 0.047688
Step #1830 (total examples = 29280)	Loss: 0.400934
Step #1840 (total examples = 29440)	Loss: 0.025662
Step #1850 (total examples = 29600)	Loss: 0.006254
Step #1860 (total examples = 29760)	Loss: 0.006307
Step #1870 (total examples = 29920)	Loss: 0.012534
Step #1880 (total examples = 30080)	Loss: 0.288093
Step #1890 (total examples = 30240)	Loss: 0.088748
Step #1900 (total examples = 30400)	Loss: 0.061024
Step #1910 (total examples = 30560)	Loss: 0.007682
Step #1920 (total examples = 30720)	Loss: 0.435286
Step #1930 (total examples = 30880)	Loss: 0.015308
Step #1940 (total examples = 31040)	Loss: 0.031055
Step #1950 (total examples = 31200)	Loss: 0.004106
Step #1960 (total examples = 31360)	Loss: 0.876507
Step #1970 (total examples = 31520)	Loss: 0.040324
Step #1980 (total examples = 31680)	Loss: 0.024755
Step #1990 (total examples = 31840)	Loss: 0.048018
  1/313 [..............................] - ETA: 2:23 - loss: 5.3578 - accuracy: 0.9688  3/313 [..............................] - ETA: 11s - loss: 1.7859 - accuracy: 0.9896   5/313 [..............................] - ETA: 11s - loss: 4.2793 - accuracy: 0.9875  6/313 [..............................] - ETA: 12s - loss: 3.5661 - accuracy: 0.9896  7/313 [..............................] - ETA: 14s - loss: 3.0567 - accuracy: 0.9911  8/313 [..............................] - ETA: 15s - loss: 2.6746 - accuracy: 0.9922  9/313 [..............................] - ETA: 16s - loss: 2.3774 - accuracy: 0.9931 10/313 [..............................] - ETA: 16s - loss: 2.1397 - accuracy: 0.9937 11/313 [>.............................] - ETA: 17s - loss: 7.7552 - accuracy: 0.9830 12/313 [>.............................] - ETA: 17s - loss: 7.1089 - accuracy: 0.9844 13/313 [>.............................] - ETA: 18s - loss: 6.5621 - accuracy: 0.9856 14/313 [>.............................] - ETA: 18s - loss: 7.1594 - accuracy: 0.9844 15/313 [>.............................] - ETA: 18s - loss: 6.9117 - accuracy: 0.9833 16/313 [>.............................] - ETA: 18s - loss: 6.4797 - accuracy: 0.9844 17/313 [>.............................] - ETA: 18s - loss: 6.0986 - accuracy: 0.9853 18/313 [>.............................] - ETA: 18s - loss: 5.7598 - accuracy: 0.9861 19/313 [>.............................] - ETA: 18s - loss: 5.4730 - accuracy: 0.9852 21/313 [=>............................] - ETA: 18s - loss: 5.8918 - accuracy: 0.9851 23/313 [=>............................] - ETA: 17s - loss: 6.5698 - accuracy: 0.9837 25/313 [=>............................] - ETA: 16s - loss: 6.7548 - accuracy: 0.9837 27/313 [=>............................] - ETA: 16s - loss: 6.8450 - accuracy: 0.9826 29/313 [=>............................] - ETA: 15s - loss: 6.5346 - accuracy: 0.9828 31/313 [=>............................] - ETA: 15s - loss: 6.4760 - accuracy: 0.9819 33/313 [==>...........................] - ETA: 14s - loss: 8.5232 - accuracy: 0.9801 35/313 [==>...........................] - ETA: 14s - loss: 8.6250 - accuracy: 0.9804 37/313 [==>...........................] - ETA: 14s - loss: 8.1587 - accuracy: 0.9814 39/313 [==>...........................] - ETA: 13s - loss: 9.0400 - accuracy: 0.9800 41/313 [==>...........................] - ETA: 13s - loss: 8.6808 - accuracy: 0.9802 43/313 [===>..........................] - ETA: 13s - loss: 8.2770 - accuracy: 0.9811 45/313 [===>..........................] - ETA: 13s - loss: 7.9522 - accuracy: 0.9806 47/313 [===>..........................] - ETA: 12s - loss: 7.6139 - accuracy: 0.9814 49/313 [===>..........................] - ETA: 12s - loss: 8.6724 - accuracy: 0.9802 51/313 [===>..........................] - ETA: 12s - loss: 8.4005 - accuracy: 0.9804 53/313 [====>.........................] - ETA: 12s - loss: 8.3036 - accuracy: 0.9800 55/313 [====>.........................] - ETA: 11s - loss: 8.8811 - accuracy: 0.9790 57/313 [====>.........................] - ETA: 11s - loss: 8.5695 - accuracy: 0.9797 59/313 [====>.........................] - ETA: 11s - loss: 8.2790 - accuracy: 0.9804 61/313 [====>.........................] - ETA: 11s - loss: 8.8990 - accuracy: 0.9805 63/313 [=====>........................] - ETA: 11s - loss: 8.6165 - accuracy: 0.9812 65/313 [=====>........................] - ETA: 11s - loss: 9.3812 - accuracy: 0.9793 67/313 [=====>........................] - ETA: 11s - loss: 11.1964 - accuracy: 0.9771 69/313 [=====>........................] - ETA: 10s - loss: 10.8980 - accuracy: 0.9769 71/313 [=====>........................] - ETA: 10s - loss: 10.5910 - accuracy: 0.9776 73/313 [=====>........................] - ETA: 10s - loss: 10.4056 - accuracy: 0.9777 75/313 [======>.......................] - ETA: 10s - loss: 10.7688 - accuracy: 0.9775 77/313 [======>.......................] - ETA: 10s - loss: 11.2777 - accuracy: 0.9761 79/313 [======>.......................] - ETA: 10s - loss: 11.1125 - accuracy: 0.9763 81/313 [======>.......................] - ETA: 10s - loss: 10.9386 - accuracy: 0.9765 83/313 [======>.......................] - ETA: 9s - loss: 11.7602 - accuracy: 0.9763  85/313 [=======>......................] - ETA: 9s - loss: 11.4835 - accuracy: 0.9768 87/313 [=======>......................] - ETA: 9s - loss: 11.4450 - accuracy: 0.9767 89/313 [=======>......................] - ETA: 9s - loss: 11.1878 - accuracy: 0.9772 91/313 [=======>......................] - ETA: 9s - loss: 11.1446 - accuracy: 0.9773 93/313 [=======>......................] - ETA: 9s - loss: 11.2235 - accuracy: 0.9775 95/313 [========>.....................] - ETA: 9s - loss: 11.2407 - accuracy: 0.9773 97/313 [========>.....................] - ETA: 9s - loss: 11.1797 - accuracy: 0.9774 99/313 [========>.....................] - ETA: 9s - loss: 10.9538 - accuracy: 0.9779101/313 [========>.....................] - ETA: 8s - loss: 10.8279 - accuracy: 0.9780103/313 [========>.....................] - ETA: 8s - loss: 10.6176 - accuracy: 0.9785105/313 [=========>....................] - ETA: 8s - loss: 10.4826 - accuracy: 0.9786107/313 [=========>....................] - ETA: 8s - loss: 10.7069 - accuracy: 0.9787109/313 [=========>....................] - ETA: 8s - loss: 10.5105 - accuracy: 0.9791111/313 [=========>....................] - ETA: 8s - loss: 11.4446 - accuracy: 0.9786113/313 [=========>....................] - ETA: 8s - loss: 11.5564 - accuracy: 0.9782115/313 [==========>...................] - ETA: 8s - loss: 11.3554 - accuracy: 0.9785117/313 [==========>...................] - ETA: 8s - loss: 11.3446 - accuracy: 0.9786119/313 [==========>...................] - ETA: 8s - loss: 11.5407 - accuracy: 0.9785121/313 [==========>...................] - ETA: 7s - loss: 12.1078 - accuracy: 0.9780123/313 [==========>...................] - ETA: 7s - loss: 12.1035 - accuracy: 0.9782125/313 [==========>...................] - ETA: 7s - loss: 11.9710 - accuracy: 0.9783127/313 [===========>..................] - ETA: 7s - loss: 12.1130 - accuracy: 0.9783129/313 [===========>..................] - ETA: 7s - loss: 11.9252 - accuracy: 0.9787131/313 [===========>..................] - ETA: 7s - loss: 12.0764 - accuracy: 0.9788133/313 [===========>..................] - ETA: 7s - loss: 12.4927 - accuracy: 0.9784135/313 [===========>..................] - ETA: 7s - loss: 12.3076 - accuracy: 0.9787137/313 [============>.................] - ETA: 7s - loss: 12.1407 - accuracy: 0.9788139/313 [============>.................] - ETA: 7s - loss: 11.9660 - accuracy: 0.9791141/313 [============>.................] - ETA: 7s - loss: 11.8480 - accuracy: 0.9792143/313 [============>.................] - ETA: 6s - loss: 11.9477 - accuracy: 0.9790145/313 [============>.................] - ETA: 6s - loss: 11.7829 - accuracy: 0.9793147/313 [=============>................] - ETA: 6s - loss: 11.6225 - accuracy: 0.9796149/313 [=============>................] - ETA: 6s - loss: 11.8628 - accuracy: 0.9794151/313 [=============>................] - ETA: 6s - loss: 11.8492 - accuracy: 0.9793153/313 [=============>................] - ETA: 6s - loss: 12.1247 - accuracy: 0.9792155/313 [=============>................] - ETA: 6s - loss: 12.0098 - accuracy: 0.9792157/313 [==============>...............] - ETA: 6s - loss: 11.8568 - accuracy: 0.9795159/313 [==============>...............] - ETA: 6s - loss: 11.7077 - accuracy: 0.9798161/313 [==============>...............] - ETA: 6s - loss: 11.5622 - accuracy: 0.9800163/313 [==============>...............] - ETA: 6s - loss: 11.4204 - accuracy: 0.9803165/313 [==============>...............] - ETA: 5s - loss: 11.3703 - accuracy: 0.9803167/313 [===============>..............] - ETA: 5s - loss: 11.2341 - accuracy: 0.9805169/313 [===============>..............] - ETA: 5s - loss: 11.1012 - accuracy: 0.9808171/313 [===============>..............] - ETA: 5s - loss: 10.9714 - accuracy: 0.9810173/313 [===============>..............] - ETA: 5s - loss: 10.8445 - accuracy: 0.9812175/313 [===============>..............] - ETA: 5s - loss: 10.7206 - accuracy: 0.9814177/313 [===============>..............] - ETA: 5s - loss: 10.6571 - accuracy: 0.9813179/313 [================>.............] - ETA: 5s - loss: 10.5380 - accuracy: 0.9815181/313 [================>.............] - ETA: 5s - loss: 10.4216 - accuracy: 0.9817183/313 [================>.............] - ETA: 5s - loss: 10.3077 - accuracy: 0.9819185/313 [================>.............] - ETA: 5s - loss: 10.2538 - accuracy: 0.9819187/313 [================>.............] - ETA: 5s - loss: 10.5869 - accuracy: 0.9816189/313 [=================>............] - ETA: 4s - loss: 10.5686 - accuracy: 0.9816191/313 [=================>............] - ETA: 4s - loss: 10.6158 - accuracy: 0.9813193/313 [=================>............] - ETA: 4s - loss: 10.5927 - accuracy: 0.9811195/313 [=================>............] - ETA: 4s - loss: 10.4840 - accuracy: 0.9812197/313 [=================>............] - ETA: 4s - loss: 10.3776 - accuracy: 0.9814199/313 [==================>...........] - ETA: 4s - loss: 10.2733 - accuracy: 0.9816201/313 [==================>...........] - ETA: 4s - loss: 10.1711 - accuracy: 0.9818203/313 [==================>...........] - ETA: 4s - loss: 10.0708 - accuracy: 0.9820205/313 [==================>...........] - ETA: 4s - loss: 10.0404 - accuracy: 0.9820207/313 [==================>...........] - ETA: 4s - loss: 9.9591 - accuracy: 0.9819 209/313 [===================>..........] - ETA: 4s - loss: 10.0110 - accuracy: 0.9818211/313 [===================>..........] - ETA: 4s - loss: 9.9518 - accuracy: 0.9818 213/313 [===================>..........] - ETA: 3s - loss: 9.8584 - accuracy: 0.9820215/313 [===================>..........] - ETA: 3s - loss: 9.7667 - accuracy: 0.9821217/313 [===================>..........] - ETA: 3s - loss: 9.6767 - accuracy: 0.9823219/313 [===================>..........] - ETA: 3s - loss: 9.5883 - accuracy: 0.9824221/313 [====================>.........] - ETA: 3s - loss: 9.5015 - accuracy: 0.9826223/313 [====================>.........] - ETA: 3s - loss: 9.4163 - accuracy: 0.9828225/313 [====================>.........] - ETA: 3s - loss: 9.3326 - accuracy: 0.9829227/313 [====================>.........] - ETA: 3s - loss: 9.2504 - accuracy: 0.9831229/313 [====================>.........] - ETA: 3s - loss: 9.1696 - accuracy: 0.9832231/313 [=====================>........] - ETA: 3s - loss: 9.0902 - accuracy: 0.9834233/313 [=====================>........] - ETA: 3s - loss: 9.0122 - accuracy: 0.9835235/313 [=====================>........] - ETA: 3s - loss: 8.9355 - accuracy: 0.9836237/313 [=====================>........] - ETA: 2s - loss: 8.8601 - accuracy: 0.9838239/313 [=====================>........] - ETA: 2s - loss: 8.7859 - accuracy: 0.9839241/313 [======================>.......] - ETA: 2s - loss: 8.7130 - accuracy: 0.9841243/313 [======================>.......] - ETA: 2s - loss: 8.6413 - accuracy: 0.9842245/313 [======================>.......] - ETA: 2s - loss: 8.5708 - accuracy: 0.9843247/313 [======================>.......] - ETA: 2s - loss: 8.5661 - accuracy: 0.9842249/313 [======================>.......] - ETA: 2s - loss: 8.6497 - accuracy: 0.9841251/313 [=======================>......] - ETA: 2s - loss: 8.5808 - accuracy: 0.9842253/313 [=======================>......] - ETA: 2s - loss: 8.6709 - accuracy: 0.9841255/313 [=======================>......] - ETA: 2s - loss: 8.6029 - accuracy: 0.9842257/313 [=======================>......] - ETA: 2s - loss: 8.5360 - accuracy: 0.9843259/313 [=======================>......] - ETA: 2s - loss: 8.4769 - accuracy: 0.9843261/313 [========================>.....] - ETA: 2s - loss: 8.4120 - accuracy: 0.9844263/313 [========================>.....] - ETA: 1s - loss: 8.3674 - accuracy: 0.9844265/313 [========================>.....] - ETA: 1s - loss: 8.3043 - accuracy: 0.9846267/313 [========================>.....] - ETA: 1s - loss: 8.2421 - accuracy: 0.9847269/313 [========================>.....] - ETA: 1s - loss: 8.1808 - accuracy: 0.9848271/313 [========================>.....] - ETA: 1s - loss: 8.1204 - accuracy: 0.9849273/313 [=========================>....] - ETA: 1s - loss: 8.0609 - accuracy: 0.9850275/313 [=========================>....] - ETA: 1s - loss: 8.0023 - accuracy: 0.9851277/313 [=========================>....] - ETA: 1s - loss: 7.9445 - accuracy: 0.9852279/313 [=========================>....] - ETA: 1s - loss: 7.8876 - accuracy: 0.9853281/313 [=========================>....] - ETA: 1s - loss: 7.8314 - accuracy: 0.9854283/313 [==========================>...] - ETA: 1s - loss: 8.3219 - accuracy: 0.9851285/313 [==========================>...] - ETA: 1s - loss: 8.2635 - accuracy: 0.9852287/313 [==========================>...] - ETA: 1s - loss: 8.2059 - accuracy: 0.9853289/313 [==========================>...] - ETA: 0s - loss: 8.1491 - accuracy: 0.9854291/313 [==========================>...] - ETA: 0s - loss: 8.0931 - accuracy: 0.9855293/313 [===========================>..] - ETA: 0s - loss: 8.0378 - accuracy: 0.9856295/313 [===========================>..] - ETA: 0s - loss: 7.9833 - accuracy: 0.9857297/313 [===========================>..] - ETA: 0s - loss: 7.9296 - accuracy: 0.9858299/313 [===========================>..] - ETA: 0s - loss: 7.9395 - accuracy: 0.9858301/313 [===========================>..] - ETA: 0s - loss: 7.9230 - accuracy: 0.9858303/313 [============================>.] - ETA: 0s - loss: 7.9630 - accuracy: 0.9856305/313 [============================>.] - ETA: 0s - loss: 8.0293 - accuracy: 0.9856307/313 [============================>.] - ETA: 0s - loss: 8.1278 - accuracy: 0.9855309/313 [============================>.] - ETA: 0s - loss: 8.0920 - accuracy: 0.9855311/313 [============================>.] - ETA: 0s - loss: 8.0480 - accuracy: 0.9855313/313 [==============================] - ETA: 0s - loss: 8.0094 - accuracy: 0.9856313/313 [==============================] - 13s 39ms/step - loss: 8.0094 - accuracy: 0.9856
Final model accuracy: 0.985600
Total training time: 237.486680
2021-12-03 21:06:16.916598: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 21:06:16.916776: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-12-03 21:06:16.940116: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 21:06:16.940238: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-12-03 21:06:21.057595: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 21:06:21.057720: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2021-12-03 21:06:21.057812: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (node-0005): /proc/driver/nvidia/version does not exist
2021-12-03 21:06:21.109365: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 21:06:21.109585: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2021-12-03 21:06:21.109661: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (node-0001): /proc/driver/nvidia/version does not exist
===========
rank: 0
Batch size: 16
Total number of batches: 4000
Total training examples: 64000
Num of workers: 2
===========
Step #0 (total examples = 0)	Loss: 2.308785
Step #10 (total examples = 160)	Loss: 1.246184
Step #20 (total examples = 320)	Loss: 0.975048
Step #30 (total examples = 480)	Loss: 0.636039
Step #40 (total examples = 640)	Loss: 0.487721
Step #50 (total examples = 800)	Loss: 0.413577
Step #60 (total examples = 960)	Loss: 0.141230
Step #70 (total examples = 1120)	Loss: 1.041306
Step #80 (total examples = 1280)	Loss: 0.182375
Step #90 (total examples = 1440)	Loss: 0.153929
Step #100 (total examples = 1600)	Loss: 0.080368
Step #110 (total examples = 1760)	Loss: 0.553565
Step #120 (total examples = 1920)	Loss: 0.279758
Step #130 (total examples = 2080)	Loss: 0.027885
Step #140 (total examples = 2240)	Loss: 0.308415
Step #150 (total examples = 2400)	Loss: 0.378552
Step #160 (total examples = 2560)	Loss: 0.674986
Step #170 (total examples = 2720)	Loss: 0.162789
Step #180 (total examples = 2880)	Loss: 0.023716
Step #190 (total examples = 3040)	Loss: 0.019422
Step #200 (total examples = 3200)	Loss: 0.036597
Step #210 (total examples = 3360)	Loss: 0.037561
Step #220 (total examples = 3520)	Loss: 0.348887
Step #230 (total examples = 3680)	Loss: 0.138826
Step #240 (total examples = 3840)	Loss: 0.044182
Step #250 (total examples = 4000)	Loss: 0.027453
Step #260 (total examples = 4160)	Loss: 0.244097
Step #270 (total examples = 4320)	Loss: 0.322573
Step #280 (total examples = 4480)	Loss: 0.019752
Step #290 (total examples = 4640)	Loss: 0.088452
Step #300 (total examples = 4800)	Loss: 0.088385
Step #310 (total examples = 4960)	Loss: 0.227471
Step #320 (total examples = 5120)	Loss: 0.518027
Step #330 (total examples = 5280)	Loss: 0.020881
Step #340 (total examples = 5440)	Loss: 0.084948
Step #350 (total examples = 5600)	Loss: 0.808365
Step #360 (total examples = 5760)	Loss: 0.303939
Step #370 (total examples = 5920)	Loss: 0.760404
Step #380 (total examples = 6080)	Loss: 0.031217
Step #390 (total examples = 6240)	Loss: 0.279418
Step #400 (total examples = 6400)	Loss: 0.120733
Step #410 (total examples = 6560)	Loss: 0.106938
Step #420 (total examples = 6720)	Loss: 0.112380
Step #430 (total examples = 6880)	Loss: 1.133290
Step #440 (total examples = 7040)	Loss: 0.097791
Step #450 (total examples = 7200)	Loss: 0.144326
Step #460 (total examples = 7360)	Loss: 0.047576
Step #470 (total examples = 7520)	Loss: 0.134037
Step #480 (total examples = 7680)	Loss: 0.193509
Step #490 (total examples = 7840)	Loss: 0.021623
Step #500 (total examples = 8000)	Loss: 0.050326
Step #510 (total examples = 8160)	Loss: 0.163027
Step #520 (total examples = 8320)	Loss: 0.029508
Step #530 (total examples = 8480)	Loss: 0.009151
Step #540 (total examples = 8640)	Loss: 0.065627
Step #550 (total examples = 8800)	Loss: 0.071373
Step #560 (total examples = 8960)	Loss: 0.521996
Step #570 (total examples = 9120)	Loss: 0.042208
Step #580 (total examples = 9280)	Loss: 0.121252
Step #590 (total examples = 9440)	Loss: 0.059886
Step #600 (total examples = 9600)	Loss: 0.024555
Step #610 (total examples = 9760)	Loss: 0.032114
Step #620 (total examples = 9920)	Loss: 0.022599
Step #630 (total examples = 10080)	Loss: 0.014562
Step #640 (total examples = 10240)	Loss: 0.023094
Step #650 (total examples = 10400)	Loss: 0.036418
Step #660 (total examples = 10560)	Loss: 0.004549
Step #670 (total examples = 10720)	Loss: 0.020447
Step #680 (total examples = 10880)	Loss: 0.039657
Step #690 (total examples = 11040)	Loss: 0.332252
Step #700 (total examples = 11200)	Loss: 0.123693
Step #710 (total examples = 11360)	Loss: 0.246358
Step #720 (total examples = 11520)	Loss: 0.058240
Step #730 (total examples = 11680)	Loss: 0.648278
Step #740 (total examples = 11840)	Loss: 0.033046
Step #750 (total examples = 12000)	Loss: 0.134623
Step #760 (total examples = 12160)	Loss: 0.140789
Step #770 (total examples = 12320)	Loss: 0.134486
Step #780 (total examples = 12480)	Loss: 0.013686
Step #790 (total examples = 12640)	Loss: 0.273882
Step #800 (total examples = 12800)	Loss: 0.003466
Step #810 (total examples = 12960)	Loss: 0.086218
Step #820 (total examples = 13120)	Loss: 0.364010
Step #830 (total examples = 13280)	Loss: 0.122663
Step #840 (total examples = 13440)	Loss: 0.004868
Step #850 (total examples = 13600)	Loss: 0.016316
Step #860 (total examples = 13760)	Loss: 0.184068
Step #870 (total examples = 13920)	Loss: 0.138295
Step #880 (total examples = 14080)	Loss: 0.129329
Step #890 (total examples = 14240)	Loss: 0.132591
Step #900 (total examples = 14400)	Loss: 0.130005
Step #910 (total examples = 14560)	Loss: 0.114025
Step #920 (total examples = 14720)	Loss: 0.019998
Step #930 (total examples = 14880)	Loss: 0.058123
Step #940 (total examples = 15040)	Loss: 0.018395
Step #950 (total examples = 15200)	Loss: 0.051532
Step #960 (total examples = 15360)	Loss: 0.016881
Step #970 (total examples = 15520)	Loss: 0.005925
Step #980 (total examples = 15680)	Loss: 0.022896
Step #990 (total examples = 15840)	Loss: 0.045518
Step #1000 (total examples = 16000)	Loss: 0.022812
Step #1010 (total examples = 16160)	Loss: 0.003301
Step #1020 (total examples = 16320)	Loss: 0.064241
Step #1030 (total examples = 16480)	Loss: 0.061718
Step #1040 (total examples = 16640)	Loss: 0.025242
Step #1050 (total examples = 16800)	Loss: 0.037794
Step #1060 (total examples = 16960)	Loss: 0.023416
Step #1070 (total examples = 17120)	Loss: 0.003006
Step #1080 (total examples = 17280)	Loss: 0.132538
Step #1090 (total examples = 17440)	Loss: 0.102197
Step #1100 (total examples = 17600)	Loss: 0.006723
Step #1110 (total examples = 17760)	Loss: 0.023310
Step #1120 (total examples = 17920)	Loss: 0.009846
Step #1130 (total examples = 18080)	Loss: 0.058669
Step #1140 (total examples = 18240)	Loss: 0.008059
Step #1150 (total examples = 18400)	Loss: 0.073107
Step #1160 (total examples = 18560)	Loss: 0.068739
Step #1170 (total examples = 18720)	Loss: 0.081503
Step #1180 (total examples = 18880)	Loss: 0.009453
Step #1190 (total examples = 19040)	Loss: 0.008602
Step #1200 (total examples = 19200)	Loss: 0.010432
Step #1210 (total examples = 19360)	Loss: 0.012440
Step #1220 (total examples = 19520)	Loss: 0.022996
Step #1230 (total examples = 19680)	Loss: 0.001863
Step #1240 (total examples = 19840)	Loss: 0.021357
Step #1250 (total examples = 20000)	Loss: 0.068314
Step #1260 (total examples = 20160)	Loss: 0.030121
Step #1270 (total examples = 20320)	Loss: 0.022707
Step #1280 (total examples = 20480)	Loss: 0.129834
Step #1290 (total examples = 20640)	Loss: 0.006114
Step #1300 (total examples = 20800)	Loss: 0.209210
Step #1310 (total examples = 20960)	Loss: 0.010343
Step #1320 (total examples = 21120)	Loss: 0.068895
Step #1330 (total examples = 21280)	Loss: 0.081391
Step #1340 (total examples = 21440)	Loss: 0.031672
Step #1350 (total examples = 21600)	Loss: 0.090520
Step #1360 (total examples = 21760)	Loss: 0.183905
Step #1370 (total examples = 21920)	Loss: 0.145735
Step #1380 (total examples = 22080)	Loss: 0.023374
Step #1390 (total examples = 22240)	Loss: 0.176077
Step #1400 (total examples = 22400)	Loss: 0.065241
Step #1410 (total examples = 22560)	Loss: 0.314519
Step #1420 (total examples = 22720)	Loss: 0.071025
Step #1430 (total examples = 22880)	Loss: 0.237599
Step #1440 (total examples = 23040)	Loss: 0.009244
Step #1450 (total examples = 23200)	Loss: 0.083734
Step #1460 (total examples = 23360)	Loss: 0.008583
Step #1470 (total examples = 23520)	Loss: 0.026188
Step #1480 (total examples = 23680)	Loss: 0.020424
Step #1490 (total examples = 23840)	Loss: 0.010091
Step #1500 (total examples = 24000)	Loss: 0.374001
Step #1510 (total examples = 24160)	Loss: 0.152545
Step #1520 (total examples = 24320)	Loss: 0.042830
Step #1530 (total examples = 24480)	Loss: 0.059440
Step #1540 (total examples = 24640)	Loss: 0.286266
Step #1550 (total examples = 24800)	Loss: 0.602028
Step #1560 (total examples = 24960)	Loss: 0.007254
Step #1570 (total examples = 25120)	Loss: 0.028647
Step #1580 (total examples = 25280)	Loss: 0.222282
Step #1590 (total examples = 25440)	Loss: 0.027917
Step #1600 (total examples = 25600)	Loss: 0.921405
Step #1610 (total examples = 25760)	Loss: 0.005677===========
rank: 1
===========
Step #0 (total examples = 0)	Loss: 2.294307
Step #10 (total examples = 160)	Loss: 1.331291
Step #20 (total examples = 320)	Loss: 0.805902
Step #30 (total examples = 480)	Loss: 0.401742
Step #40 (total examples = 640)	Loss: 1.227326
Step #50 (total examples = 800)	Loss: 0.352572
Step #60 (total examples = 960)	Loss: 0.382262
Step #70 (total examples = 1120)	Loss: 0.373423
Step #80 (total examples = 1280)	Loss: 1.629017
Step #90 (total examples = 1440)	Loss: 0.287345
Step #100 (total examples = 1600)	Loss: 0.583654
Step #110 (total examples = 1760)	Loss: 0.457708
Step #120 (total examples = 1920)	Loss: 0.119650
Step #130 (total examples = 2080)	Loss: 0.360413
Step #140 (total examples = 2240)	Loss: 0.122469
Step #150 (total examples = 2400)	Loss: 0.260937
Step #160 (total examples = 2560)	Loss: 0.154026
Step #170 (total examples = 2720)	Loss: 0.187999
Step #180 (total examples = 2880)	Loss: 0.404798
Step #190 (total examples = 3040)	Loss: 0.053410
Step #200 (total examples = 3200)	Loss: 0.247385
Step #210 (total examples = 3360)	Loss: 0.125560
Step #220 (total examples = 3520)	Loss: 0.164315
Step #230 (total examples = 3680)	Loss: 0.240805
Step #240 (total examples = 3840)	Loss: 0.120799
Step #250 (total examples = 4000)	Loss: 0.628112
Step #260 (total examples = 4160)	Loss: 0.143851
Step #270 (total examples = 4320)	Loss: 0.026535
Step #280 (total examples = 4480)	Loss: 0.061741
Step #290 (total examples = 4640)	Loss: 0.068056
Step #300 (total examples = 4800)	Loss: 0.156412
Step #310 (total examples = 4960)	Loss: 0.091424
Step #320 (total examples = 5120)	Loss: 0.158753
Step #330 (total examples = 5280)	Loss: 0.114473
Step #340 (total examples = 5440)	Loss: 0.079530
Step #350 (total examples = 5600)	Loss: 0.437489
Step #360 (total examples = 5760)	Loss: 0.090281
Step #370 (total examples = 5920)	Loss: 0.234127
Step #380 (total examples = 6080)	Loss: 0.078592
Step #390 (total examples = 6240)	Loss: 0.176692
Step #400 (total examples = 6400)	Loss: 0.144983
Step #410 (total examples = 6560)	Loss: 0.012865
Step #420 (total examples = 6720)	Loss: 0.098894
Step #430 (total examples = 6880)	Loss: 0.011955
Step #440 (total examples = 7040)	Loss: 0.146597
Step #450 (total examples = 7200)	Loss: 0.178844
Step #460 (total examples = 7360)	Loss: 0.376227
Step #470 (total examples = 7520)	Loss: 0.020684
Step #480 (total examples = 7680)	Loss: 0.352886
Step #490 (total examples = 7840)	Loss: 0.225045
Step #500 (total examples = 8000)	Loss: 0.433315
Step #510 (total examples = 8160)	Loss: 0.022541
Step #520 (total examples = 8320)	Loss: 0.123136
Step #530 (total examples = 8480)	Loss: 0.157334
Step #540 (total examples = 8640)	Loss: 0.195598
Step #550 (total examples = 8800)	Loss: 0.504727
Step #560 (total examples = 8960)	Loss: 0.011174
Step #570 (total examples = 9120)	Loss: 0.058165
Step #580 (total examples = 9280)	Loss: 0.032055
Step #590 (total examples = 9440)	Loss: 0.045956
Step #600 (total examples = 9600)	Loss: 0.081256
Step #610 (total examples = 9760)	Loss: 0.029139
Step #620 (total examples = 9920)	Loss: 0.044120
Step #630 (total examples = 10080)	Loss: 0.024674
Step #640 (total examples = 10240)	Loss: 0.099970
Step #650 (total examples = 10400)	Loss: 0.034561
Step #660 (total examples = 10560)	Loss: 0.057711
Step #670 (total examples = 10720)	Loss: 0.201038
Step #680 (total examples = 10880)	Loss: 0.080629
Step #690 (total examples = 11040)	Loss: 0.002842
Step #700 (total examples = 11200)	Loss: 0.109352
Step #710 (total examples = 11360)	Loss: 0.021944
Step #720 (total examples = 11520)	Loss: 0.077142
Step #730 (total examples = 11680)	Loss: 0.146714
Step #740 (total examples = 11840)	Loss: 0.008135
Step #750 (total examples = 12000)	Loss: 0.230534
Step #760 (total examples = 12160)	Loss: 0.079205
Step #770 (total examples = 12320)	Loss: 0.346654
Step #780 (total examples = 12480)	Loss: 0.029050
Step #790 (total examples = 12640)	Loss: 0.248671
Step #800 (total examples = 12800)	Loss: 0.055378
Step #810 (total examples = 12960)	Loss: 0.020547
Step #820 (total examples = 13120)	Loss: 0.190826
Step #830 (total examples = 13280)	Loss: 0.114714
Step #840 (total examples = 13440)	Loss: 0.166485
Step #850 (total examples = 13600)	Loss: 0.059311
Step #860 (total examples = 13760)	Loss: 0.039551
Step #870 (total examples = 13920)	Loss: 0.023612
Step #880 (total examples = 14080)	Loss: 0.067967
Step #890 (total examples = 14240)	Loss: 0.059792
Step #900 (total examples = 14400)	Loss: 0.012940
Step #910 (total examples = 14560)	Loss: 0.052354
Step #920 (total examples = 14720)	Loss: 0.028815
Step #930 (total examples = 14880)	Loss: 0.104479
Step #940 (total examples = 15040)	Loss: 0.192480
Step #950 (total examples = 15200)	Loss: 0.004314
Step #960 (total examples = 15360)	Loss: 0.003946
Step #970 (total examples = 15520)	Loss: 0.146166
Step #980 (total examples = 15680)	Loss: 0.022774
Step #990 (total examples = 15840)	Loss: 0.091804
Step #1000 (total examples = 16000)	Loss: 0.269535
Step #1010 (total examples = 16160)	Loss: 0.018883
Step #1020 (total examples = 16320)	Loss: 0.254393
Step #1030 (total examples = 16480)	Loss: 0.021465
Step #1040 (total examples = 16640)	Loss: 0.016941
Step #1050 (total examples = 16800)	Loss: 0.249033
Step #1060 (total examples = 16960)	Loss: 0.012621
Step #1070 (total examples = 17120)	Loss: 0.378239
Step #1080 (total examples = 17280)	Loss: 0.028972
Step #1090 (total examples = 17440)	Loss: 0.016925
Step #1100 (total examples = 17600)	Loss: 0.023755
Step #1110 (total examples = 17760)	Loss: 0.042921
Step #1120 (total examples = 17920)	Loss: 0.176014
Step #1130 (total examples = 18080)	Loss: 0.030648
Step #1140 (total examples = 18240)	Loss: 0.065954
Step #1150 (total examples = 18400)	Loss: 0.085683
Step #1160 (total examples = 18560)	Loss: 0.176602
Step #1170 (total examples = 18720)	Loss: 0.147625
Step #1180 (total examples = 18880)	Loss: 0.011751
Step #1190 (total examples = 19040)	Loss: 0.086023
Step #1200 (total examples = 19200)	Loss: 0.011023
Step #1210 (total examples = 19360)	Loss: 0.013871
Step #1220 (total examples = 19520)	Loss: 0.390249
Step #1230 (total examples = 19680)	Loss: 0.003008
Step #1240 (total examples = 19840)	Loss: 0.062789
Step #1250 (total examples = 20000)	Loss: 0.045967
Step #1260 (total examples = 20160)	Loss: 0.106025
Step #1270 (total examples = 20320)	Loss: 0.010981
Step #1280 (total examples = 20480)	Loss: 0.068252
Step #1290 (total examples = 20640)	Loss: 0.112080
Step #1300 (total examples = 20800)	Loss: 0.035023
Step #1310 (total examples = 20960)	Loss: 0.000839
Step #1320 (total examples = 21120)	Loss: 0.002423
Step #1330 (total examples = 21280)	Loss: 0.366336
Step #1340 (total examples = 21440)	Loss: 0.124593
Step #1350 (total examples = 21600)	Loss: 0.005667
Step #1360 (total examples = 21760)	Loss: 0.148992
Step #1370 (total examples = 21920)	Loss: 0.028549
Step #1380 (total examples = 22080)	Loss: 0.005220
Step #1390 (total examples = 22240)	Loss: 0.028565
Step #1400 (total examples = 22400)	Loss: 0.001890
Step #1410 (total examples = 22560)	Loss: 0.030267
Step #1420 (total examples = 22720)	Loss: 0.005349
Step #1430 (total examples = 22880)	Loss: 0.137320
Step #1440 (total examples = 23040)	Loss: 0.006576
Step #1450 (total examples = 23200)	Loss: 0.057348
Step #1460 (total examples = 23360)	Loss: 0.220907
Step #1470 (total examples = 23520)	Loss: 0.001639
Step #1480 (total examples = 23680)	Loss: 0.044752
Step #1490 (total examples = 23840)	Loss: 0.012520
Step #1500 (total examples = 24000)	Loss: 0.003609
Step #1510 (total examples = 24160)	Loss: 0.058127
Step #1520 (total examples = 24320)	Loss: 0.025326
Step #1530 (total examples = 24480)	Loss: 0.020335
Step #1540 (total examples = 24640)	Loss: 0.025659
Step #1550 (total examples = 24800)	Loss: 0.003146
Step #1560 (total examples = 24960)	Loss: 0.012165
Step #1570 (total examples = 25120)	Loss: 0.043013
Step #1580 (total examples = 25280)	Loss: 0.191906
Step #1590 (total examples = 25440)	Loss: 0.045410
Step #1600 (total examples = 25600)	Loss: 0.182945
Step #1610 (total examples = 25760)	Loss: 0.034157
Step #1620 (total examples = 25920)	Loss: 0.012082
Step #1630 (total examples = 26080)	Loss: 0.002091
Step #1640 (total examples = 26240)	Loss: 0.003310
Step #1650 (total examples = 26400)	Loss: 0.002240
Step #1660 (total examples = 26560)	Loss: 0.003530
Step #1670 (total examples = 26720)	Loss: 0.012549
Step #1680 (total examples = 26880)	Loss: 0.005837
Step #1690 (total examples = 27040)	Loss: 0.012433
Step #1700 (total examples = 27200)	Loss: 0.000857
Step #1710 (total examples = 27360)	Loss: 0.000890
Step #1720 (total examples = 27520)	Loss: 0.005179
Step #1730 (total examples = 27680)	Loss: 0.005515
Step #1740 (total examples = 27840)	Loss: 0.018729
Step #1750 (total examples = 28000)	Loss: 0.002741
Step #1760 (total examples = 28160)	Loss: 0.080688
Step #1770 (total examples = 28320)	Loss: 0.003452
Step #1780 (total examples = 28480)	Loss: 0.047801
Step #1790 (total examples = 28640)	Loss: 0.232444
Step #1800 (total examples = 28800)	Loss: 0.557198
Step #1810 (total examples = 28960)	Loss: 0.004407
Step #1820 (total examples = 29120)	Loss: 0.003699
Step #1830 (total examples = 29280)	Loss: 0.000068
Step #1840 (total examples = 29440)	Loss: 0.078156
Step #1850 (total examples = 29600)	Loss: 0.007005
Step #1860 (total examples = 29760)	Loss: 0.029367
Step #1870 (total examples = 29920)	Loss: 0.016573
Step #1880 (total examples = 30080)	Loss: 0.012842
Step #1890 (total examples = 30240)	Loss: 0.002448
Step #1900 (total examples = 30400)	Loss: 0.006448
Step #1910 (total examples = 30560)	Loss: 0.002066
Step #1920 (total examples = 30720)	Loss: 0.042151
Step #1930 (total examples = 30880)	Loss: 0.136558
Step #1940 (total examples = 31040)	Loss: 0.002607
Step #1950 (total examples = 31200)	Loss: 0.014816
Step #1960 (total examples = 31360)	Loss: 0.258677
Step #1970 (total examples = 31520)	Loss: 0.031080
Step #1980 (total examples = 31680)	Loss: 0.293446
Step #1990 (total examples = 31840)	Loss: 0.002783

Step #1620 (total examples = 25920)	Loss: 0.179158
Step #1630 (total examples = 26080)	Loss: 0.015675
Step #1640 (total examples = 26240)	Loss: 0.013876
Step #1650 (total examples = 26400)	Loss: 0.001978
Step #1660 (total examples = 26560)	Loss: 0.035356
Step #1670 (total examples = 26720)	Loss: 0.115262
Step #1680 (total examples = 26880)	Loss: 0.094001
Step #1690 (total examples = 27040)	Loss: 0.032964
Step #1700 (total examples = 27200)	Loss: 0.029775
Step #1710 (total examples = 27360)	Loss: 0.028109
Step #1720 (total examples = 27520)	Loss: 0.000912
Step #1730 (total examples = 27680)	Loss: 0.037789
Step #1740 (total examples = 27840)	Loss: 0.058446
Step #1750 (total examples = 28000)	Loss: 0.041516
Step #1760 (total examples = 28160)	Loss: 0.069459
Step #1770 (total examples = 28320)	Loss: 0.077698
Step #1780 (total examples = 28480)	Loss: 0.021718
Step #1790 (total examples = 28640)	Loss: 0.002479
Step #1800 (total examples = 28800)	Loss: 0.002310
Step #1810 (total examples = 28960)	Loss: 0.015355
Step #1820 (total examples = 29120)	Loss: 0.006482
Step #1830 (total examples = 29280)	Loss: 0.111021
Step #1840 (total examples = 29440)	Loss: 0.011544
Step #1850 (total examples = 29600)	Loss: 0.012149
Step #1860 (total examples = 29760)	Loss: 0.005650
Step #1870 (total examples = 29920)	Loss: 0.120707
Step #1880 (total examples = 30080)	Loss: 0.198779
Step #1890 (total examples = 30240)	Loss: 0.001698
Step #1900 (total examples = 30400)	Loss: 0.060320
Step #1910 (total examples = 30560)	Loss: 0.249284
Step #1920 (total examples = 30720)	Loss: 0.083901
Step #1930 (total examples = 30880)	Loss: 0.097741
Step #1940 (total examples = 31040)	Loss: 0.183629
Step #1950 (total examples = 31200)	Loss: 0.004131
Step #1960 (total examples = 31360)	Loss: 0.689974
Step #1970 (total examples = 31520)	Loss: 0.003359
Step #1980 (total examples = 31680)	Loss: 0.056434
Step #1990 (total examples = 31840)	Loss: 0.332984
  1/313 [..............................] - ETA: 2:21 - loss: 0.0000e+00 - accuracy: 1.0000  3/313 [..............................] - ETA: 11s - loss: 2.2293 - accuracy: 0.9896       5/313 [..............................] - ETA: 11s - loss: 1.8890 - accuracy: 0.9875  7/313 [..............................] - ETA: 13s - loss: 1.3493 - accuracy: 0.9911  8/313 [..............................] - ETA: 14s - loss: 1.1806 - accuracy: 0.9922  9/313 [..............................] - ETA: 15s - loss: 1.0494 - accuracy: 0.9931 10/313 [..............................] - ETA: 16s - loss: 0.9445 - accuracy: 0.9937 11/313 [>.............................] - ETA: 16s - loss: 4.5808 - accuracy: 0.9858 12/313 [>.............................] - ETA: 17s - loss: 4.1991 - accuracy: 0.9870 13/313 [>.............................] - ETA: 17s - loss: 3.8761 - accuracy: 0.9880 14/313 [>.............................] - ETA: 17s - loss: 5.4739 - accuracy: 0.9866 15/313 [>.............................] - ETA: 18s - loss: 5.1090 - accuracy: 0.9875 16/313 [>.............................] - ETA: 18s - loss: 6.4850 - accuracy: 0.9863 17/313 [>.............................] - ETA: 18s - loss: 6.1035 - accuracy: 0.9871 18/313 [>.............................] - ETA: 18s - loss: 5.7645 - accuracy: 0.9878 19/313 [>.............................] - ETA: 18s - loss: 6.8576 - accuracy: 0.9868 20/313 [>.............................] - ETA: 18s - loss: 6.5147 - accuracy: 0.9875 22/313 [=>............................] - ETA: 17s - loss: 7.1208 - accuracy: 0.9858 24/313 [=>............................] - ETA: 16s - loss: 8.6503 - accuracy: 0.9831 26/313 [=>............................] - ETA: 16s - loss: 8.4180 - accuracy: 0.9820 28/313 [=>............................] - ETA: 15s - loss: 8.0407 - accuracy: 0.9810 30/313 [=>............................] - ETA: 15s - loss: 7.9985 - accuracy: 0.9802 32/313 [==>...........................] - ETA: 14s - loss: 9.1597 - accuracy: 0.9795 34/313 [==>...........................] - ETA: 14s - loss: 9.0588 - accuracy: 0.9789 36/313 [==>...........................] - ETA: 14s - loss: 9.6300 - accuracy: 0.9774 38/313 [==>...........................] - ETA: 13s - loss: 9.1476 - accuracy: 0.9778 40/313 [==>...........................] - ETA: 13s - loss: 11.2152 - accuracy: 0.9750 42/313 [===>..........................] - ETA: 13s - loss: 11.2016 - accuracy: 0.9740 44/313 [===>..........................] - ETA: 13s - loss: 11.1768 - accuracy: 0.9744 46/313 [===>..........................] - ETA: 12s - loss: 10.6908 - accuracy: 0.9755 48/313 [===>..........................] - ETA: 12s - loss: 10.7808 - accuracy: 0.9753 50/313 [===>..........................] - ETA: 12s - loss: 10.5450 - accuracy: 0.9756 52/313 [===>..........................] - ETA: 12s - loss: 10.2856 - accuracy: 0.9754 54/313 [====>.........................] - ETA: 12s - loss: 11.3394 - accuracy: 0.9745 56/313 [====>.........................] - ETA: 11s - loss: 11.3604 - accuracy: 0.9743 58/313 [====>.........................] - ETA: 11s - loss: 11.3811 - accuracy: 0.9747 60/313 [====>.........................] - ETA: 11s - loss: 11.9634 - accuracy: 0.9740 62/313 [====>.........................] - ETA: 11s - loss: 11.5774 - accuracy: 0.9748 64/313 [=====>........................] - ETA: 11s - loss: 12.4300 - accuracy: 0.9736 66/313 [=====>........................] - ETA: 11s - loss: 12.7829 - accuracy: 0.9721 68/313 [=====>........................] - ETA: 10s - loss: 14.5363 - accuracy: 0.9710 70/313 [=====>........................] - ETA: 10s - loss: 14.3822 - accuracy: 0.9714 72/313 [=====>........................] - ETA: 10s - loss: 14.4384 - accuracy: 0.9718 74/313 [======>.......................] - ETA: 10s - loss: 14.0482 - accuracy: 0.9726 76/313 [======>.......................] - ETA: 10s - loss: 14.9648 - accuracy: 0.9716 78/313 [======>.......................] - ETA: 10s - loss: 15.6977 - accuracy: 0.9712 80/313 [======>.......................] - ETA: 10s - loss: 15.3053 - accuracy: 0.9719 82/313 [======>.......................] - ETA: 9s - loss: 15.6485 - accuracy: 0.9718  84/313 [=======>......................] - ETA: 9s - loss: 16.2116 - accuracy: 0.9721 86/313 [=======>......................] - ETA: 9s - loss: 15.9020 - accuracy: 0.9724 88/313 [=======>......................] - ETA: 9s - loss: 15.5406 - accuracy: 0.9730 90/313 [=======>......................] - ETA: 9s - loss: 15.1952 - accuracy: 0.9736 92/313 [=======>......................] - ETA: 9s - loss: 15.4936 - accuracy: 0.9732 94/313 [========>.....................] - ETA: 9s - loss: 15.5243 - accuracy: 0.9731 96/313 [========>.....................] - ETA: 9s - loss: 15.4525 - accuracy: 0.9730 98/313 [========>.....................] - ETA: 9s - loss: 15.2104 - accuracy: 0.9732100/313 [========>.....................] - ETA: 8s - loss: 14.9061 - accuracy: 0.9737102/313 [========>.....................] - ETA: 8s - loss: 14.6139 - accuracy: 0.9743104/313 [========>.....................] - ETA: 8s - loss: 14.3328 - accuracy: 0.9748106/313 [=========>....................] - ETA: 8s - loss: 14.3639 - accuracy: 0.9746108/313 [=========>....................] - ETA: 8s - loss: 14.2850 - accuracy: 0.9748110/313 [=========>....................] - ETA: 8s - loss: 14.4168 - accuracy: 0.9750112/313 [=========>....................] - ETA: 8s - loss: 15.1478 - accuracy: 0.9746114/313 [=========>....................] - ETA: 8s - loss: 14.9014 - accuracy: 0.9748116/313 [==========>...................] - ETA: 8s - loss: 14.9712 - accuracy: 0.9749118/313 [==========>...................] - ETA: 8s - loss: 14.9484 - accuracy: 0.9751120/313 [==========>...................] - ETA: 7s - loss: 15.0404 - accuracy: 0.9747122/313 [==========>...................] - ETA: 7s - loss: 15.0648 - accuracy: 0.9744124/313 [==========>...................] - ETA: 7s - loss: 14.8643 - accuracy: 0.9745126/313 [===========>..................] - ETA: 7s - loss: 14.6764 - accuracy: 0.9747128/313 [===========>..................] - ETA: 7s - loss: 14.4980 - accuracy: 0.9746130/313 [===========>..................] - ETA: 7s - loss: 14.2750 - accuracy: 0.9750132/313 [===========>..................] - ETA: 7s - loss: 14.3005 - accuracy: 0.9749134/313 [===========>..................] - ETA: 7s - loss: 14.3589 - accuracy: 0.9748136/313 [============>.................] - ETA: 7s - loss: 14.1478 - accuracy: 0.9752138/313 [============>.................] - ETA: 7s - loss: 13.9427 - accuracy: 0.9755140/313 [============>.................] - ETA: 7s - loss: 13.7435 - accuracy: 0.9759142/313 [============>.................] - ETA: 6s - loss: 13.7738 - accuracy: 0.9754144/313 [============>.................] - ETA: 6s - loss: 13.6427 - accuracy: 0.9755146/313 [============>.................] - ETA: 6s - loss: 13.4761 - accuracy: 0.9756148/313 [=============>................] - ETA: 6s - loss: 13.2940 - accuracy: 0.9759150/313 [=============>................] - ETA: 6s - loss: 13.4269 - accuracy: 0.9758152/313 [=============>................] - ETA: 6s - loss: 13.6711 - accuracy: 0.9753154/313 [=============>................] - ETA: 6s - loss: 13.4935 - accuracy: 0.9756156/313 [=============>................] - ETA: 6s - loss: 13.3205 - accuracy: 0.9760158/313 [==============>...............] - ETA: 6s - loss: 13.1519 - accuracy: 0.9763160/313 [==============>...............] - ETA: 6s - loss: 12.9875 - accuracy: 0.9766162/313 [==============>...............] - ETA: 6s - loss: 12.8272 - accuracy: 0.9769164/313 [==============>...............] - ETA: 5s - loss: 12.7302 - accuracy: 0.9769166/313 [==============>...............] - ETA: 5s - loss: 12.5822 - accuracy: 0.9770168/313 [===============>..............] - ETA: 5s - loss: 12.4590 - accuracy: 0.9771170/313 [===============>..............] - ETA: 5s - loss: 12.3124 - accuracy: 0.9774172/313 [===============>..............] - ETA: 5s - loss: 12.1692 - accuracy: 0.9777174/313 [===============>..............] - ETA: 5s - loss: 12.0293 - accuracy: 0.9779176/313 [===============>..............] - ETA: 5s - loss: 12.0451 - accuracy: 0.9780178/313 [================>.............] - ETA: 5s - loss: 11.9097 - accuracy: 0.9782180/313 [================>.............] - ETA: 5s - loss: 11.7774 - accuracy: 0.9785182/313 [================>.............] - ETA: 5s - loss: 11.6480 - accuracy: 0.9787184/313 [================>.............] - ETA: 5s - loss: 11.6649 - accuracy: 0.9786186/313 [================>.............] - ETA: 5s - loss: 11.5800 - accuracy: 0.9787188/313 [=================>............] - ETA: 4s - loss: 11.7130 - accuracy: 0.9787190/313 [=================>............] - ETA: 4s - loss: 11.6634 - accuracy: 0.9788192/313 [=================>............] - ETA: 4s - loss: 11.5908 - accuracy: 0.9788194/313 [=================>............] - ETA: 4s - loss: 11.5321 - accuracy: 0.9786196/313 [=================>............] - ETA: 4s - loss: 11.4145 - accuracy: 0.9788198/313 [=================>............] - ETA: 4s - loss: 11.2992 - accuracy: 0.9790200/313 [==================>...........] - ETA: 4s - loss: 11.1862 - accuracy: 0.9792202/313 [==================>...........] - ETA: 4s - loss: 11.0754 - accuracy: 0.9794204/313 [==================>...........] - ETA: 4s - loss: 11.2005 - accuracy: 0.9795206/313 [==================>...........] - ETA: 4s - loss: 11.2357 - accuracy: 0.9795208/313 [==================>...........] - ETA: 4s - loss: 11.3755 - accuracy: 0.9793210/313 [===================>..........] - ETA: 4s - loss: 11.2672 - accuracy: 0.9795212/313 [===================>..........] - ETA: 3s - loss: 11.2185 - accuracy: 0.9795214/313 [===================>..........] - ETA: 3s - loss: 11.1137 - accuracy: 0.9797216/313 [===================>..........] - ETA: 3s - loss: 11.0108 - accuracy: 0.9799218/313 [===================>..........] - ETA: 3s - loss: 10.9098 - accuracy: 0.9801220/313 [====================>.........] - ETA: 3s - loss: 10.8106 - accuracy: 0.9803222/313 [====================>.........] - ETA: 3s - loss: 10.7132 - accuracy: 0.9804224/313 [====================>.........] - ETA: 3s - loss: 10.6175 - accuracy: 0.9806226/313 [====================>.........] - ETA: 3s - loss: 10.5236 - accuracy: 0.9808228/313 [====================>.........] - ETA: 3s - loss: 10.4313 - accuracy: 0.9809230/313 [=====================>........] - ETA: 3s - loss: 10.3406 - accuracy: 0.9811232/313 [=====================>........] - ETA: 3s - loss: 10.2514 - accuracy: 0.9813234/313 [=====================>........] - ETA: 3s - loss: 10.2401 - accuracy: 0.9813236/313 [=====================>........] - ETA: 3s - loss: 10.1669 - accuracy: 0.9813238/313 [=====================>........] - ETA: 2s - loss: 10.0815 - accuracy: 0.9815240/313 [======================>.......] - ETA: 2s - loss: 9.9974 - accuracy: 0.9816 242/313 [======================>.......] - ETA: 2s - loss: 9.9148 - accuracy: 0.9818244/313 [======================>.......] - ETA: 2s - loss: 9.8336 - accuracy: 0.9819246/313 [======================>.......] - ETA: 2s - loss: 9.7807 - accuracy: 0.9818248/313 [======================>.......] - ETA: 2s - loss: 9.7115 - accuracy: 0.9819250/313 [======================>.......] - ETA: 2s - loss: 9.6379 - accuracy: 0.9819252/313 [=======================>......] - ETA: 2s - loss: 9.5619 - accuracy: 0.9819254/313 [=======================>......] - ETA: 2s - loss: 9.5229 - accuracy: 0.9819256/313 [=======================>......] - ETA: 2s - loss: 9.4485 - accuracy: 0.9821258/313 [=======================>......] - ETA: 2s - loss: 9.3753 - accuracy: 0.9822260/313 [=======================>......] - ETA: 2s - loss: 9.3031 - accuracy: 0.9823262/313 [========================>.....] - ETA: 1s - loss: 9.2321 - accuracy: 0.9825264/313 [========================>.....] - ETA: 1s - loss: 9.1622 - accuracy: 0.9826266/313 [========================>.....] - ETA: 1s - loss: 9.0933 - accuracy: 0.9827268/313 [========================>.....] - ETA: 1s - loss: 9.0254 - accuracy: 0.9829270/313 [========================>.....] - ETA: 1s - loss: 8.9586 - accuracy: 0.9830272/313 [=========================>....] - ETA: 1s - loss: 8.8927 - accuracy: 0.9831274/313 [=========================>....] - ETA: 1s - loss: 8.8278 - accuracy: 0.9832276/313 [=========================>....] - ETA: 1s - loss: 8.7638 - accuracy: 0.9834278/313 [=========================>....] - ETA: 1s - loss: 8.7008 - accuracy: 0.9835280/313 [=========================>....] - ETA: 1s - loss: 8.6386 - accuracy: 0.9836282/313 [==========================>...] - ETA: 1s - loss: 9.0991 - accuracy: 0.9834284/313 [==========================>...] - ETA: 1s - loss: 9.0732 - accuracy: 0.9834286/313 [==========================>...] - ETA: 1s - loss: 9.0098 - accuracy: 0.9835288/313 [==========================>...] - ETA: 0s - loss: 8.9472 - accuracy: 0.9836290/313 [==========================>...] - ETA: 0s - loss: 8.8855 - accuracy: 0.9837292/313 [==========================>...] - ETA: 0s - loss: 8.8246 - accuracy: 0.9838294/313 [===========================>..] - ETA: 0s - loss: 8.7646 - accuracy: 0.9839296/313 [===========================>..] - ETA: 0s - loss: 8.7054 - accuracy: 0.9841298/313 [===========================>..] - ETA: 0s - loss: 8.7721 - accuracy: 0.9841300/313 [===========================>..] - ETA: 0s - loss: 8.7137 - accuracy: 0.9842302/313 [===========================>..] - ETA: 0s - loss: 8.7154 - accuracy: 0.9841304/313 [============================>.] - ETA: 0s - loss: 8.7452 - accuracy: 0.9839306/313 [============================>.] - ETA: 0s - loss: 8.9889 - accuracy: 0.9838308/313 [============================>.] - ETA: 0s - loss: 8.9638 - accuracy: 0.9838310/313 [============================>.] - ETA: 0s - loss: 9.0312 - accuracy: 0.9837312/313 [============================>.] - ETA: 0s - loss: 9.0166 - accuracy: 0.9837313/313 [==============================] - 12s 39ms/step - loss: 9.0022 - accuracy: 0.9837
Final model accuracy: 0.983700
Total training time: 241.612433
