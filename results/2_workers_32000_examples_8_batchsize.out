2021-12-03 20:12:00.304641: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 20:12:00.304736: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-12-03 20:12:00.362818: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 20:12:00.363007: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-12-03 20:12:04.435013: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 20:12:04.435113: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2021-12-03 20:12:04.435166: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (node-0005): /proc/driver/nvidia/version does not exist
2021-12-03 20:12:04.578311: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 20:12:04.578514: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2021-12-03 20:12:04.578591: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (node-0001): /proc/driver/nvidia/version does not exist
===========
rank: 0
Batch size: 8
Total number of batches: 4000
Total training examples: 32000
Num of workers: 2
===========
Step #0 (total examples = 0)	Loss: 2.335255
Step #10 (total examples = 80)	Loss: 2.075814
Step #20 (total examples = 160)	Loss: 1.317591
Step #30 (total examples = 240)	Loss: 0.570578
Step #40 (total examples = 320)	Loss: 0.991363
Step #50 (total examples = 400)	Loss: 0.301467
Step #60 (total examples = 480)	Loss: 0.945328
Step #70 (total examples = 560)	Loss: 0.088555
Step #80 (total examples = 640)	Loss: 0.145268
Step #90 (total examples = 720)	Loss: 1.065983
Step #100 (total examples = 800)	Loss: 0.738214
Step #110 (total examples = 880)	Loss: 0.284443
Step #120 (total examples = 960)	Loss: 0.317815
Step #130 (total examples = 1040)	Loss: 0.059403
Step #140 (total examples = 1120)	Loss: 0.883536
Step #150 (total examples = 1200)	Loss: 0.266008
Step #160 (total examples = 1280)	Loss: 0.286198
Step #170 (total examples = 1360)	Loss: 0.094892
Step #180 (total examples = 1440)	Loss: 0.066530
Step #190 (total examples = 1520)	Loss: 0.160479
Step #200 (total examples = 1600)	Loss: 0.446688
Step #210 (total examples = 1680)	Loss: 0.049076
Step #220 (total examples = 1760)	Loss: 0.461119
Step #230 (total examples = 1840)	Loss: 0.041566
Step #240 (total examples = 1920)	Loss: 0.013508
Step #250 (total examples = 2000)	Loss: 0.033689
Step #260 (total examples = 2080)	Loss: 0.599950
Step #270 (total examples = 2160)	Loss: 0.108907
Step #280 (total examples = 2240)	Loss: 0.146701
Step #290 (total examples = 2320)	Loss: 0.117750
Step #300 (total examples = 2400)	Loss: 0.218832
Step #310 (total examples = 2480)	Loss: 0.007183
Step #320 (total examples = 2560)	Loss: 0.351123
Step #330 (total examples = 2640)	Loss: 0.046490
Step #340 (total examples = 2720)	Loss: 0.085917
Step #350 (total examples = 2800)	Loss: 0.137655
Step #360 (total examples = 2880)	Loss: 0.045569
Step #370 (total examples = 2960)	Loss: 0.095717
Step #380 (total examples = 3040)	Loss: 0.462525
Step #390 (total examples = 3120)	Loss: 1.013728
Step #400 (total examples = 3200)	Loss: 0.083555
Step #410 (total examples = 3280)	Loss: 0.663898
Step #420 (total examples = 3360)	Loss: 0.012942
Step #430 (total examples = 3440)	Loss: 0.003180
Step #440 (total examples = 3520)	Loss: 0.089026
Step #450 (total examples = 3600)	Loss: 0.620070
Step #460 (total examples = 3680)	Loss: 0.018227
Step #470 (total examples = 3760)	Loss: 0.365049
Step #480 (total examples = 3840)	Loss: 0.015131
Step #490 (total examples = 3920)	Loss: 0.684677
Step #500 (total examples = 4000)	Loss: 0.014215
Step #510 (total examples = 4080)	Loss: 0.028495
Step #520 (total examples = 4160)	Loss: 0.130009
Step #530 (total examples = 4240)	Loss: 0.037341
Step #540 (total examples = 4320)	Loss: 0.049762
Step #550 (total examples = 4400)	Loss: 0.009963
Step #560 (total examples = 4480)	Loss: 0.058461
Step #570 (total examples = 4560)	Loss: 0.685984
Step #580 (total examples = 4640)	Loss: 0.083288
Step #590 (total examples = 4720)	Loss: 0.615065
Step #600 (total examples = 4800)	Loss: 0.259759
Step #610 (total examples = 4880)	Loss: 0.040260
Step #620 (total examples = 4960)	Loss: 0.015472
Step #630 (total examples = 5040)	Loss: 0.008264
Step #640 (total examples = 5120)	Loss: 0.236216
Step #650 (total examples = 5200)	Loss: 0.910255
Step #660 (total examples = 5280)	Loss: 0.149830
Step #670 (total examples = 5360)	Loss: 0.217093
Step #680 (total examples = 5440)	Loss: 0.099943
Step #690 (total examples = 5520)	Loss: 0.045363
Step #700 (total examples = 5600)	Loss: 0.632121
Step #710 (total examples = 5680)	Loss: 0.003835
Step #720 (total examples = 5760)	Loss: 0.010174
Step #730 (total examples = 5840)	Loss: 0.141032
Step #740 (total examples = 5920)	Loss: 0.223688
Step #750 (total examples = 6000)	Loss: 0.118903
Step #760 (total examples = 6080)	Loss: 0.007373
Step #770 (total examples = 6160)	Loss: 0.118116
Step #780 (total examples = 6240)	Loss: 0.144655
Step #790 (total examples = 6320)	Loss: 0.035377
Step #800 (total examples = 6400)	Loss: 0.109801
Step #810 (total examples = 6480)	Loss: 0.013874
Step #820 (total examples = 6560)	Loss: 0.072991
Step #830 (total examples = 6640)	Loss: 0.041428
Step #840 (total examples = 6720)	Loss: 0.002446
Step #850 (total examples = 6800)	Loss: 0.000620
Step #860 (total examples = 6880)	Loss: 0.159450
Step #870 (total examples = 6960)	Loss: 0.037394
Step #880 (total examples = 7040)	Loss: 0.287766
Step #890 (total examples = 7120)	Loss: 0.725932
Step #900 (total examples = 7200)	Loss: 0.311809
Step #910 (total examples = 7280)	Loss: 0.019757
Step #920 (total examples = 7360)	Loss: 0.006265
Step #930 (total examples = 7440)	Loss: 0.583191
Step #940 (total examples = 7520)	Loss: 0.565563
Step #950 (total examples = 7600)	Loss: 0.106260
Step #960 (total examples = 7680)	Loss: 0.049403
Step #970 (total examples = 7760)	Loss: 0.431448
Step #980 (total examples = 7840)	Loss: 0.019709
Step #990 (total examples = 7920)	Loss: 0.014609
Step #1000 (total examples = 8000)	Loss: 0.004553
Step #1010 (total examples = 8080)	Loss: 0.100095
Step #1020 (total examples = 8160)	Loss: 0.009756
Step #1030 (total examples = 8240)	Loss: 0.241685
Step #1040 (total examples = 8320)	Loss: 0.018089
Step #1050 (total examples = 8400)	Loss: 0.009029
Step #1060 (total examples = 8480)	Loss: 0.039358
Step #1070 (total examples = 8560)	Loss: 0.003401
Step #1080 (total examples = 8640)	Loss: 0.241881
Step #1090 (total examples = 8720)	Loss: 0.140850
Step #1100 (total examples = 8800)	Loss: 0.353263
Step #1110 (total examples = 8880)	Loss: 0.374089
Step #1120 (total examples = 8960)	Loss: 0.003980
Step #1130 (total examples = 9040)	Loss: 0.048907
Step #1140 (total examples = 9120)	Loss: 0.108161
Step #1150 (total examples = 9200)	Loss: 0.048648
Step #1160 (total examples = 9280)	Loss: 0.492417
Step #1170 (total examples = 9360)	Loss: 0.000946
Step #1180 (total examples = 9440)	Loss: 0.038361
Step #1190 (total examples = 9520)	Loss: 0.000944
Step #1200 (total examples = 9600)	Loss: 0.124550
Step #1210 (total examples = 9680)	Loss: 0.021412
Step #1220 (total examples = 9760)	Loss: 0.067095
Step #1230 (total examples = 9840)	Loss: 0.115601
Step #1240 (total examples = 9920)	Loss: 0.048607
Step #1250 (total examples = 10000)	Loss: 0.222785
Step #1260 (total examples = 10080)	Loss: 0.529742
Step #1270 (total examples = 10160)	Loss: 0.250255
Step #1280 (total examples = 10240)	Loss: 0.000409
Step #1290 (total examples = 10320)	Loss: 0.002311
Step #1300 (total examples = 10400)	Loss: 0.012475
Step #1310 (total examples = 10480)	Loss: 0.000318
Step #1320 (total examples = 10560)	Loss: 0.359185
Step #1330 (total examples = 10640)	Loss: 0.196472
Step #1340 (total examples = 10720)	Loss: 0.089745
Step #1350 (total examples = 10800)	Loss: 0.056359
Step #1360 (total examples = 10880)	Loss: 0.014480
Step #1370 (total examples = 10960)	Loss: 0.017918
Step #1380 (total examples = 11040)	Loss: 0.023340
Step #1390 (total examples = 11120)	Loss: 0.005819
Step #1400 (total examples = 11200)	Loss: 0.007364
Step #1410 (total examples = 11280)	Loss: 0.166187
Step #1420 (total examples = 11360)	Loss: 0.032843
Step #1430 (total examples = 11440)	Loss: 0.048918
Step #1440 (total examples = 11520)	Loss: 0.104709
Step #1450 (total examples = 11600)	Loss: 0.137787
Step #1460 (total examples = 11680)	Loss: 0.873605
Step #1470 (total examples = 11760)	Loss: 0.490310
Step #1480 (total examples = 11840)	Loss: 0.180416
Step #1490 (total examples = 11920)	Loss: 0.148038
Step #1500 (total examples = 12000)	Loss: 0.004355
Step #1510 (total examples = 12080)	Loss: 0.001829
Step #1520 (total examples = 12160)	Loss: 0.310110
Step #1530 (total examples = 12240)	Loss: 0.389119
Step #1540 (total examples = 12320)	Loss: 0.026782
Step #1550 (total examples = 12400)	Loss: 0.042933
Step #1560 (total examples = 12480)	Loss: 0.016462
Step #1570 (total examples = 12560)	Loss: 0.019442
Step #1580 (total examples = 12640)	Loss: 0.106081
Step #1590 (total examples = 12720)	Loss: 0.012161
Step #1600 (total examples = 12800)	Loss: 0.020279
Step #1610 (total examples = 12880)	Loss: 0.023974
Step #1620 (total examples = 12960)	Loss: 0.006079
Step #1630 (total examples = 13040)	Loss: 0.020383===========
rank: 1
===========
Step #0 (total examples = 0)	Loss: 2.280246
Step #10 (total examples = 80)	Loss: 2.061791
Step #20 (total examples = 160)	Loss: 1.754671
Step #30 (total examples = 240)	Loss: 0.693128
Step #40 (total examples = 320)	Loss: 0.445502
Step #50 (total examples = 400)	Loss: 0.306312
Step #60 (total examples = 480)	Loss: 0.308153
Step #70 (total examples = 560)	Loss: 0.765101
Step #80 (total examples = 640)	Loss: 1.013352
Step #90 (total examples = 720)	Loss: 0.404124
Step #100 (total examples = 800)	Loss: 0.023623
Step #110 (total examples = 880)	Loss: 0.591759
Step #120 (total examples = 960)	Loss: 0.666615
Step #130 (total examples = 1040)	Loss: 0.085585
Step #140 (total examples = 1120)	Loss: 0.485705
Step #150 (total examples = 1200)	Loss: 0.192673
Step #160 (total examples = 1280)	Loss: 0.222390
Step #170 (total examples = 1360)	Loss: 0.256198
Step #180 (total examples = 1440)	Loss: 0.208320
Step #190 (total examples = 1520)	Loss: 0.009781
Step #200 (total examples = 1600)	Loss: 0.943075
Step #210 (total examples = 1680)	Loss: 0.415665
Step #220 (total examples = 1760)	Loss: 0.030222
Step #230 (total examples = 1840)	Loss: 0.147438
Step #240 (total examples = 1920)	Loss: 0.318675
Step #250 (total examples = 2000)	Loss: 0.262810
Step #260 (total examples = 2080)	Loss: 0.441568
Step #270 (total examples = 2160)	Loss: 0.098376
Step #280 (total examples = 2240)	Loss: 0.383729
Step #290 (total examples = 2320)	Loss: 0.521551
Step #300 (total examples = 2400)	Loss: 0.133479
Step #310 (total examples = 2480)	Loss: 0.209834
Step #320 (total examples = 2560)	Loss: 0.107916
Step #330 (total examples = 2640)	Loss: 0.253864
Step #340 (total examples = 2720)	Loss: 0.011325
Step #350 (total examples = 2800)	Loss: 0.079680
Step #360 (total examples = 2880)	Loss: 0.184694
Step #370 (total examples = 2960)	Loss: 0.031165
Step #380 (total examples = 3040)	Loss: 0.008273
Step #390 (total examples = 3120)	Loss: 0.036422
Step #400 (total examples = 3200)	Loss: 0.335013
Step #410 (total examples = 3280)	Loss: 0.396335
Step #420 (total examples = 3360)	Loss: 0.279909
Step #430 (total examples = 3440)	Loss: 0.071439
Step #440 (total examples = 3520)	Loss: 0.261993
Step #450 (total examples = 3600)	Loss: 0.009248
Step #460 (total examples = 3680)	Loss: 0.079874
Step #470 (total examples = 3760)	Loss: 0.172089
Step #480 (total examples = 3840)	Loss: 0.341000
Step #490 (total examples = 3920)	Loss: 0.036177
Step #500 (total examples = 4000)	Loss: 0.485321
Step #510 (total examples = 4080)	Loss: 0.012418
Step #520 (total examples = 4160)	Loss: 0.139267
Step #530 (total examples = 4240)	Loss: 0.281708
Step #540 (total examples = 4320)	Loss: 0.023528
Step #550 (total examples = 4400)	Loss: 0.040373
Step #560 (total examples = 4480)	Loss: 0.182370
Step #570 (total examples = 4560)	Loss: 0.129383
Step #580 (total examples = 4640)	Loss: 0.057974
Step #590 (total examples = 4720)	Loss: 0.295365
Step #600 (total examples = 4800)	Loss: 0.454920
Step #610 (total examples = 4880)	Loss: 0.062153
Step #620 (total examples = 4960)	Loss: 0.091444
Step #630 (total examples = 5040)	Loss: 0.033331
Step #640 (total examples = 5120)	Loss: 0.153788
Step #650 (total examples = 5200)	Loss: 0.056641
Step #660 (total examples = 5280)	Loss: 0.655496
Step #670 (total examples = 5360)	Loss: 0.064343
Step #680 (total examples = 5440)	Loss: 0.035813
Step #690 (total examples = 5520)	Loss: 0.017132
Step #700 (total examples = 5600)	Loss: 0.245356
Step #710 (total examples = 5680)	Loss: 0.032427
Step #720 (total examples = 5760)	Loss: 0.011446
Step #730 (total examples = 5840)	Loss: 0.051199
Step #740 (total examples = 5920)	Loss: 0.095606
Step #750 (total examples = 6000)	Loss: 0.259806
Step #760 (total examples = 6080)	Loss: 0.010279
Step #770 (total examples = 6160)	Loss: 0.031634
Step #780 (total examples = 6240)	Loss: 0.357123
Step #790 (total examples = 6320)	Loss: 0.070708
Step #800 (total examples = 6400)	Loss: 0.274859
Step #810 (total examples = 6480)	Loss: 0.002216
Step #820 (total examples = 6560)	Loss: 0.008661
Step #830 (total examples = 6640)	Loss: 0.019583
Step #840 (total examples = 6720)	Loss: 0.037367
Step #850 (total examples = 6800)	Loss: 0.009542
Step #860 (total examples = 6880)	Loss: 0.061307
Step #870 (total examples = 6960)	Loss: 0.147289
Step #880 (total examples = 7040)	Loss: 0.042215
Step #890 (total examples = 7120)	Loss: 0.483485
Step #900 (total examples = 7200)	Loss: 0.044276
Step #910 (total examples = 7280)	Loss: 0.053104
Step #920 (total examples = 7360)	Loss: 0.837951
Step #930 (total examples = 7440)	Loss: 0.008995
Step #940 (total examples = 7520)	Loss: 0.182667
Step #950 (total examples = 7600)	Loss: 0.560780
Step #960 (total examples = 7680)	Loss: 0.363781
Step #970 (total examples = 7760)	Loss: 0.031114
Step #980 (total examples = 7840)	Loss: 0.222603
Step #990 (total examples = 7920)	Loss: 0.228926
Step #1000 (total examples = 8000)	Loss: 0.282843
Step #1010 (total examples = 8080)	Loss: 0.086167
Step #1020 (total examples = 8160)	Loss: 0.015015
Step #1030 (total examples = 8240)	Loss: 0.039069
Step #1040 (total examples = 8320)	Loss: 0.012699
Step #1050 (total examples = 8400)	Loss: 0.502458
Step #1060 (total examples = 8480)	Loss: 0.157010
Step #1070 (total examples = 8560)	Loss: 0.125190
Step #1080 (total examples = 8640)	Loss: 0.018476
Step #1090 (total examples = 8720)	Loss: 0.195867
Step #1100 (total examples = 8800)	Loss: 0.915338
Step #1110 (total examples = 8880)	Loss: 0.197545
Step #1120 (total examples = 8960)	Loss: 0.019705
Step #1130 (total examples = 9040)	Loss: 0.529996
Step #1140 (total examples = 9120)	Loss: 0.026269
Step #1150 (total examples = 9200)	Loss: 0.003136
Step #1160 (total examples = 9280)	Loss: 0.067646
Step #1170 (total examples = 9360)	Loss: 0.020942
Step #1180 (total examples = 9440)	Loss: 0.015074
Step #1190 (total examples = 9520)	Loss: 0.002877
Step #1200 (total examples = 9600)	Loss: 0.007503
Step #1210 (total examples = 9680)	Loss: 0.071669
Step #1220 (total examples = 9760)	Loss: 0.003612
Step #1230 (total examples = 9840)	Loss: 0.460825
Step #1240 (total examples = 9920)	Loss: 0.068454
Step #1250 (total examples = 10000)	Loss: 0.001085
Step #1260 (total examples = 10080)	Loss: 0.218361
Step #1270 (total examples = 10160)	Loss: 0.148015
Step #1280 (total examples = 10240)	Loss: 0.275620
Step #1290 (total examples = 10320)	Loss: 0.013160
Step #1300 (total examples = 10400)	Loss: 0.001006
Step #1310 (total examples = 10480)	Loss: 0.003838
Step #1320 (total examples = 10560)	Loss: 0.727603
Step #1330 (total examples = 10640)	Loss: 0.000399
Step #1340 (total examples = 10720)	Loss: 0.111681
Step #1350 (total examples = 10800)	Loss: 0.052951
Step #1360 (total examples = 10880)	Loss: 0.112072
Step #1370 (total examples = 10960)	Loss: 0.000396
Step #1380 (total examples = 11040)	Loss: 0.157188
Step #1390 (total examples = 11120)	Loss: 0.069674
Step #1400 (total examples = 11200)	Loss: 0.010451
Step #1410 (total examples = 11280)	Loss: 0.003587
Step #1420 (total examples = 11360)	Loss: 0.019673
Step #1430 (total examples = 11440)	Loss: 0.096895
Step #1440 (total examples = 11520)	Loss: 0.002436
Step #1450 (total examples = 11600)	Loss: 0.002020
Step #1460 (total examples = 11680)	Loss: 0.003343
Step #1470 (total examples = 11760)	Loss: 0.019084
Step #1480 (total examples = 11840)	Loss: 0.017025
Step #1490 (total examples = 11920)	Loss: 0.078506
Step #1500 (total examples = 12000)	Loss: 0.014808
Step #1510 (total examples = 12080)	Loss: 0.007732
Step #1520 (total examples = 12160)	Loss: 0.076515
Step #1530 (total examples = 12240)	Loss: 0.019197
Step #1540 (total examples = 12320)	Loss: 1.494332
Step #1550 (total examples = 12400)	Loss: 0.753904
Step #1560 (total examples = 12480)	Loss: 0.072691
Step #1570 (total examples = 12560)	Loss: 0.155282
Step #1580 (total examples = 12640)	Loss: 0.143979
Step #1590 (total examples = 12720)	Loss: 0.033684
Step #1600 (total examples = 12800)	Loss: 0.037935
Step #1610 (total examples = 12880)	Loss: 0.027397
Step #1620 (total examples = 12960)	Loss: 0.029826
Step #1630 (total examples = 13040)	Loss: 0.671071
Step #1640 (total examples = 13120)	Loss: 0.016711
Step #1650 (total examples = 13200)	Loss: 0.014725
Step #1660 (total examples = 13280)	Loss: 0.043008
Step #1670 (total examples = 13360)	Loss: 0.003386
Step #1680 (total examples = 13440)	Loss: 0.133383
Step #1690 (total examples = 13520)	Loss: 0.002375
Step #1700 (total examples = 13600)	Loss: 0.041906
Step #1710 (total examples = 13680)	Loss: 0.004233
Step #1720 (total examples = 13760)	Loss: 0.008908
Step #1730 (total examples = 13840)	Loss: 0.029586
Step #1740 (total examples = 13920)	Loss: 0.010580
Step #1750 (total examples = 14000)	Loss: 0.109156
Step #1760 (total examples = 14080)	Loss: 0.060053
Step #1770 (total examples = 14160)	Loss: 0.052928
Step #1780 (total examples = 14240)	Loss: 0.008869
Step #1790 (total examples = 14320)	Loss: 0.114443
Step #1800 (total examples = 14400)	Loss: 0.004778
Step #1810 (total examples = 14480)	Loss: 0.200948
Step #1820 (total examples = 14560)	Loss: 0.332724
Step #1830 (total examples = 14640)	Loss: 0.283258
Step #1840 (total examples = 14720)	Loss: 0.052446
Step #1850 (total examples = 14800)	Loss: 0.073676
Step #1860 (total examples = 14880)	Loss: 0.001407
Step #1870 (total examples = 14960)	Loss: 0.000961
Step #1880 (total examples = 15040)	Loss: 0.036590
Step #1890 (total examples = 15120)	Loss: 0.000590
Step #1900 (total examples = 15200)	Loss: 0.002444
Step #1910 (total examples = 15280)	Loss: 0.021426
Step #1920 (total examples = 15360)	Loss: 0.003318
Step #1930 (total examples = 15440)	Loss: 0.022047
Step #1940 (total examples = 15520)	Loss: 0.644833
Step #1950 (total examples = 15600)	Loss: 0.002475
Step #1960 (total examples = 15680)	Loss: 0.047616
Step #1970 (total examples = 15760)	Loss: 0.007652
Step #1980 (total examples = 15840)	Loss: 0.303300
Step #1990 (total examples = 15920)	Loss: 0.098439

Step #1640 (total examples = 13120)	Loss: 0.548457
Step #1650 (total examples = 13200)	Loss: 0.005676
Step #1660 (total examples = 13280)	Loss: 0.052386
Step #1670 (total examples = 13360)	Loss: 0.006121
Step #1680 (total examples = 13440)	Loss: 0.003234
Step #1690 (total examples = 13520)	Loss: 0.006674
Step #1700 (total examples = 13600)	Loss: 0.007582
Step #1710 (total examples = 13680)	Loss: 0.041617
Step #1720 (total examples = 13760)	Loss: 0.221398
Step #1730 (total examples = 13840)	Loss: 0.208597
Step #1740 (total examples = 13920)	Loss: 0.007616
Step #1750 (total examples = 14000)	Loss: 0.349372
Step #1760 (total examples = 14080)	Loss: 0.004363
Step #1770 (total examples = 14160)	Loss: 0.025584
Step #1780 (total examples = 14240)	Loss: 0.019734
Step #1790 (total examples = 14320)	Loss: 0.000924
Step #1800 (total examples = 14400)	Loss: 0.013052
Step #1810 (total examples = 14480)	Loss: 0.047886
Step #1820 (total examples = 14560)	Loss: 0.038750
Step #1830 (total examples = 14640)	Loss: 0.153796
Step #1840 (total examples = 14720)	Loss: 0.098409
Step #1850 (total examples = 14800)	Loss: 0.092883
Step #1860 (total examples = 14880)	Loss: 0.019673
Step #1870 (total examples = 14960)	Loss: 0.000326
Step #1880 (total examples = 15040)	Loss: 0.001445
Step #1890 (total examples = 15120)	Loss: 0.004295
Step #1900 (total examples = 15200)	Loss: 0.001632
Step #1910 (total examples = 15280)	Loss: 0.002009
Step #1920 (total examples = 15360)	Loss: 0.573574
Step #1930 (total examples = 15440)	Loss: 0.004323
Step #1940 (total examples = 15520)	Loss: 0.013054
Step #1950 (total examples = 15600)	Loss: 0.002334
Step #1960 (total examples = 15680)	Loss: 0.164096
Step #1970 (total examples = 15760)	Loss: 0.012115
Step #1980 (total examples = 15840)	Loss: 0.134286
Step #1990 (total examples = 15920)	Loss: 0.007393
  1/313 [..............................] - ETA: 2:23 - loss: 22.1682 - accuracy: 0.9688  3/313 [..............................] - ETA: 11s - loss: 9.3328 - accuracy: 0.9792    5/313 [..............................] - ETA: 11s - loss: 6.2426 - accuracy: 0.9750  6/313 [..............................] - ETA: 12s - loss: 5.2022 - accuracy: 0.9792  7/313 [..............................] - ETA: 13s - loss: 4.4590 - accuracy: 0.9821  8/313 [..............................] - ETA: 15s - loss: 3.9016 - accuracy: 0.9844  9/313 [..............................] - ETA: 16s - loss: 3.4681 - accuracy: 0.9861 10/313 [..............................] - ETA: 16s - loss: 3.1213 - accuracy: 0.9875 11/313 [>.............................] - ETA: 17s - loss: 11.7592 - accuracy: 0.9801 12/313 [>.............................] - ETA: 17s - loss: 10.7793 - accuracy: 0.9818 13/313 [>.............................] - ETA: 17s - loss: 9.9501 - accuracy: 0.9832  14/313 [>.............................] - ETA: 18s - loss: 9.6966 - accuracy: 0.9821 15/313 [>.............................] - ETA: 18s - loss: 11.0584 - accuracy: 0.9792 16/313 [>.............................] - ETA: 18s - loss: 11.5206 - accuracy: 0.9785 17/313 [>.............................] - ETA: 18s - loss: 10.8429 - accuracy: 0.9798 18/313 [>.............................] - ETA: 18s - loss: 11.0393 - accuracy: 0.9774 19/313 [>.............................] - ETA: 18s - loss: 10.4583 - accuracy: 0.9786 21/313 [=>............................] - ETA: 18s - loss: 11.3394 - accuracy: 0.9762 23/313 [=>............................] - ETA: 17s - loss: 12.5066 - accuracy: 0.9728 25/313 [=>............................] - ETA: 16s - loss: 12.8161 - accuracy: 0.9712 27/313 [=>............................] - ETA: 16s - loss: 12.4670 - accuracy: 0.9722 29/313 [=>............................] - ETA: 15s - loss: 13.6428 - accuracy: 0.9709 31/313 [=>............................] - ETA: 15s - loss: 14.3987 - accuracy: 0.9698 33/313 [==>...........................] - ETA: 14s - loss: 14.7847 - accuracy: 0.9697 35/313 [==>...........................] - ETA: 14s - loss: 14.8732 - accuracy: 0.9705 37/313 [==>...........................] - ETA: 14s - loss: 15.8494 - accuracy: 0.9688 39/313 [==>...........................] - ETA: 13s - loss: 16.9288 - accuracy: 0.9671 41/313 [==>...........................] - ETA: 13s - loss: 17.2353 - accuracy: 0.9665 43/313 [===>..........................] - ETA: 13s - loss: 16.8777 - accuracy: 0.9666 45/313 [===>..........................] - ETA: 12s - loss: 16.2889 - accuracy: 0.9667 47/313 [===>..........................] - ETA: 12s - loss: 15.5957 - accuracy: 0.9681 49/313 [===>..........................] - ETA: 12s - loss: 15.8165 - accuracy: 0.9668 51/313 [===>..........................] - ETA: 12s - loss: 15.7580 - accuracy: 0.9669 53/313 [====>.........................] - ETA: 12s - loss: 16.0579 - accuracy: 0.9670 55/313 [====>.........................] - ETA: 11s - loss: 15.6304 - accuracy: 0.9670 57/313 [====>.........................] - ETA: 11s - loss: 15.5605 - accuracy: 0.9677 59/313 [====>.........................] - ETA: 11s - loss: 15.0330 - accuracy: 0.9688 61/313 [====>.........................] - ETA: 11s - loss: 15.0464 - accuracy: 0.9693 63/313 [=====>........................] - ETA: 11s - loss: 14.6574 - accuracy: 0.9697 65/313 [=====>........................] - ETA: 11s - loss: 16.1476 - accuracy: 0.9673 67/313 [=====>........................] - ETA: 10s - loss: 17.3927 - accuracy: 0.9664 69/313 [=====>........................] - ETA: 10s - loss: 18.1469 - accuracy: 0.9660 71/313 [=====>........................] - ETA: 10s - loss: 17.6358 - accuracy: 0.9670 73/313 [=====>........................] - ETA: 10s - loss: 17.6554 - accuracy: 0.9666 75/313 [======>.......................] - ETA: 10s - loss: 17.1846 - accuracy: 0.9675 77/313 [======>.......................] - ETA: 10s - loss: 17.6877 - accuracy: 0.9663 79/313 [======>.......................] - ETA: 10s - loss: 17.5818 - accuracy: 0.9668 81/313 [======>.......................] - ETA: 10s - loss: 17.4157 - accuracy: 0.9672 83/313 [======>.......................] - ETA: 9s - loss: 18.4198 - accuracy: 0.9672  85/313 [=======>......................] - ETA: 9s - loss: 17.9863 - accuracy: 0.9680 87/313 [=======>......................] - ETA: 9s - loss: 17.6804 - accuracy: 0.9684 89/313 [=======>......................] - ETA: 9s - loss: 17.3347 - accuracy: 0.9688 91/313 [=======>......................] - ETA: 9s - loss: 17.0939 - accuracy: 0.9688 93/313 [=======>......................] - ETA: 9s - loss: 17.0393 - accuracy: 0.9681 95/313 [========>.....................] - ETA: 9s - loss: 17.0887 - accuracy: 0.9681 97/313 [========>.....................] - ETA: 9s - loss: 16.8064 - accuracy: 0.9684 99/313 [========>.....................] - ETA: 9s - loss: 16.4755 - accuracy: 0.9688101/313 [========>.....................] - ETA: 8s - loss: 16.2170 - accuracy: 0.9691103/313 [========>.....................] - ETA: 8s - loss: 15.9663 - accuracy: 0.9691105/313 [=========>....................] - ETA: 8s - loss: 15.9001 - accuracy: 0.9690107/313 [=========>....................] - ETA: 8s - loss: 15.7359 - accuracy: 0.9693109/313 [=========>....................] - ETA: 8s - loss: 15.5398 - accuracy: 0.9696111/313 [=========>....................] - ETA: 8s - loss: 15.8456 - accuracy: 0.9690113/313 [=========>....................] - ETA: 8s - loss: 15.8367 - accuracy: 0.9690115/313 [==========>...................] - ETA: 8s - loss: 15.5613 - accuracy: 0.9696117/313 [==========>...................] - ETA: 8s - loss: 15.5122 - accuracy: 0.9696119/313 [==========>...................] - ETA: 8s - loss: 16.3754 - accuracy: 0.9690121/313 [==========>...................] - ETA: 7s - loss: 16.7364 - accuracy: 0.9688123/313 [==========>...................] - ETA: 7s - loss: 16.5640 - accuracy: 0.9690125/313 [==========>...................] - ETA: 7s - loss: 16.6495 - accuracy: 0.9685127/313 [===========>..................] - ETA: 7s - loss: 16.5501 - accuracy: 0.9685129/313 [===========>..................] - ETA: 7s - loss: 16.2935 - accuracy: 0.9690131/313 [===========>..................] - ETA: 7s - loss: 16.1349 - accuracy: 0.9692133/313 [===========>..................] - ETA: 7s - loss: 16.4618 - accuracy: 0.9688135/313 [===========>..................] - ETA: 7s - loss: 16.8032 - accuracy: 0.9683137/313 [============>.................] - ETA: 7s - loss: 16.8476 - accuracy: 0.9678139/313 [============>.................] - ETA: 7s - loss: 16.7106 - accuracy: 0.9679141/313 [============>.................] - ETA: 6s - loss: 16.5800 - accuracy: 0.9679143/313 [============>.................] - ETA: 6s - loss: 16.3481 - accuracy: 0.9683145/313 [============>.................] - ETA: 6s - loss: 16.3918 - accuracy: 0.9681147/313 [=============>................] - ETA: 6s - loss: 16.1688 - accuracy: 0.9685149/313 [=============>................] - ETA: 6s - loss: 16.4430 - accuracy: 0.9683151/313 [=============>................] - ETA: 6s - loss: 16.4979 - accuracy: 0.9681153/313 [=============>................] - ETA: 6s - loss: 16.2822 - accuracy: 0.9685155/313 [=============>................] - ETA: 6s - loss: 16.3072 - accuracy: 0.9683157/313 [==============>...............] - ETA: 6s - loss: 16.1503 - accuracy: 0.9684159/313 [==============>...............] - ETA: 6s - loss: 15.9471 - accuracy: 0.9688161/313 [==============>...............] - ETA: 6s - loss: 15.7490 - accuracy: 0.9691163/313 [==============>...............] - ETA: 6s - loss: 15.5941 - accuracy: 0.9693165/313 [==============>...............] - ETA: 5s - loss: 15.4050 - accuracy: 0.9697167/313 [===============>..............] - ETA: 5s - loss: 15.2206 - accuracy: 0.9701169/313 [===============>..............] - ETA: 5s - loss: 15.0404 - accuracy: 0.9704171/313 [===============>..............] - ETA: 5s - loss: 14.8645 - accuracy: 0.9708173/313 [===============>..............] - ETA: 5s - loss: 14.6927 - accuracy: 0.9711175/313 [===============>..............] - ETA: 5s - loss: 14.5248 - accuracy: 0.9714177/313 [===============>..............] - ETA: 5s - loss: 14.5165 - accuracy: 0.9714179/313 [================>.............] - ETA: 5s - loss: 14.3543 - accuracy: 0.9717181/313 [================>.............] - ETA: 5s - loss: 14.2166 - accuracy: 0.9719183/313 [================>.............] - ETA: 5s - loss: 14.0613 - accuracy: 0.9722185/313 [================>.............] - ETA: 5s - loss: 13.9093 - accuracy: 0.9725187/313 [================>.............] - ETA: 5s - loss: 14.2896 - accuracy: 0.9723189/313 [=================>............] - ETA: 4s - loss: 14.2445 - accuracy: 0.9724191/313 [=================>............] - ETA: 4s - loss: 14.4867 - accuracy: 0.9720193/313 [=================>............] - ETA: 4s - loss: 14.3366 - accuracy: 0.9723195/313 [=================>............] - ETA: 4s - loss: 14.1895 - accuracy: 0.9726197/313 [=================>............] - ETA: 4s - loss: 14.0455 - accuracy: 0.9729199/313 [==================>...........] - ETA: 4s - loss: 13.9043 - accuracy: 0.9731201/313 [==================>...........] - ETA: 4s - loss: 13.7662 - accuracy: 0.9733203/313 [==================>...........] - ETA: 4s - loss: 13.6306 - accuracy: 0.9735205/313 [==================>...........] - ETA: 4s - loss: 13.7386 - accuracy: 0.9733207/313 [==================>...........] - ETA: 4s - loss: 13.8599 - accuracy: 0.9733209/313 [===================>..........] - ETA: 4s - loss: 13.9398 - accuracy: 0.9734211/313 [===================>..........] - ETA: 4s - loss: 13.8857 - accuracy: 0.9733213/313 [===================>..........] - ETA: 3s - loss: 13.7553 - accuracy: 0.9736215/313 [===================>..........] - ETA: 3s - loss: 13.6274 - accuracy: 0.9738217/313 [===================>..........] - ETA: 3s - loss: 13.5018 - accuracy: 0.9741219/313 [===================>..........] - ETA: 3s - loss: 13.3785 - accuracy: 0.9743221/313 [====================>.........] - ETA: 3s - loss: 13.2747 - accuracy: 0.9744223/313 [====================>.........] - ETA: 3s - loss: 13.1556 - accuracy: 0.9746225/313 [====================>.........] - ETA: 3s - loss: 13.0387 - accuracy: 0.9749227/313 [====================>.........] - ETA: 3s - loss: 12.9902 - accuracy: 0.9749229/313 [====================>.........] - ETA: 3s - loss: 12.8768 - accuracy: 0.9752231/313 [=====================>........] - ETA: 3s - loss: 12.7653 - accuracy: 0.9754233/313 [=====================>........] - ETA: 3s - loss: 12.7866 - accuracy: 0.9755235/313 [=====================>........] - ETA: 3s - loss: 12.6778 - accuracy: 0.9757237/313 [=====================>........] - ETA: 2s - loss: 12.5708 - accuracy: 0.9759239/313 [=====================>........] - ETA: 2s - loss: 12.4656 - accuracy: 0.9761241/313 [======================>.......] - ETA: 2s - loss: 12.3622 - accuracy: 0.9763243/313 [======================>.......] - ETA: 2s - loss: 12.2604 - accuracy: 0.9765245/313 [======================>.......] - ETA: 2s - loss: 12.1604 - accuracy: 0.9767247/313 [======================>.......] - ETA: 2s - loss: 12.1413 - accuracy: 0.9765249/313 [======================>.......] - ETA: 2s - loss: 12.0528 - accuracy: 0.9765251/313 [=======================>......] - ETA: 2s - loss: 11.9567 - accuracy: 0.9767253/313 [=======================>......] - ETA: 2s - loss: 12.3372 - accuracy: 0.9763255/313 [=======================>......] - ETA: 2s - loss: 12.2404 - accuracy: 0.9765257/313 [=======================>......] - ETA: 2s - loss: 12.1452 - accuracy: 0.9767259/313 [=======================>......] - ETA: 2s - loss: 12.0576 - accuracy: 0.9767261/313 [========================>.....] - ETA: 2s - loss: 11.9880 - accuracy: 0.9768263/313 [========================>.....] - ETA: 1s - loss: 12.0059 - accuracy: 0.9767265/313 [========================>.....] - ETA: 1s - loss: 11.9153 - accuracy: 0.9769267/313 [========================>.....] - ETA: 1s - loss: 11.8859 - accuracy: 0.9769269/313 [========================>.....] - ETA: 1s - loss: 11.7975 - accuracy: 0.9771271/313 [========================>.....] - ETA: 1s - loss: 11.7105 - accuracy: 0.9773273/313 [=========================>....] - ETA: 1s - loss: 11.6247 - accuracy: 0.9774275/313 [=========================>....] - ETA: 1s - loss: 11.5401 - accuracy: 0.9776277/313 [=========================>....] - ETA: 1s - loss: 11.4568 - accuracy: 0.9778279/313 [=========================>....] - ETA: 1s - loss: 11.3747 - accuracy: 0.9779281/313 [=========================>....] - ETA: 1s - loss: 11.2937 - accuracy: 0.9781283/313 [==========================>...] - ETA: 1s - loss: 11.3802 - accuracy: 0.9779285/313 [==========================>...] - ETA: 1s - loss: 11.3003 - accuracy: 0.9781287/313 [==========================>...] - ETA: 1s - loss: 11.2216 - accuracy: 0.9782289/313 [==========================>...] - ETA: 0s - loss: 11.1439 - accuracy: 0.9784291/313 [==========================>...] - ETA: 0s - loss: 11.0673 - accuracy: 0.9785293/313 [===========================>..] - ETA: 0s - loss: 10.9918 - accuracy: 0.9787295/313 [===========================>..] - ETA: 0s - loss: 10.9173 - accuracy: 0.9788297/313 [===========================>..] - ETA: 0s - loss: 10.8438 - accuracy: 0.9790299/313 [===========================>..] - ETA: 0s - loss: 10.9112 - accuracy: 0.9790301/313 [===========================>..] - ETA: 0s - loss: 10.8531 - accuracy: 0.9790303/313 [============================>.] - ETA: 0s - loss: 10.8823 - accuracy: 0.9790305/313 [============================>.] - ETA: 0s - loss: 10.9675 - accuracy: 0.9790307/313 [============================>.] - ETA: 0s - loss: 11.1567 - accuracy: 0.9787309/313 [============================>.] - ETA: 0s - loss: 11.3164 - accuracy: 0.9785311/313 [============================>.] - ETA: 0s - loss: 11.3711 - accuracy: 0.9782313/313 [==============================] - ETA: 0s - loss: 11.4250 - accuracy: 0.9782313/313 [==============================] - 13s 39ms/step - loss: 11.4250 - accuracy: 0.9782
Final model accuracy: 0.978200
Total training time: 155.980757
2021-12-03 20:14:56.674381: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 20:14:56.674478: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-12-03 20:14:56.689198: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 20:14:56.689391: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-12-03 20:15:00.637110: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 20:15:00.637281: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2021-12-03 20:15:00.637340: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (node-0005): /proc/driver/nvidia/version does not exist
2021-12-03 20:15:00.740528: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 20:15:00.740798: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2021-12-03 20:15:00.740869: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (node-0001): /proc/driver/nvidia/version does not exist
===========
rank: 0
Batch size: 8
Total number of batches: 4000
Total training examples: 32000
Num of workers: 2
===========
Step #0 (total examples = 0)	Loss: 2.339539
Step #10 (total examples = 80)	Loss: 2.015301
Step #20 (total examples = 160)	Loss: 0.852467
Step #30 (total examples = 240)	Loss: 0.379940
Step #40 (total examples = 320)	Loss: 0.605937
Step #50 (total examples = 400)	Loss: 0.265196
Step #60 (total examples = 480)	Loss: 0.669683
Step #70 (total examples = 560)	Loss: 0.175226
Step #80 (total examples = 640)	Loss: 0.217610
Step #90 (total examples = 720)	Loss: 0.896012
Step #100 (total examples = 800)	Loss: 0.218198
Step #110 (total examples = 880)	Loss: 0.587724
Step #120 (total examples = 960)	Loss: 0.112058
Step #130 (total examples = 1040)	Loss: 0.117785
Step #140 (total examples = 1120)	Loss: 0.936210
Step #150 (total examples = 1200)	Loss: 0.457887
Step #160 (total examples = 1280)	Loss: 0.622810
Step #170 (total examples = 1360)	Loss: 0.084291
Step #180 (total examples = 1440)	Loss: 0.091596
Step #190 (total examples = 1520)	Loss: 0.251786
Step #200 (total examples = 1600)	Loss: 0.073963
Step #210 (total examples = 1680)	Loss: 0.100752
Step #220 (total examples = 1760)	Loss: 0.609382
Step #230 (total examples = 1840)	Loss: 0.052818
Step #240 (total examples = 1920)	Loss: 0.034715
Step #250 (total examples = 2000)	Loss: 0.020963
Step #260 (total examples = 2080)	Loss: 0.305494
Step #270 (total examples = 2160)	Loss: 0.063032
Step #280 (total examples = 2240)	Loss: 0.530325
Step #290 (total examples = 2320)	Loss: 0.188880
Step #300 (total examples = 2400)	Loss: 0.120075
Step #310 (total examples = 2480)	Loss: 0.046619
Step #320 (total examples = 2560)	Loss: 0.110675
Step #330 (total examples = 2640)	Loss: 0.016087
Step #340 (total examples = 2720)	Loss: 0.056549
Step #350 (total examples = 2800)	Loss: 0.312686
Step #360 (total examples = 2880)	Loss: 0.074235
Step #370 (total examples = 2960)	Loss: 0.039339
Step #380 (total examples = 3040)	Loss: 0.030615
Step #390 (total examples = 3120)	Loss: 0.871554
Step #400 (total examples = 3200)	Loss: 0.013771
Step #410 (total examples = 3280)	Loss: 0.068029
Step #420 (total examples = 3360)	Loss: 0.007737
Step #430 (total examples = 3440)	Loss: 0.001477
Step #440 (total examples = 3520)	Loss: 0.567585
Step #450 (total examples = 3600)	Loss: 0.224860
Step #460 (total examples = 3680)	Loss: 0.105924
Step #470 (total examples = 3760)	Loss: 0.105213
Step #480 (total examples = 3840)	Loss: 0.024408
Step #490 (total examples = 3920)	Loss: 0.613011
Step #500 (total examples = 4000)	Loss: 0.462324
Step #510 (total examples = 4080)	Loss: 0.058924
Step #520 (total examples = 4160)	Loss: 0.530490
Step #530 (total examples = 4240)	Loss: 0.065969
Step #540 (total examples = 4320)	Loss: 0.033707
Step #550 (total examples = 4400)	Loss: 0.017521
Step #560 (total examples = 4480)	Loss: 0.024182
Step #570 (total examples = 4560)	Loss: 0.929014
Step #580 (total examples = 4640)	Loss: 0.039437
Step #590 (total examples = 4720)	Loss: 0.319202
Step #600 (total examples = 4800)	Loss: 0.048355
Step #610 (total examples = 4880)	Loss: 0.016131
Step #620 (total examples = 4960)	Loss: 0.218287
Step #630 (total examples = 5040)	Loss: 0.154546
Step #640 (total examples = 5120)	Loss: 0.334209
Step #650 (total examples = 5200)	Loss: 0.689098
Step #660 (total examples = 5280)	Loss: 0.108710
Step #670 (total examples = 5360)	Loss: 0.496730
Step #680 (total examples = 5440)	Loss: 0.014983
Step #690 (total examples = 5520)	Loss: 0.005392
Step #700 (total examples = 5600)	Loss: 0.206900
Step #710 (total examples = 5680)	Loss: 0.034409
Step #720 (total examples = 5760)	Loss: 0.030191
Step #730 (total examples = 5840)	Loss: 0.274388
Step #740 (total examples = 5920)	Loss: 0.072032
Step #750 (total examples = 6000)	Loss: 0.030186
Step #760 (total examples = 6080)	Loss: 0.005676
Step #770 (total examples = 6160)	Loss: 0.007788
Step #780 (total examples = 6240)	Loss: 0.197868
Step #790 (total examples = 6320)	Loss: 0.004610
Step #800 (total examples = 6400)	Loss: 0.111398
Step #810 (total examples = 6480)	Loss: 0.028949
Step #820 (total examples = 6560)	Loss: 0.031362
Step #830 (total examples = 6640)	Loss: 0.018532
Step #840 (total examples = 6720)	Loss: 0.050161
Step #850 (total examples = 6800)	Loss: 0.000815
Step #860 (total examples = 6880)	Loss: 0.511666
Step #870 (total examples = 6960)	Loss: 0.002058
Step #880 (total examples = 7040)	Loss: 0.307325
Step #890 (total examples = 7120)	Loss: 0.288279
Step #900 (total examples = 7200)	Loss: 0.631608
Step #910 (total examples = 7280)	Loss: 0.035618
Step #920 (total examples = 7360)	Loss: 0.009600
Step #930 (total examples = 7440)	Loss: 0.710087
Step #940 (total examples = 7520)	Loss: 0.024819
Step #950 (total examples = 7600)	Loss: 0.612767
Step #960 (total examples = 7680)	Loss: 0.283508
Step #970 (total examples = 7760)	Loss: 0.318972
Step #980 (total examples = 7840)	Loss: 0.002022
Step #990 (total examples = 7920)	Loss: 0.016186
Step #1000 (total examples = 8000)	Loss: 0.045670
Step #1010 (total examples = 8080)	Loss: 0.123898
Step #1020 (total examples = 8160)	Loss: 0.188414
Step #1030 (total examples = 8240)	Loss: 0.061515
Step #1040 (total examples = 8320)	Loss: 0.022558
Step #1050 (total examples = 8400)	Loss: 0.010789
Step #1060 (total examples = 8480)	Loss: 0.015577
Step #1070 (total examples = 8560)	Loss: 0.000483
Step #1080 (total examples = 8640)	Loss: 0.079870
Step #1090 (total examples = 8720)	Loss: 0.886299
Step #1100 (total examples = 8800)	Loss: 0.042018
Step #1110 (total examples = 8880)	Loss: 0.004941
Step #1120 (total examples = 8960)	Loss: 0.000755
Step #1130 (total examples = 9040)	Loss: 0.004290
Step #1140 (total examples = 9120)	Loss: 0.008637
Step #1150 (total examples = 9200)	Loss: 0.000932
Step #1160 (total examples = 9280)	Loss: 0.003842
Step #1170 (total examples = 9360)	Loss: 0.000890
Step #1180 (total examples = 9440)	Loss: 0.029472
Step #1190 (total examples = 9520)	Loss: 0.001854
Step #1200 (total examples = 9600)	Loss: 0.190820
Step #1210 (total examples = 9680)	Loss: 0.079501
Step #1220 (total examples = 9760)	Loss: 0.018984
Step #1230 (total examples = 9840)	Loss: 0.023312
Step #1240 (total examples = 9920)	Loss: 0.022842
Step #1250 (total examples = 10000)	Loss: 0.031169
Step #1260 (total examples = 10080)	Loss: 0.045503
Step #1270 (total examples = 10160)	Loss: 0.495203
Step #1280 (total examples = 10240)	Loss: 0.043704
Step #1290 (total examples = 10320)	Loss: 0.011827
Step #1300 (total examples = 10400)	Loss: 0.013676
Step #1310 (total examples = 10480)	Loss: 0.020682
Step #1320 (total examples = 10560)	Loss: 0.063898
Step #1330 (total examples = 10640)	Loss: 0.168005
Step #1340 (total examples = 10720)	Loss: 0.003210
Step #1350 (total examples = 10800)	Loss: 0.139975
Step #1360 (total examples = 10880)	Loss: 0.021069
Step #1370 (total examples = 10960)	Loss: 0.002457
Step #1380 (total examples = 11040)	Loss: 0.032719
Step #1390 (total examples = 11120)	Loss: 0.013760
Step #1400 (total examples = 11200)	Loss: 0.002540
Step #1410 (total examples = 11280)	Loss: 0.214585
Step #1420 (total examples = 11360)	Loss: 0.006849
Step #1430 (total examples = 11440)	Loss: 0.034295
Step #1440 (total examples = 11520)	Loss: 0.036913
Step #1450 (total examples = 11600)	Loss: 0.086506
Step #1460 (total examples = 11680)	Loss: 1.168830
Step #1470 (total examples = 11760)	Loss: 0.100193
Step #1480 (total examples = 11840)	Loss: 0.038150
Step #1490 (total examples = 11920)	Loss: 0.091586
Step #1500 (total examples = 12000)	Loss: 0.067628
Step #1510 (total examples = 12080)	Loss: 0.001726
Step #1520 (total examples = 12160)	Loss: 0.782942
Step #1530 (total examples = 12240)	Loss: 0.180328
Step #1540 (total examples = 12320)	Loss: 0.469131
Step #1550 (total examples = 12400)	Loss: 0.189620
Step #1560 (total examples = 12480)	Loss: 0.003920
Step #1570 (total examples = 12560)	Loss: 0.011377
Step #1580 (total examples = 12640)	Loss: 0.016396
Step #1590 (total examples = 12720)	Loss: 0.011275
Step #1600 (total examples = 12800)	Loss: 0.024018
Step #1610 (total examples = 12880)	Loss: 0.031340
Step #1620 (total examples = 12960)	Loss: 0.013215
Step #1630 (total examples = 13040)	Loss: 0.006729===========
rank: 1
===========
Step #0 (total examples = 0)	Loss: 2.369688
Step #10 (total examples = 80)	Loss: 2.010170
Step #20 (total examples = 160)	Loss: 1.065144
Step #30 (total examples = 240)	Loss: 0.359634
Step #40 (total examples = 320)	Loss: 0.357774
Step #50 (total examples = 400)	Loss: 1.218730
Step #60 (total examples = 480)	Loss: 0.590573
Step #70 (total examples = 560)	Loss: 0.497079
Step #80 (total examples = 640)	Loss: 1.250789
Step #90 (total examples = 720)	Loss: 0.388201
Step #100 (total examples = 800)	Loss: 0.011880
Step #110 (total examples = 880)	Loss: 0.234527
Step #120 (total examples = 960)	Loss: 0.216452
Step #130 (total examples = 1040)	Loss: 0.442550
Step #140 (total examples = 1120)	Loss: 0.727612
Step #150 (total examples = 1200)	Loss: 0.078289
Step #160 (total examples = 1280)	Loss: 0.157907
Step #170 (total examples = 1360)	Loss: 0.256328
Step #180 (total examples = 1440)	Loss: 0.017328
Step #190 (total examples = 1520)	Loss: 0.063727
Step #200 (total examples = 1600)	Loss: 0.234094
Step #210 (total examples = 1680)	Loss: 0.136624
Step #220 (total examples = 1760)	Loss: 0.075940
Step #230 (total examples = 1840)	Loss: 0.395673
Step #240 (total examples = 1920)	Loss: 0.020359
Step #250 (total examples = 2000)	Loss: 0.147920
Step #260 (total examples = 2080)	Loss: 0.412616
Step #270 (total examples = 2160)	Loss: 0.059922
Step #280 (total examples = 2240)	Loss: 0.368079
Step #290 (total examples = 2320)	Loss: 0.152642
Step #300 (total examples = 2400)	Loss: 0.270908
Step #310 (total examples = 2480)	Loss: 0.037949
Step #320 (total examples = 2560)	Loss: 0.065202
Step #330 (total examples = 2640)	Loss: 0.061060
Step #340 (total examples = 2720)	Loss: 0.014552
Step #350 (total examples = 2800)	Loss: 0.099646
Step #360 (total examples = 2880)	Loss: 0.128445
Step #370 (total examples = 2960)	Loss: 0.104883
Step #380 (total examples = 3040)	Loss: 0.007604
Step #390 (total examples = 3120)	Loss: 0.026820
Step #400 (total examples = 3200)	Loss: 0.213224
Step #410 (total examples = 3280)	Loss: 0.367567
Step #420 (total examples = 3360)	Loss: 0.066585
Step #430 (total examples = 3440)	Loss: 0.271731
Step #440 (total examples = 3520)	Loss: 1.025943
Step #450 (total examples = 3600)	Loss: 0.026788
Step #460 (total examples = 3680)	Loss: 0.055459
Step #470 (total examples = 3760)	Loss: 0.149556
Step #480 (total examples = 3840)	Loss: 0.327379
Step #490 (total examples = 3920)	Loss: 0.066819
Step #500 (total examples = 4000)	Loss: 0.409598
Step #510 (total examples = 4080)	Loss: 0.003230
Step #520 (total examples = 4160)	Loss: 0.742586
Step #530 (total examples = 4240)	Loss: 0.035847
Step #540 (total examples = 4320)	Loss: 0.079532
Step #550 (total examples = 4400)	Loss: 0.007990
Step #560 (total examples = 4480)	Loss: 0.302456
Step #570 (total examples = 4560)	Loss: 0.282598
Step #580 (total examples = 4640)	Loss: 0.062561
Step #590 (total examples = 4720)	Loss: 0.051902
Step #600 (total examples = 4800)	Loss: 0.198450
Step #610 (total examples = 4880)	Loss: 0.021171
Step #620 (total examples = 4960)	Loss: 0.096479
Step #630 (total examples = 5040)	Loss: 0.221270
Step #640 (total examples = 5120)	Loss: 0.064503
Step #650 (total examples = 5200)	Loss: 0.219318
Step #660 (total examples = 5280)	Loss: 0.035160
Step #670 (total examples = 5360)	Loss: 0.124848
Step #680 (total examples = 5440)	Loss: 0.058567
Step #690 (total examples = 5520)	Loss: 0.004275
Step #700 (total examples = 5600)	Loss: 0.413089
Step #710 (total examples = 5680)	Loss: 0.084249
Step #720 (total examples = 5760)	Loss: 0.001014
Step #730 (total examples = 5840)	Loss: 0.210527
Step #740 (total examples = 5920)	Loss: 0.123509
Step #750 (total examples = 6000)	Loss: 0.698930
Step #760 (total examples = 6080)	Loss: 0.005964
Step #770 (total examples = 6160)	Loss: 0.004300
Step #780 (total examples = 6240)	Loss: 0.237166
Step #790 (total examples = 6320)	Loss: 0.007787
Step #800 (total examples = 6400)	Loss: 0.614441
Step #810 (total examples = 6480)	Loss: 0.016281
Step #820 (total examples = 6560)	Loss: 0.004873
Step #830 (total examples = 6640)	Loss: 0.058505
Step #840 (total examples = 6720)	Loss: 0.027113
Step #850 (total examples = 6800)	Loss: 0.005309
Step #860 (total examples = 6880)	Loss: 0.075513
Step #870 (total examples = 6960)	Loss: 0.136137
Step #880 (total examples = 7040)	Loss: 0.057039
Step #890 (total examples = 7120)	Loss: 0.336577
Step #900 (total examples = 7200)	Loss: 0.147344
Step #910 (total examples = 7280)	Loss: 0.012095
Step #920 (total examples = 7360)	Loss: 0.240923
Step #930 (total examples = 7440)	Loss: 0.100721
Step #940 (total examples = 7520)	Loss: 0.071776
Step #950 (total examples = 7600)	Loss: 0.357446
Step #960 (total examples = 7680)	Loss: 0.470690
Step #970 (total examples = 7760)	Loss: 0.100052
Step #980 (total examples = 7840)	Loss: 0.416147
Step #990 (total examples = 7920)	Loss: 0.306694
Step #1000 (total examples = 8000)	Loss: 0.473301
Step #1010 (total examples = 8080)	Loss: 0.033799
Step #1020 (total examples = 8160)	Loss: 0.007546
Step #1030 (total examples = 8240)	Loss: 0.071810
Step #1040 (total examples = 8320)	Loss: 0.084750
Step #1050 (total examples = 8400)	Loss: 0.417123
Step #1060 (total examples = 8480)	Loss: 0.076262
Step #1070 (total examples = 8560)	Loss: 0.036021
Step #1080 (total examples = 8640)	Loss: 0.063755
Step #1090 (total examples = 8720)	Loss: 0.133826
Step #1100 (total examples = 8800)	Loss: 0.857543
Step #1110 (total examples = 8880)	Loss: 0.059897
Step #1120 (total examples = 8960)	Loss: 0.098451
Step #1130 (total examples = 9040)	Loss: 0.470499
Step #1140 (total examples = 9120)	Loss: 0.005866
Step #1150 (total examples = 9200)	Loss: 0.027215
Step #1160 (total examples = 9280)	Loss: 0.018450
Step #1170 (total examples = 9360)	Loss: 0.038812
Step #1180 (total examples = 9440)	Loss: 0.339212
Step #1190 (total examples = 9520)	Loss: 0.001400
Step #1200 (total examples = 9600)	Loss: 0.000491
Step #1210 (total examples = 9680)	Loss: 0.211266
Step #1220 (total examples = 9760)	Loss: 0.026145
Step #1230 (total examples = 9840)	Loss: 0.334348
Step #1240 (total examples = 9920)	Loss: 0.110012
Step #1250 (total examples = 10000)	Loss: 0.001038
Step #1260 (total examples = 10080)	Loss: 0.120535
Step #1270 (total examples = 10160)	Loss: 0.170567
Step #1280 (total examples = 10240)	Loss: 0.015149
Step #1290 (total examples = 10320)	Loss: 0.017564
Step #1300 (total examples = 10400)	Loss: 0.005330
Step #1310 (total examples = 10480)	Loss: 0.017544
Step #1320 (total examples = 10560)	Loss: 0.221682
Step #1330 (total examples = 10640)	Loss: 0.018423
Step #1340 (total examples = 10720)	Loss: 0.078349
Step #1350 (total examples = 10800)	Loss: 0.001202
Step #1360 (total examples = 10880)	Loss: 0.236305
Step #1370 (total examples = 10960)	Loss: 0.002517
Step #1380 (total examples = 11040)	Loss: 0.015817
Step #1390 (total examples = 11120)	Loss: 0.005367
Step #1400 (total examples = 11200)	Loss: 0.005972
Step #1410 (total examples = 11280)	Loss: 0.000184
Step #1420 (total examples = 11360)	Loss: 0.028144
Step #1430 (total examples = 11440)	Loss: 0.011950
Step #1440 (total examples = 11520)	Loss: 0.022134
Step #1450 (total examples = 11600)	Loss: 0.000575
Step #1460 (total examples = 11680)	Loss: 0.071554
Step #1470 (total examples = 11760)	Loss: 0.009890
Step #1480 (total examples = 11840)	Loss: 0.003266
Step #1490 (total examples = 11920)	Loss: 0.005996
Step #1500 (total examples = 12000)	Loss: 0.074137
Step #1510 (total examples = 12080)	Loss: 0.014783
Step #1520 (total examples = 12160)	Loss: 0.182240
Step #1530 (total examples = 12240)	Loss: 0.053413
Step #1540 (total examples = 12320)	Loss: 1.085284
Step #1550 (total examples = 12400)	Loss: 0.409781
Step #1560 (total examples = 12480)	Loss: 0.016553
Step #1570 (total examples = 12560)	Loss: 0.129356
Step #1580 (total examples = 12640)	Loss: 0.266735
Step #1590 (total examples = 12720)	Loss: 0.099234
Step #1600 (total examples = 12800)	Loss: 0.001782
Step #1610 (total examples = 12880)	Loss: 0.014476
Step #1620 (total examples = 12960)	Loss: 0.009813
Step #1630 (total examples = 13040)	Loss: 0.015294
Step #1640 (total examples = 13120)	Loss: 0.007179
Step #1650 (total examples = 13200)	Loss: 0.034703
Step #1660 (total examples = 13280)	Loss: 0.508364
Step #1670 (total examples = 13360)	Loss: 0.045282
Step #1680 (total examples = 13440)	Loss: 0.179384
Step #1690 (total examples = 13520)	Loss: 0.004297
Step #1700 (total examples = 13600)	Loss: 0.005224
Step #1710 (total examples = 13680)	Loss: 0.007444
Step #1720 (total examples = 13760)	Loss: 0.002015
Step #1730 (total examples = 13840)	Loss: 0.026025
Step #1740 (total examples = 13920)	Loss: 0.006227
Step #1750 (total examples = 14000)	Loss: 0.024781
Step #1760 (total examples = 14080)	Loss: 0.153191
Step #1770 (total examples = 14160)	Loss: 0.004680
Step #1780 (total examples = 14240)	Loss: 0.013901
Step #1790 (total examples = 14320)	Loss: 0.016699
Step #1800 (total examples = 14400)	Loss: 0.000962
Step #1810 (total examples = 14480)	Loss: 0.322005
Step #1820 (total examples = 14560)	Loss: 0.187646
Step #1830 (total examples = 14640)	Loss: 0.018792
Step #1840 (total examples = 14720)	Loss: 0.054735
Step #1850 (total examples = 14800)	Loss: 0.034808
Step #1860 (total examples = 14880)	Loss: 0.242971
Step #1870 (total examples = 14960)	Loss: 0.142865
Step #1880 (total examples = 15040)	Loss: 0.017558
Step #1890 (total examples = 15120)	Loss: 0.002818
Step #1900 (total examples = 15200)	Loss: 0.029571
Step #1910 (total examples = 15280)	Loss: 0.511600
Step #1920 (total examples = 15360)	Loss: 0.005716
Step #1930 (total examples = 15440)	Loss: 0.168276
Step #1940 (total examples = 15520)	Loss: 0.521149
Step #1950 (total examples = 15600)	Loss: 0.020735
Step #1960 (total examples = 15680)	Loss: 0.021889
Step #1970 (total examples = 15760)	Loss: 0.001519
Step #1980 (total examples = 15840)	Loss: 0.183790
Step #1990 (total examples = 15920)	Loss: 0.003463

Step #1640 (total examples = 13120)	Loss: 0.123174
Step #1650 (total examples = 13200)	Loss: 0.009123
Step #1660 (total examples = 13280)	Loss: 0.161989
Step #1670 (total examples = 13360)	Loss: 0.003558
Step #1680 (total examples = 13440)	Loss: 0.003019
Step #1690 (total examples = 13520)	Loss: 0.001496
Step #1700 (total examples = 13600)	Loss: 0.225131
Step #1710 (total examples = 13680)	Loss: 0.168610
Step #1720 (total examples = 13760)	Loss: 0.171701
Step #1730 (total examples = 13840)	Loss: 0.470713
Step #1740 (total examples = 13920)	Loss: 0.002810
Step #1750 (total examples = 14000)	Loss: 0.415294
Step #1760 (total examples = 14080)	Loss: 0.001406
Step #1770 (total examples = 14160)	Loss: 0.401847
Step #1780 (total examples = 14240)	Loss: 0.046623
Step #1790 (total examples = 14320)	Loss: 0.006190
Step #1800 (total examples = 14400)	Loss: 0.050278
Step #1810 (total examples = 14480)	Loss: 0.136392
Step #1820 (total examples = 14560)	Loss: 0.010952
Step #1830 (total examples = 14640)	Loss: 0.280272
Step #1840 (total examples = 14720)	Loss: 0.013712
Step #1850 (total examples = 14800)	Loss: 0.005785
Step #1860 (total examples = 14880)	Loss: 0.013212
Step #1870 (total examples = 14960)	Loss: 0.446813
Step #1880 (total examples = 15040)	Loss: 0.003209
Step #1890 (total examples = 15120)	Loss: 0.011853
Step #1900 (total examples = 15200)	Loss: 0.014800
Step #1910 (total examples = 15280)	Loss: 0.006898
Step #1920 (total examples = 15360)	Loss: 0.156000
Step #1930 (total examples = 15440)	Loss: 0.048340
Step #1940 (total examples = 15520)	Loss: 0.006354
Step #1950 (total examples = 15600)	Loss: 0.001151
Step #1960 (total examples = 15680)	Loss: 0.028136
Step #1970 (total examples = 15760)	Loss: 0.010327
Step #1980 (total examples = 15840)	Loss: 0.379442
Step #1990 (total examples = 15920)	Loss: 0.039722
  1/313 [..............................] - ETA: 2:20 - loss: 0.0000e+00 - accuracy: 1.0000  3/313 [..............................] - ETA: 11s - loss: 0.0000e+00 - accuracy: 1.0000   5/313 [..............................] - ETA: 11s - loss: 1.7022 - accuracy: 0.9937      7/313 [..............................] - ETA: 13s - loss: 2.4087 - accuracy: 0.9911  8/313 [..............................] - ETA: 14s - loss: 2.1077 - accuracy: 0.9922  9/313 [..............................] - ETA: 15s - loss: 1.9361 - accuracy: 0.9896 10/313 [..............................] - ETA: 16s - loss: 1.7425 - accuracy: 0.9906 11/313 [>.............................] - ETA: 16s - loss: 9.3012 - accuracy: 0.9830 12/313 [>.............................] - ETA: 17s - loss: 8.5261 - accuracy: 0.9844 13/313 [>.............................] - ETA: 17s - loss: 7.8703 - accuracy: 0.9856 14/313 [>.............................] - ETA: 17s - loss: 11.7368 - accuracy: 0.9821 15/313 [>.............................] - ETA: 17s - loss: 11.4383 - accuracy: 0.9812 16/313 [>.............................] - ETA: 18s - loss: 12.8017 - accuracy: 0.9805 17/313 [>.............................] - ETA: 18s - loss: 12.0486 - accuracy: 0.9816 18/313 [>.............................] - ETA: 18s - loss: 11.6365 - accuracy: 0.9792 19/313 [>.............................] - ETA: 18s - loss: 11.5719 - accuracy: 0.9786 20/313 [>.............................] - ETA: 18s - loss: 15.1754 - accuracy: 0.9750 22/313 [=>............................] - ETA: 17s - loss: 14.9336 - accuracy: 0.9730 24/313 [=>............................] - ETA: 16s - loss: 17.9573 - accuracy: 0.9688 26/313 [=>............................] - ETA: 16s - loss: 16.5760 - accuracy: 0.9712 28/313 [=>............................] - ETA: 15s - loss: 15.8535 - accuracy: 0.9710 30/313 [=>............................] - ETA: 15s - loss: 15.7234 - accuracy: 0.9708 32/313 [==>...........................] - ETA: 14s - loss: 16.5917 - accuracy: 0.9717 34/313 [==>...........................] - ETA: 14s - loss: 16.6509 - accuracy: 0.9706 36/313 [==>...........................] - ETA: 14s - loss: 17.1230 - accuracy: 0.9714 38/313 [==>...........................] - ETA: 13s - loss: 17.1695 - accuracy: 0.9712 40/313 [==>...........................] - ETA: 13s - loss: 17.6545 - accuracy: 0.9703 42/313 [===>..........................] - ETA: 13s - loss: 17.1732 - accuracy: 0.9710 44/313 [===>..........................] - ETA: 13s - loss: 16.4506 - accuracy: 0.9716 46/313 [===>..........................] - ETA: 12s - loss: 15.7353 - accuracy: 0.9728 48/313 [===>..........................] - ETA: 12s - loss: 15.8001 - accuracy: 0.9714 50/313 [===>..........................] - ETA: 12s - loss: 15.3761 - accuracy: 0.9719 52/313 [===>..........................] - ETA: 12s - loss: 15.0304 - accuracy: 0.9718 54/313 [====>.........................] - ETA: 12s - loss: 15.1914 - accuracy: 0.9705 56/313 [====>.........................] - ETA: 11s - loss: 14.7583 - accuracy: 0.9710 58/313 [====>.........................] - ETA: 11s - loss: 14.2931 - accuracy: 0.9714 60/313 [====>.........................] - ETA: 11s - loss: 14.9670 - accuracy: 0.9708 62/313 [====>.........................] - ETA: 11s - loss: 14.4842 - accuracy: 0.9718 64/313 [=====>........................] - ETA: 11s - loss: 14.6592 - accuracy: 0.9717 66/313 [=====>........................] - ETA: 11s - loss: 15.1174 - accuracy: 0.9706 68/313 [=====>........................] - ETA: 10s - loss: 17.0729 - accuracy: 0.9692 70/313 [=====>........................] - ETA: 10s - loss: 17.1272 - accuracy: 0.9692 72/313 [=====>........................] - ETA: 10s - loss: 16.7633 - accuracy: 0.9696 74/313 [======>.......................] - ETA: 10s - loss: 16.4160 - accuracy: 0.9700 76/313 [======>.......................] - ETA: 10s - loss: 16.1853 - accuracy: 0.9700 78/313 [======>.......................] - ETA: 10s - loss: 16.8951 - accuracy: 0.9692 80/313 [======>.......................] - ETA: 10s - loss: 16.4727 - accuracy: 0.9699 82/313 [======>.......................] - ETA: 10s - loss: 16.4946 - accuracy: 0.9695 84/313 [=======>......................] - ETA: 9s - loss: 17.1032 - accuracy: 0.9695  86/313 [=======>......................] - ETA: 9s - loss: 16.7195 - accuracy: 0.9698 88/313 [=======>......................] - ETA: 9s - loss: 16.4302 - accuracy: 0.9698 90/313 [=======>......................] - ETA: 9s - loss: 16.0651 - accuracy: 0.9705 92/313 [=======>......................] - ETA: 9s - loss: 16.2178 - accuracy: 0.9701 94/313 [========>.....................] - ETA: 9s - loss: 16.3822 - accuracy: 0.9701 96/313 [========>.....................] - ETA: 9s - loss: 16.0415 - accuracy: 0.9704 98/313 [========>.....................] - ETA: 9s - loss: 15.9273 - accuracy: 0.9707100/313 [========>.....................] - ETA: 8s - loss: 15.6087 - accuracy: 0.9712102/313 [========>.....................] - ETA: 8s - loss: 15.4512 - accuracy: 0.9712104/313 [========>.....................] - ETA: 8s - loss: 15.2570 - accuracy: 0.9715106/313 [=========>....................] - ETA: 8s - loss: 14.9691 - accuracy: 0.9720108/313 [=========>....................] - ETA: 8s - loss: 14.9666 - accuracy: 0.9722110/313 [=========>....................] - ETA: 8s - loss: 14.9548 - accuracy: 0.9722112/313 [=========>....................] - ETA: 8s - loss: 15.0104 - accuracy: 0.9724114/313 [=========>....................] - ETA: 8s - loss: 14.7721 - accuracy: 0.9726116/313 [==========>...................] - ETA: 8s - loss: 14.5174 - accuracy: 0.9731118/313 [==========>...................] - ETA: 8s - loss: 14.3716 - accuracy: 0.9727120/313 [==========>...................] - ETA: 7s - loss: 15.2556 - accuracy: 0.9724122/313 [==========>...................] - ETA: 7s - loss: 15.3659 - accuracy: 0.9726124/313 [==========>...................] - ETA: 7s - loss: 15.3508 - accuracy: 0.9725126/313 [===========>..................] - ETA: 7s - loss: 15.2761 - accuracy: 0.9727128/313 [===========>..................] - ETA: 7s - loss: 15.1317 - accuracy: 0.9729130/313 [===========>..................] - ETA: 7s - loss: 14.8989 - accuracy: 0.9733132/313 [===========>..................] - ETA: 7s - loss: 14.7482 - accuracy: 0.9732134/313 [===========>..................] - ETA: 7s - loss: 14.7964 - accuracy: 0.9729136/313 [============>.................] - ETA: 7s - loss: 14.7048 - accuracy: 0.9727138/313 [============>.................] - ETA: 7s - loss: 14.6206 - accuracy: 0.9726140/313 [============>.................] - ETA: 7s - loss: 14.4117 - accuracy: 0.9730142/313 [============>.................] - ETA: 6s - loss: 14.3745 - accuracy: 0.9729144/313 [============>.................] - ETA: 6s - loss: 14.5158 - accuracy: 0.9727146/313 [============>.................] - ETA: 6s - loss: 14.4514 - accuracy: 0.9728148/313 [=============>................] - ETA: 6s - loss: 14.2561 - accuracy: 0.9732150/313 [=============>................] - ETA: 6s - loss: 14.2962 - accuracy: 0.9733152/313 [=============>................] - ETA: 6s - loss: 14.4642 - accuracy: 0.9733154/313 [=============>................] - ETA: 6s - loss: 14.2764 - accuracy: 0.9736156/313 [=============>................] - ETA: 6s - loss: 14.3749 - accuracy: 0.9738158/313 [==============>...............] - ETA: 6s - loss: 14.1929 - accuracy: 0.9741160/313 [==============>...............] - ETA: 6s - loss: 14.0155 - accuracy: 0.9744162/313 [==============>...............] - ETA: 6s - loss: 13.8797 - accuracy: 0.9745164/313 [==============>...............] - ETA: 5s - loss: 13.7104 - accuracy: 0.9748166/313 [==============>...............] - ETA: 5s - loss: 13.5966 - accuracy: 0.9750168/313 [===============>..............] - ETA: 5s - loss: 13.4347 - accuracy: 0.9753170/313 [===============>..............] - ETA: 5s - loss: 13.2767 - accuracy: 0.9756172/313 [===============>..............] - ETA: 5s - loss: 13.1223 - accuracy: 0.9758174/313 [===============>..............] - ETA: 5s - loss: 12.9715 - accuracy: 0.9761176/313 [===============>..............] - ETA: 5s - loss: 12.8241 - accuracy: 0.9764178/313 [================>.............] - ETA: 5s - loss: 12.7822 - accuracy: 0.9763180/313 [================>.............] - ETA: 5s - loss: 12.6726 - accuracy: 0.9764182/313 [================>.............] - ETA: 5s - loss: 12.5333 - accuracy: 0.9766184/313 [================>.............] - ETA: 5s - loss: 12.3971 - accuracy: 0.9769186/313 [================>.............] - ETA: 5s - loss: 12.3802 - accuracy: 0.9766188/313 [=================>............] - ETA: 4s - loss: 12.5992 - accuracy: 0.9764190/313 [=================>............] - ETA: 4s - loss: 12.6689 - accuracy: 0.9765192/313 [=================>............] - ETA: 4s - loss: 12.6453 - accuracy: 0.9762194/313 [=================>............] - ETA: 4s - loss: 12.5418 - accuracy: 0.9763196/313 [=================>............] - ETA: 4s - loss: 12.4138 - accuracy: 0.9766198/313 [=================>............] - ETA: 4s - loss: 12.2884 - accuracy: 0.9768200/313 [==================>...........] - ETA: 4s - loss: 12.1655 - accuracy: 0.9770202/313 [==================>...........] - ETA: 4s - loss: 12.0451 - accuracy: 0.9773204/313 [==================>...........] - ETA: 4s - loss: 12.1777 - accuracy: 0.9773206/313 [==================>...........] - ETA: 4s - loss: 12.0595 - accuracy: 0.9775208/313 [==================>...........] - ETA: 4s - loss: 12.4332 - accuracy: 0.9775210/313 [===================>..........] - ETA: 4s - loss: 12.3148 - accuracy: 0.9777212/313 [===================>..........] - ETA: 3s - loss: 12.2809 - accuracy: 0.9777214/313 [===================>..........] - ETA: 3s - loss: 12.1661 - accuracy: 0.9779216/313 [===================>..........] - ETA: 3s - loss: 12.0535 - accuracy: 0.9782218/313 [===================>..........] - ETA: 3s - loss: 11.9429 - accuracy: 0.9784220/313 [====================>.........] - ETA: 3s - loss: 11.8343 - accuracy: 0.9786222/313 [====================>.........] - ETA: 3s - loss: 11.7277 - accuracy: 0.9787224/313 [====================>.........] - ETA: 3s - loss: 11.6230 - accuracy: 0.9789226/313 [====================>.........] - ETA: 3s - loss: 11.5918 - accuracy: 0.9790228/313 [====================>.........] - ETA: 3s - loss: 11.4901 - accuracy: 0.9792230/313 [=====================>........] - ETA: 3s - loss: 11.3902 - accuracy: 0.9793232/313 [=====================>........] - ETA: 3s - loss: 11.2920 - accuracy: 0.9795234/313 [=====================>........] - ETA: 3s - loss: 11.2753 - accuracy: 0.9796236/313 [=====================>........] - ETA: 3s - loss: 11.1798 - accuracy: 0.9797238/313 [=====================>........] - ETA: 2s - loss: 11.0858 - accuracy: 0.9799240/313 [======================>.......] - ETA: 2s - loss: 10.9934 - accuracy: 0.9801242/313 [======================>.......] - ETA: 2s - loss: 10.9026 - accuracy: 0.9802244/313 [======================>.......] - ETA: 2s - loss: 10.8132 - accuracy: 0.9804246/313 [======================>.......] - ETA: 2s - loss: 10.8068 - accuracy: 0.9804248/313 [======================>.......] - ETA: 2s - loss: 11.0576 - accuracy: 0.9798250/313 [======================>.......] - ETA: 2s - loss: 11.0101 - accuracy: 0.9799252/313 [=======================>......] - ETA: 2s - loss: 10.9327 - accuracy: 0.9799254/313 [=======================>......] - ETA: 2s - loss: 11.0371 - accuracy: 0.9796256/313 [=======================>......] - ETA: 2s - loss: 10.9509 - accuracy: 0.9797258/313 [=======================>......] - ETA: 2s - loss: 10.9063 - accuracy: 0.9798260/313 [=======================>......] - ETA: 2s - loss: 10.9035 - accuracy: 0.9798262/313 [========================>.....] - ETA: 1s - loss: 10.8833 - accuracy: 0.9798264/313 [========================>.....] - ETA: 1s - loss: 10.8392 - accuracy: 0.9799266/313 [========================>.....] - ETA: 1s - loss: 10.7577 - accuracy: 0.9800268/313 [========================>.....] - ETA: 1s - loss: 10.7119 - accuracy: 0.9799270/313 [========================>.....] - ETA: 1s - loss: 10.6325 - accuracy: 0.9801272/313 [=========================>....] - ETA: 1s - loss: 10.5543 - accuracy: 0.9802274/313 [=========================>....] - ETA: 1s - loss: 10.4773 - accuracy: 0.9804276/313 [=========================>....] - ETA: 1s - loss: 10.4014 - accuracy: 0.9805278/313 [=========================>....] - ETA: 1s - loss: 10.3265 - accuracy: 0.9807280/313 [=========================>....] - ETA: 1s - loss: 10.2528 - accuracy: 0.9808282/313 [==========================>...] - ETA: 1s - loss: 10.3049 - accuracy: 0.9807284/313 [==========================>...] - ETA: 1s - loss: 10.2680 - accuracy: 0.9806286/313 [==========================>...] - ETA: 1s - loss: 10.1961 - accuracy: 0.9808288/313 [==========================>...] - ETA: 0s - loss: 10.1253 - accuracy: 0.9809290/313 [==========================>...] - ETA: 0s - loss: 10.0555 - accuracy: 0.9810292/313 [==========================>...] - ETA: 0s - loss: 9.9866 - accuracy: 0.9812 294/313 [===========================>..] - ETA: 0s - loss: 9.9187 - accuracy: 0.9813296/313 [===========================>..] - ETA: 0s - loss: 9.8517 - accuracy: 0.9814298/313 [===========================>..] - ETA: 0s - loss: 9.7856 - accuracy: 0.9815300/313 [===========================>..] - ETA: 0s - loss: 9.7340 - accuracy: 0.9816302/313 [===========================>..] - ETA: 0s - loss: 9.6696 - accuracy: 0.9817304/313 [============================>.] - ETA: 0s - loss: 9.7438 - accuracy: 0.9816306/313 [============================>.] - ETA: 0s - loss: 10.0313 - accuracy: 0.9813308/313 [============================>.] - ETA: 0s - loss: 10.3810 - accuracy: 0.9810310/313 [============================>.] - ETA: 0s - loss: 10.5059 - accuracy: 0.9805312/313 [============================>.] - ETA: 0s - loss: 10.4775 - accuracy: 0.9805313/313 [==============================] - 13s 39ms/step - loss: 10.4607 - accuracy: 0.9805
Final model accuracy: 0.980500
Total training time: 161.117964
2021-12-03 20:17:56.623942: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 20:17:56.624104: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-12-03 20:17:56.653735: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 20:17:56.653824: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-12-03 20:18:00.655128: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 20:18:00.655249: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2021-12-03 20:18:00.655328: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (node-0005): /proc/driver/nvidia/version does not exist
2021-12-03 20:18:00.713910: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 20:18:00.714110: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2021-12-03 20:18:00.714179: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (node-0001): /proc/driver/nvidia/version does not exist
===========
rank: 0
Batch size: 8
Total number of batches: 4000
Total training examples: 32000
Num of workers: 2
===========
Step #0 (total examples = 0)	Loss: 2.255095
Step #10 (total examples = 80)	Loss: 2.024657
Step #20 (total examples = 160)	Loss: 1.002296
Step #30 (total examples = 240)	Loss: 0.604451
Step #40 (total examples = 320)	Loss: 0.501311
Step #50 (total examples = 400)	Loss: 0.603298
Step #60 (total examples = 480)	Loss: 0.901555
Step #70 (total examples = 560)	Loss: 0.240236
Step #80 (total examples = 640)	Loss: 0.524565
Step #90 (total examples = 720)	Loss: 0.443601
Step #100 (total examples = 800)	Loss: 0.381006
Step #110 (total examples = 880)	Loss: 0.798545
Step #120 (total examples = 960)	Loss: 0.222927
Step #130 (total examples = 1040)	Loss: 0.093585
Step #140 (total examples = 1120)	Loss: 0.532716
Step #150 (total examples = 1200)	Loss: 0.480093
Step #160 (total examples = 1280)	Loss: 0.250934
Step #170 (total examples = 1360)	Loss: 0.064236
Step #180 (total examples = 1440)	Loss: 0.145895
Step #190 (total examples = 1520)	Loss: 0.058937
Step #200 (total examples = 1600)	Loss: 0.072887
Step #210 (total examples = 1680)	Loss: 0.399116
Step #220 (total examples = 1760)	Loss: 0.277697
Step #230 (total examples = 1840)	Loss: 0.306752
Step #240 (total examples = 1920)	Loss: 0.006571
Step #250 (total examples = 2000)	Loss: 0.006429
Step #260 (total examples = 2080)	Loss: 0.070958
Step #270 (total examples = 2160)	Loss: 0.030394
Step #280 (total examples = 2240)	Loss: 0.320133
Step #290 (total examples = 2320)	Loss: 0.073046
Step #300 (total examples = 2400)	Loss: 0.117937
Step #310 (total examples = 2480)	Loss: 0.011210
Step #320 (total examples = 2560)	Loss: 0.101989
Step #330 (total examples = 2640)	Loss: 0.025161
Step #340 (total examples = 2720)	Loss: 0.064438
Step #350 (total examples = 2800)	Loss: 0.114924
Step #360 (total examples = 2880)	Loss: 0.011458
Step #370 (total examples = 2960)	Loss: 0.148116
Step #380 (total examples = 3040)	Loss: 0.049350
Step #390 (total examples = 3120)	Loss: 0.617466
Step #400 (total examples = 3200)	Loss: 0.335173
Step #410 (total examples = 3280)	Loss: 0.383035
Step #420 (total examples = 3360)	Loss: 0.005539
Step #430 (total examples = 3440)	Loss: 0.002819
Step #440 (total examples = 3520)	Loss: 0.036138
Step #450 (total examples = 3600)	Loss: 0.049050
Step #460 (total examples = 3680)	Loss: 0.023244
Step #470 (total examples = 3760)	Loss: 0.201326
Step #480 (total examples = 3840)	Loss: 0.054380
Step #490 (total examples = 3920)	Loss: 0.293321
Step #500 (total examples = 4000)	Loss: 0.036484
Step #510 (total examples = 4080)	Loss: 0.048446
Step #520 (total examples = 4160)	Loss: 0.183973
Step #530 (total examples = 4240)	Loss: 0.134458
Step #540 (total examples = 4320)	Loss: 0.012713
Step #550 (total examples = 4400)	Loss: 0.136452
Step #560 (total examples = 4480)	Loss: 0.067610
Step #570 (total examples = 4560)	Loss: 0.845001
Step #580 (total examples = 4640)	Loss: 0.034296
Step #590 (total examples = 4720)	Loss: 0.692208
Step #600 (total examples = 4800)	Loss: 0.046084
Step #610 (total examples = 4880)	Loss: 0.205737
Step #620 (total examples = 4960)	Loss: 0.024915
Step #630 (total examples = 5040)	Loss: 0.014755
Step #640 (total examples = 5120)	Loss: 0.074889
Step #650 (total examples = 5200)	Loss: 0.039336
Step #660 (total examples = 5280)	Loss: 0.270974
Step #670 (total examples = 5360)	Loss: 0.601407
Step #680 (total examples = 5440)	Loss: 0.038449
Step #690 (total examples = 5520)	Loss: 0.010678
Step #700 (total examples = 5600)	Loss: 0.212196
Step #710 (total examples = 5680)	Loss: 0.014807
Step #720 (total examples = 5760)	Loss: 0.023597
Step #730 (total examples = 5840)	Loss: 0.122168
Step #740 (total examples = 5920)	Loss: 0.282640
Step #750 (total examples = 6000)	Loss: 0.137327
Step #760 (total examples = 6080)	Loss: 0.008940
Step #770 (total examples = 6160)	Loss: 0.002957
Step #780 (total examples = 6240)	Loss: 0.100398
Step #790 (total examples = 6320)	Loss: 0.009293
Step #800 (total examples = 6400)	Loss: 0.040756
Step #810 (total examples = 6480)	Loss: 0.086948
Step #820 (total examples = 6560)	Loss: 0.078960
Step #830 (total examples = 6640)	Loss: 0.007412
Step #840 (total examples = 6720)	Loss: 0.001744
Step #850 (total examples = 6800)	Loss: 0.010999
Step #860 (total examples = 6880)	Loss: 0.016814
Step #870 (total examples = 6960)	Loss: 0.046434
Step #880 (total examples = 7040)	Loss: 0.199285
Step #890 (total examples = 7120)	Loss: 0.092946
Step #900 (total examples = 7200)	Loss: 0.077450
Step #910 (total examples = 7280)	Loss: 0.003338
Step #920 (total examples = 7360)	Loss: 0.013025
Step #930 (total examples = 7440)	Loss: 0.312455
Step #940 (total examples = 7520)	Loss: 0.045443
Step #950 (total examples = 7600)	Loss: 0.056724
Step #960 (total examples = 7680)	Loss: 0.030965
Step #970 (total examples = 7760)	Loss: 0.692986
Step #980 (total examples = 7840)	Loss: 0.022368
Step #990 (total examples = 7920)	Loss: 0.146441
Step #1000 (total examples = 8000)	Loss: 0.004338
Step #1010 (total examples = 8080)	Loss: 0.015201
Step #1020 (total examples = 8160)	Loss: 0.017200
Step #1030 (total examples = 8240)	Loss: 0.030830
Step #1040 (total examples = 8320)	Loss: 0.005178
Step #1050 (total examples = 8400)	Loss: 0.002422
Step #1060 (total examples = 8480)	Loss: 0.002831
Step #1070 (total examples = 8560)	Loss: 0.000888
Step #1080 (total examples = 8640)	Loss: 0.757795
Step #1090 (total examples = 8720)	Loss: 0.339598
Step #1100 (total examples = 8800)	Loss: 0.130838
Step #1110 (total examples = 8880)	Loss: 0.023019
Step #1120 (total examples = 8960)	Loss: 0.001527
Step #1130 (total examples = 9040)	Loss: 0.001570
Step #1140 (total examples = 9120)	Loss: 0.031515
Step #1150 (total examples = 9200)	Loss: 0.056235
Step #1160 (total examples = 9280)	Loss: 0.115476
Step #1170 (total examples = 9360)	Loss: 0.001812
Step #1180 (total examples = 9440)	Loss: 0.019139
Step #1190 (total examples = 9520)	Loss: 0.006513
Step #1200 (total examples = 9600)	Loss: 0.008137
Step #1210 (total examples = 9680)	Loss: 0.030329
Step #1220 (total examples = 9760)	Loss: 0.024736
Step #1230 (total examples = 9840)	Loss: 0.035761
Step #1240 (total examples = 9920)	Loss: 0.007722
Step #1250 (total examples = 10000)	Loss: 0.065577
Step #1260 (total examples = 10080)	Loss: 0.037273
Step #1270 (total examples = 10160)	Loss: 0.012770
Step #1280 (total examples = 10240)	Loss: 0.000142
Step #1290 (total examples = 10320)	Loss: 0.017093
Step #1300 (total examples = 10400)	Loss: 0.051327
Step #1310 (total examples = 10480)	Loss: 0.009933
Step #1320 (total examples = 10560)	Loss: 0.113051
Step #1330 (total examples = 10640)	Loss: 0.068361
Step #1340 (total examples = 10720)	Loss: 0.028514
Step #1350 (total examples = 10800)	Loss: 0.034439
Step #1360 (total examples = 10880)	Loss: 0.027618
Step #1370 (total examples = 10960)	Loss: 0.020834
Step #1380 (total examples = 11040)	Loss: 0.178123
Step #1390 (total examples = 11120)	Loss: 0.002273
Step #1400 (total examples = 11200)	Loss: 0.005507
Step #1410 (total examples = 11280)	Loss: 0.165394
Step #1420 (total examples = 11360)	Loss: 0.009145
Step #1430 (total examples = 11440)	Loss: 0.028600
Step #1440 (total examples = 11520)	Loss: 0.222152
Step #1450 (total examples = 11600)	Loss: 0.427301
Step #1460 (total examples = 11680)	Loss: 0.733627
Step #1470 (total examples = 11760)	Loss: 0.198651
Step #1480 (total examples = 11840)	Loss: 0.000836
Step #1490 (total examples = 11920)	Loss: 0.050915
Step #1500 (total examples = 12000)	Loss: 0.010237
Step #1510 (total examples = 12080)	Loss: 0.000126
Step #1520 (total examples = 12160)	Loss: 0.136964
Step #1530 (total examples = 12240)	Loss: 0.315300
Step #1540 (total examples = 12320)	Loss: 0.092886
Step #1550 (total examples = 12400)	Loss: 0.139381
Step #1560 (total examples = 12480)	Loss: 0.094320
Step #1570 (total examples = 12560)	Loss: 0.042795
Step #1580 (total examples = 12640)	Loss: 0.007746
Step #1590 (total examples = 12720)	Loss: 0.085623
Step #1600 (total examples = 12800)	Loss: 0.074705
Step #1610 (total examples = 12880)	Loss: 0.063429
Step #1620 (total examples = 12960)	Loss: 0.009918
Step #1630 (total examples = 13040)	Loss: 0.070847===========
rank: 1
===========
Step #0 (total examples = 0)	Loss: 2.288079
Step #10 (total examples = 80)	Loss: 1.973546
Step #20 (total examples = 160)	Loss: 1.268986
Step #30 (total examples = 240)	Loss: 1.153693
Step #40 (total examples = 320)	Loss: 0.316546
Step #50 (total examples = 400)	Loss: 0.727979
Step #60 (total examples = 480)	Loss: 0.623423
Step #70 (total examples = 560)	Loss: 0.294122
Step #80 (total examples = 640)	Loss: 1.064970
Step #90 (total examples = 720)	Loss: 0.132642
Step #100 (total examples = 800)	Loss: 0.011948
Step #110 (total examples = 880)	Loss: 0.233826
Step #120 (total examples = 960)	Loss: 0.285294
Step #130 (total examples = 1040)	Loss: 0.119554
Step #140 (total examples = 1120)	Loss: 0.395635
Step #150 (total examples = 1200)	Loss: 0.086901
Step #160 (total examples = 1280)	Loss: 0.084252
Step #170 (total examples = 1360)	Loss: 0.174981
Step #180 (total examples = 1440)	Loss: 0.180398
Step #190 (total examples = 1520)	Loss: 0.052958
Step #200 (total examples = 1600)	Loss: 0.210652
Step #210 (total examples = 1680)	Loss: 0.495483
Step #220 (total examples = 1760)	Loss: 0.316727
Step #230 (total examples = 1840)	Loss: 0.198261
Step #240 (total examples = 1920)	Loss: 0.028465
Step #250 (total examples = 2000)	Loss: 0.014021
Step #260 (total examples = 2080)	Loss: 0.360317
Step #270 (total examples = 2160)	Loss: 0.136522
Step #280 (total examples = 2240)	Loss: 0.359734
Step #290 (total examples = 2320)	Loss: 0.152085
Step #300 (total examples = 2400)	Loss: 0.227830
Step #310 (total examples = 2480)	Loss: 0.084338
Step #320 (total examples = 2560)	Loss: 0.324687
Step #330 (total examples = 2640)	Loss: 0.125358
Step #340 (total examples = 2720)	Loss: 0.147086
Step #350 (total examples = 2800)	Loss: 0.065882
Step #360 (total examples = 2880)	Loss: 0.240269
Step #370 (total examples = 2960)	Loss: 0.046099
Step #380 (total examples = 3040)	Loss: 0.041373
Step #390 (total examples = 3120)	Loss: 0.107532
Step #400 (total examples = 3200)	Loss: 0.197272
Step #410 (total examples = 3280)	Loss: 0.136003
Step #420 (total examples = 3360)	Loss: 0.041237
Step #430 (total examples = 3440)	Loss: 0.058459
Step #440 (total examples = 3520)	Loss: 0.097847
Step #450 (total examples = 3600)	Loss: 0.024122
Step #460 (total examples = 3680)	Loss: 0.172439
Step #470 (total examples = 3760)	Loss: 0.515899
Step #480 (total examples = 3840)	Loss: 0.301526
Step #490 (total examples = 3920)	Loss: 0.005220
Step #500 (total examples = 4000)	Loss: 0.232490
Step #510 (total examples = 4080)	Loss: 0.009050
Step #520 (total examples = 4160)	Loss: 0.490627
Step #530 (total examples = 4240)	Loss: 0.344212
Step #540 (total examples = 4320)	Loss: 0.292443
Step #550 (total examples = 4400)	Loss: 0.006897
Step #560 (total examples = 4480)	Loss: 0.128252
Step #570 (total examples = 4560)	Loss: 0.556545
Step #580 (total examples = 4640)	Loss: 0.118004
Step #590 (total examples = 4720)	Loss: 0.018214
Step #600 (total examples = 4800)	Loss: 0.118422
Step #610 (total examples = 4880)	Loss: 0.007411
Step #620 (total examples = 4960)	Loss: 0.004479
Step #630 (total examples = 5040)	Loss: 0.161929
Step #640 (total examples = 5120)	Loss: 0.193186
Step #650 (total examples = 5200)	Loss: 0.277827
Step #660 (total examples = 5280)	Loss: 0.068447
Step #670 (total examples = 5360)	Loss: 0.151453
Step #680 (total examples = 5440)	Loss: 0.010094
Step #690 (total examples = 5520)	Loss: 0.005843
Step #700 (total examples = 5600)	Loss: 0.089954
Step #710 (total examples = 5680)	Loss: 0.043689
Step #720 (total examples = 5760)	Loss: 0.002165
Step #730 (total examples = 5840)	Loss: 0.048966
Step #740 (total examples = 5920)	Loss: 0.018172
Step #750 (total examples = 6000)	Loss: 0.501555
Step #760 (total examples = 6080)	Loss: 0.150114
Step #770 (total examples = 6160)	Loss: 0.031103
Step #780 (total examples = 6240)	Loss: 0.538882
Step #790 (total examples = 6320)	Loss: 0.012270
Step #800 (total examples = 6400)	Loss: 0.447619
Step #810 (total examples = 6480)	Loss: 0.105328
Step #820 (total examples = 6560)	Loss: 0.006923
Step #830 (total examples = 6640)	Loss: 0.047660
Step #840 (total examples = 6720)	Loss: 0.062995
Step #850 (total examples = 6800)	Loss: 0.168585
Step #860 (total examples = 6880)	Loss: 0.098168
Step #870 (total examples = 6960)	Loss: 0.153220
Step #880 (total examples = 7040)	Loss: 0.066109
Step #890 (total examples = 7120)	Loss: 0.026795
Step #900 (total examples = 7200)	Loss: 0.019952
Step #910 (total examples = 7280)	Loss: 0.002732
Step #920 (total examples = 7360)	Loss: 0.567390
Step #930 (total examples = 7440)	Loss: 0.017025
Step #940 (total examples = 7520)	Loss: 0.006651
Step #950 (total examples = 7600)	Loss: 1.430917
Step #960 (total examples = 7680)	Loss: 0.268845
Step #970 (total examples = 7760)	Loss: 0.065030
Step #980 (total examples = 7840)	Loss: 0.299195
Step #990 (total examples = 7920)	Loss: 0.055736
Step #1000 (total examples = 8000)	Loss: 0.895991
Step #1010 (total examples = 8080)	Loss: 0.162285
Step #1020 (total examples = 8160)	Loss: 0.233985
Step #1030 (total examples = 8240)	Loss: 0.028801
Step #1040 (total examples = 8320)	Loss: 0.005037
Step #1050 (total examples = 8400)	Loss: 0.170178
Step #1060 (total examples = 8480)	Loss: 0.021743
Step #1070 (total examples = 8560)	Loss: 0.020240
Step #1080 (total examples = 8640)	Loss: 0.008817
Step #1090 (total examples = 8720)	Loss: 0.065099
Step #1100 (total examples = 8800)	Loss: 0.156657
Step #1110 (total examples = 8880)	Loss: 0.032292
Step #1120 (total examples = 8960)	Loss: 0.057605
Step #1130 (total examples = 9040)	Loss: 0.218841
Step #1140 (total examples = 9120)	Loss: 0.071474
Step #1150 (total examples = 9200)	Loss: 0.190572
Step #1160 (total examples = 9280)	Loss: 0.132652
Step #1170 (total examples = 9360)	Loss: 0.075946
Step #1180 (total examples = 9440)	Loss: 0.085644
Step #1190 (total examples = 9520)	Loss: 0.021949
Step #1200 (total examples = 9600)	Loss: 0.003075
Step #1210 (total examples = 9680)	Loss: 0.052319
Step #1220 (total examples = 9760)	Loss: 0.053722
Step #1230 (total examples = 9840)	Loss: 0.263044
Step #1240 (total examples = 9920)	Loss: 0.122825
Step #1250 (total examples = 10000)	Loss: 0.000128
Step #1260 (total examples = 10080)	Loss: 0.201399
Step #1270 (total examples = 10160)	Loss: 0.090061
Step #1280 (total examples = 10240)	Loss: 0.022771
Step #1290 (total examples = 10320)	Loss: 0.056484
Step #1300 (total examples = 10400)	Loss: 0.002949
Step #1310 (total examples = 10480)	Loss: 0.016275
Step #1320 (total examples = 10560)	Loss: 0.566957
Step #1330 (total examples = 10640)	Loss: 0.025819
Step #1340 (total examples = 10720)	Loss: 0.002345
Step #1350 (total examples = 10800)	Loss: 0.004264
Step #1360 (total examples = 10880)	Loss: 0.167778
Step #1370 (total examples = 10960)	Loss: 0.004419
Step #1380 (total examples = 11040)	Loss: 0.016485
Step #1390 (total examples = 11120)	Loss: 0.004163
Step #1400 (total examples = 11200)	Loss: 0.270436
Step #1410 (total examples = 11280)	Loss: 0.023460
Step #1420 (total examples = 11360)	Loss: 0.013850
Step #1430 (total examples = 11440)	Loss: 0.128243
Step #1440 (total examples = 11520)	Loss: 0.023377
Step #1450 (total examples = 11600)	Loss: 0.000650
Step #1460 (total examples = 11680)	Loss: 0.001043
Step #1470 (total examples = 11760)	Loss: 0.001556
Step #1480 (total examples = 11840)	Loss: 0.064379
Step #1490 (total examples = 11920)	Loss: 0.006912
Step #1500 (total examples = 12000)	Loss: 0.000857
Step #1510 (total examples = 12080)	Loss: 0.062380
Step #1520 (total examples = 12160)	Loss: 0.315948
Step #1530 (total examples = 12240)	Loss: 0.016865
Step #1540 (total examples = 12320)	Loss: 0.840417
Step #1550 (total examples = 12400)	Loss: 0.473789
Step #1560 (total examples = 12480)	Loss: 0.015571
Step #1570 (total examples = 12560)	Loss: 0.082806
Step #1580 (total examples = 12640)	Loss: 0.359423
Step #1590 (total examples = 12720)	Loss: 0.110244
Step #1600 (total examples = 12800)	Loss: 0.378272
Step #1610 (total examples = 12880)	Loss: 0.019333
Step #1620 (total examples = 12960)	Loss: 0.002038
Step #1630 (total examples = 13040)	Loss: 0.105358
Step #1640 (total examples = 13120)	Loss: 0.011153
Step #1650 (total examples = 13200)	Loss: 0.037976
Step #1660 (total examples = 13280)	Loss: 0.253167
Step #1670 (total examples = 13360)	Loss: 0.013854
Step #1680 (total examples = 13440)	Loss: 0.018250
Step #1690 (total examples = 13520)	Loss: 0.010089
Step #1700 (total examples = 13600)	Loss: 0.009202
Step #1710 (total examples = 13680)	Loss: 0.007720
Step #1720 (total examples = 13760)	Loss: 0.013902
Step #1730 (total examples = 13840)	Loss: 0.329668
Step #1740 (total examples = 13920)	Loss: 0.015707
Step #1750 (total examples = 14000)	Loss: 0.287727
Step #1760 (total examples = 14080)	Loss: 0.083063
Step #1770 (total examples = 14160)	Loss: 0.003703
Step #1780 (total examples = 14240)	Loss: 0.006274
Step #1790 (total examples = 14320)	Loss: 0.068995
Step #1800 (total examples = 14400)	Loss: 0.002982
Step #1810 (total examples = 14480)	Loss: 0.090031
Step #1820 (total examples = 14560)	Loss: 0.212681
Step #1830 (total examples = 14640)	Loss: 0.024906
Step #1840 (total examples = 14720)	Loss: 0.303271
Step #1850 (total examples = 14800)	Loss: 0.076749
Step #1860 (total examples = 14880)	Loss: 0.292790
Step #1870 (total examples = 14960)	Loss: 0.001868
Step #1880 (total examples = 15040)	Loss: 0.118349
Step #1890 (total examples = 15120)	Loss: 0.011634
Step #1900 (total examples = 15200)	Loss: 0.005824
Step #1910 (total examples = 15280)	Loss: 0.023566
Step #1920 (total examples = 15360)	Loss: 0.001508
Step #1930 (total examples = 15440)	Loss: 0.090980
Step #1940 (total examples = 15520)	Loss: 0.343093
Step #1950 (total examples = 15600)	Loss: 0.006024
Step #1960 (total examples = 15680)	Loss: 0.049603
Step #1970 (total examples = 15760)	Loss: 0.008422
Step #1980 (total examples = 15840)	Loss: 0.095660
Step #1990 (total examples = 15920)	Loss: 0.002222

Step #1640 (total examples = 13120)	Loss: 0.450023
Step #1650 (total examples = 13200)	Loss: 0.003763
Step #1660 (total examples = 13280)	Loss: 0.008588
Step #1670 (total examples = 13360)	Loss: 0.018682
Step #1680 (total examples = 13440)	Loss: 0.001481
Step #1690 (total examples = 13520)	Loss: 0.170862
Step #1700 (total examples = 13600)	Loss: 0.010832
Step #1710 (total examples = 13680)	Loss: 0.103663
Step #1720 (total examples = 13760)	Loss: 0.355558
Step #1730 (total examples = 13840)	Loss: 0.314864
Step #1740 (total examples = 13920)	Loss: 0.015942
Step #1750 (total examples = 14000)	Loss: 0.126216
Step #1760 (total examples = 14080)	Loss: 0.013335
Step #1770 (total examples = 14160)	Loss: 0.219531
Step #1780 (total examples = 14240)	Loss: 0.055879
Step #1790 (total examples = 14320)	Loss: 0.003278
Step #1800 (total examples = 14400)	Loss: 0.553086
Step #1810 (total examples = 14480)	Loss: 0.020695
Step #1820 (total examples = 14560)	Loss: 0.009475
Step #1830 (total examples = 14640)	Loss: 1.021006
Step #1840 (total examples = 14720)	Loss: 0.012805
Step #1850 (total examples = 14800)	Loss: 0.014375
Step #1860 (total examples = 14880)	Loss: 0.029693
Step #1870 (total examples = 14960)	Loss: 0.492636
Step #1880 (total examples = 15040)	Loss: 0.030216
Step #1890 (total examples = 15120)	Loss: 0.023143
Step #1900 (total examples = 15200)	Loss: 0.009123
Step #1910 (total examples = 15280)	Loss: 0.011612
Step #1920 (total examples = 15360)	Loss: 0.291445
Step #1930 (total examples = 15440)	Loss: 0.001175
Step #1940 (total examples = 15520)	Loss: 0.001633
Step #1950 (total examples = 15600)	Loss: 0.004152
Step #1960 (total examples = 15680)	Loss: 0.050288
Step #1970 (total examples = 15760)	Loss: 0.014240
Step #1980 (total examples = 15840)	Loss: 0.413781
Step #1990 (total examples = 15920)	Loss: 0.012952
  1/313 [..............................] - ETA: 2:25 - loss: 0.0000e+00 - accuracy: 1.0000  3/313 [..............................] - ETA: 11s - loss: 0.0000e+00 - accuracy: 1.0000   5/313 [..............................] - ETA: 11s - loss: 2.9012 - accuracy: 0.9937      7/313 [..............................] - ETA: 13s - loss: 2.0723 - accuracy: 0.9955  8/313 [..............................] - ETA: 15s - loss: 2.0500 - accuracy: 0.9922  9/313 [..............................] - ETA: 15s - loss: 4.5641 - accuracy: 0.9896 10/313 [..............................] - ETA: 16s - loss: 4.1077 - accuracy: 0.9906 11/313 [>.............................] - ETA: 17s - loss: 9.8675 - accuracy: 0.9858 12/313 [>.............................] - ETA: 17s - loss: 9.0526 - accuracy: 0.9844 13/313 [>.............................] - ETA: 17s - loss: 8.3562 - accuracy: 0.9856 14/313 [>.............................] - ETA: 18s - loss: 10.9847 - accuracy: 0.9799 15/313 [>.............................] - ETA: 18s - loss: 10.2524 - accuracy: 0.9812 16/313 [>.............................] - ETA: 18s - loss: 11.4786 - accuracy: 0.9785 17/313 [>.............................] - ETA: 18s - loss: 10.8034 - accuracy: 0.9798 18/313 [>.............................] - ETA: 18s - loss: 10.2032 - accuracy: 0.9809 19/313 [>.............................] - ETA: 18s - loss: 9.8404 - accuracy: 0.9803  21/313 [=>............................] - ETA: 18s - loss: 11.0627 - accuracy: 0.9777 23/313 [=>............................] - ETA: 17s - loss: 11.9284 - accuracy: 0.9769 25/313 [=>............................] - ETA: 16s - loss: 13.4735 - accuracy: 0.9762 27/313 [=>............................] - ETA: 16s - loss: 12.7630 - accuracy: 0.9769 29/313 [=>............................] - ETA: 15s - loss: 13.6344 - accuracy: 0.9763 31/313 [=>............................] - ETA: 15s - loss: 14.0794 - accuracy: 0.9748 33/313 [==>...........................] - ETA: 14s - loss: 15.0523 - accuracy: 0.9744 35/313 [==>...........................] - ETA: 14s - loss: 15.2702 - accuracy: 0.9741 37/313 [==>...........................] - ETA: 14s - loss: 14.9559 - accuracy: 0.9730 39/313 [==>...........................] - ETA: 13s - loss: 15.9556 - accuracy: 0.9704 41/313 [==>...........................] - ETA: 13s - loss: 15.9481 - accuracy: 0.9703 43/313 [===>..........................] - ETA: 13s - loss: 15.2063 - accuracy: 0.9717 45/313 [===>..........................] - ETA: 12s - loss: 14.9417 - accuracy: 0.9701 47/313 [===>..........................] - ETA: 12s - loss: 14.4729 - accuracy: 0.9707 49/313 [===>..........................] - ETA: 12s - loss: 15.7080 - accuracy: 0.9688 51/313 [===>..........................] - ETA: 12s - loss: 15.0920 - accuracy: 0.9700 53/313 [====>.........................] - ETA: 12s - loss: 15.1970 - accuracy: 0.9693 55/313 [====>.........................] - ETA: 11s - loss: 15.7612 - accuracy: 0.9688 57/313 [====>.........................] - ETA: 11s - loss: 15.5262 - accuracy: 0.9688 59/313 [====>.........................] - ETA: 11s - loss: 14.9999 - accuracy: 0.9698 61/313 [====>.........................] - ETA: 11s - loss: 14.6359 - accuracy: 0.9703 63/313 [=====>........................] - ETA: 11s - loss: 14.1712 - accuracy: 0.9712 65/313 [=====>........................] - ETA: 11s - loss: 14.9777 - accuracy: 0.9697 67/313 [=====>........................] - ETA: 10s - loss: 16.1782 - accuracy: 0.9683 69/313 [=====>........................] - ETA: 10s - loss: 16.8811 - accuracy: 0.9674 71/313 [=====>........................] - ETA: 10s - loss: 16.4607 - accuracy: 0.9679 73/313 [=====>........................] - ETA: 10s - loss: 16.4502 - accuracy: 0.9679 75/313 [======>.......................] - ETA: 10s - loss: 16.0115 - accuracy: 0.9688 77/313 [======>.......................] - ETA: 10s - loss: 16.2980 - accuracy: 0.9683 79/313 [======>.......................] - ETA: 10s - loss: 15.9293 - accuracy: 0.9688 81/313 [======>.......................] - ETA: 10s - loss: 15.5359 - accuracy: 0.9695 83/313 [======>.......................] - ETA: 9s - loss: 15.9102 - accuracy: 0.9688  85/313 [=======>......................] - ETA: 9s - loss: 15.6370 - accuracy: 0.9691 87/313 [=======>......................] - ETA: 9s - loss: 15.4737 - accuracy: 0.9695 89/313 [=======>......................] - ETA: 9s - loss: 15.2822 - accuracy: 0.9698 91/313 [=======>......................] - ETA: 9s - loss: 15.3380 - accuracy: 0.9698 93/313 [=======>......................] - ETA: 9s - loss: 15.4519 - accuracy: 0.9694 95/313 [========>.....................] - ETA: 9s - loss: 15.1964 - accuracy: 0.9694 97/313 [========>.....................] - ETA: 9s - loss: 14.9819 - accuracy: 0.9697 99/313 [========>.....................] - ETA: 9s - loss: 14.6792 - accuracy: 0.9703101/313 [========>.....................] - ETA: 8s - loss: 14.3885 - accuracy: 0.9709103/313 [========>.....................] - ETA: 8s - loss: 14.2621 - accuracy: 0.9709105/313 [=========>....................] - ETA: 8s - loss: 14.2275 - accuracy: 0.9711107/313 [=========>....................] - ETA: 8s - loss: 14.0847 - accuracy: 0.9711109/313 [=========>....................] - ETA: 8s - loss: 13.8263 - accuracy: 0.9716111/313 [=========>....................] - ETA: 8s - loss: 14.2799 - accuracy: 0.9716113/313 [=========>....................] - ETA: 8s - loss: 14.2039 - accuracy: 0.9715115/313 [==========>...................] - ETA: 8s - loss: 13.9568 - accuracy: 0.9720117/313 [==========>...................] - ETA: 8s - loss: 14.0157 - accuracy: 0.9717119/313 [==========>...................] - ETA: 8s - loss: 14.4494 - accuracy: 0.9709121/313 [==========>...................] - ETA: 7s - loss: 14.9982 - accuracy: 0.9706123/313 [==========>...................] - ETA: 7s - loss: 14.9743 - accuracy: 0.9708125/313 [==========>...................] - ETA: 7s - loss: 15.1181 - accuracy: 0.9710127/313 [===========>..................] - ETA: 7s - loss: 15.1683 - accuracy: 0.9710129/313 [===========>..................] - ETA: 7s - loss: 15.0140 - accuracy: 0.9709131/313 [===========>..................] - ETA: 7s - loss: 15.2969 - accuracy: 0.9709133/313 [===========>..................] - ETA: 7s - loss: 15.4299 - accuracy: 0.9704135/313 [===========>..................] - ETA: 7s - loss: 15.6243 - accuracy: 0.9704137/313 [============>.................] - ETA: 7s - loss: 15.4277 - accuracy: 0.9706139/313 [============>.................] - ETA: 7s - loss: 15.2057 - accuracy: 0.9710141/313 [============>.................] - ETA: 7s - loss: 15.0648 - accuracy: 0.9707143/313 [============>.................] - ETA: 6s - loss: 15.0447 - accuracy: 0.9707145/313 [============>.................] - ETA: 6s - loss: 14.9072 - accuracy: 0.9709147/313 [=============>................] - ETA: 6s - loss: 14.7044 - accuracy: 0.9713149/313 [=============>................] - ETA: 6s - loss: 14.8901 - accuracy: 0.9711151/313 [=============>................] - ETA: 6s - loss: 14.6928 - accuracy: 0.9714153/313 [=============>................] - ETA: 6s - loss: 14.6309 - accuracy: 0.9714155/313 [=============>................] - ETA: 6s - loss: 14.6040 - accuracy: 0.9714157/313 [==============>...............] - ETA: 6s - loss: 14.4180 - accuracy: 0.9717159/313 [==============>...............] - ETA: 6s - loss: 14.2915 - accuracy: 0.9719161/313 [==============>...............] - ETA: 6s - loss: 14.1139 - accuracy: 0.9722163/313 [==============>...............] - ETA: 6s - loss: 14.0112 - accuracy: 0.9724165/313 [==============>...............] - ETA: 5s - loss: 13.8413 - accuracy: 0.9727167/313 [===============>..............] - ETA: 5s - loss: 13.6756 - accuracy: 0.9731169/313 [===============>..............] - ETA: 5s - loss: 13.5137 - accuracy: 0.9734171/313 [===============>..............] - ETA: 5s - loss: 13.3557 - accuracy: 0.9737173/313 [===============>..............] - ETA: 5s - loss: 13.2013 - accuracy: 0.9740175/313 [===============>..............] - ETA: 5s - loss: 13.0504 - accuracy: 0.9743177/313 [===============>..............] - ETA: 5s - loss: 13.1727 - accuracy: 0.9742179/313 [================>.............] - ETA: 5s - loss: 13.0255 - accuracy: 0.9745181/313 [================>.............] - ETA: 5s - loss: 12.8816 - accuracy: 0.9748183/313 [================>.............] - ETA: 5s - loss: 12.7408 - accuracy: 0.9751185/313 [================>.............] - ETA: 5s - loss: 12.6502 - accuracy: 0.9752187/313 [================>.............] - ETA: 5s - loss: 13.0394 - accuracy: 0.9748189/313 [=================>............] - ETA: 4s - loss: 12.9015 - accuracy: 0.9750191/313 [=================>............] - ETA: 4s - loss: 13.2306 - accuracy: 0.9748193/313 [=================>............] - ETA: 4s - loss: 13.2825 - accuracy: 0.9744195/313 [=================>............] - ETA: 4s - loss: 13.1462 - accuracy: 0.9747197/313 [=================>............] - ETA: 4s - loss: 13.0128 - accuracy: 0.9749199/313 [==================>...........] - ETA: 4s - loss: 12.8820 - accuracy: 0.9752201/313 [==================>...........] - ETA: 4s - loss: 12.7538 - accuracy: 0.9754203/313 [==================>...........] - ETA: 4s - loss: 12.6282 - accuracy: 0.9757205/313 [==================>...........] - ETA: 4s - loss: 12.7539 - accuracy: 0.9756207/313 [==================>...........] - ETA: 4s - loss: 12.8906 - accuracy: 0.9755209/313 [===================>..........] - ETA: 4s - loss: 12.8563 - accuracy: 0.9756211/313 [===================>..........] - ETA: 4s - loss: 12.7460 - accuracy: 0.9756213/313 [===================>..........] - ETA: 3s - loss: 12.6263 - accuracy: 0.9758215/313 [===================>..........] - ETA: 3s - loss: 12.5088 - accuracy: 0.9760217/313 [===================>..........] - ETA: 3s - loss: 12.3936 - accuracy: 0.9762219/313 [===================>..........] - ETA: 3s - loss: 12.2804 - accuracy: 0.9765221/313 [====================>.........] - ETA: 3s - loss: 12.1692 - accuracy: 0.9767223/313 [====================>.........] - ETA: 3s - loss: 12.0601 - accuracy: 0.9769225/313 [====================>.........] - ETA: 3s - loss: 11.9529 - accuracy: 0.9771227/313 [====================>.........] - ETA: 3s - loss: 11.8492 - accuracy: 0.9771229/313 [====================>.........] - ETA: 3s - loss: 11.7457 - accuracy: 0.9773231/313 [=====================>........] - ETA: 3s - loss: 11.6440 - accuracy: 0.9775233/313 [=====================>........] - ETA: 3s - loss: 11.7387 - accuracy: 0.9776235/313 [=====================>........] - ETA: 3s - loss: 11.6388 - accuracy: 0.9778237/313 [=====================>........] - ETA: 2s - loss: 11.5406 - accuracy: 0.9780239/313 [=====================>........] - ETA: 2s - loss: 11.4440 - accuracy: 0.9782241/313 [======================>.......] - ETA: 2s - loss: 11.3490 - accuracy: 0.9783243/313 [======================>.......] - ETA: 2s - loss: 11.2556 - accuracy: 0.9785245/313 [======================>.......] - ETA: 2s - loss: 11.1637 - accuracy: 0.9787247/313 [======================>.......] - ETA: 2s - loss: 11.2145 - accuracy: 0.9786249/313 [======================>.......] - ETA: 2s - loss: 11.1749 - accuracy: 0.9787251/313 [=======================>......] - ETA: 2s - loss: 11.1173 - accuracy: 0.9787253/313 [=======================>......] - ETA: 2s - loss: 11.0630 - accuracy: 0.9788255/313 [=======================>......] - ETA: 2s - loss: 10.9762 - accuracy: 0.9789257/313 [=======================>......] - ETA: 2s - loss: 10.8908 - accuracy: 0.9791259/313 [=======================>......] - ETA: 2s - loss: 10.8067 - accuracy: 0.9792261/313 [========================>.....] - ETA: 2s - loss: 10.8105 - accuracy: 0.9792263/313 [========================>.....] - ETA: 1s - loss: 10.7426 - accuracy: 0.9792265/313 [========================>.....] - ETA: 1s - loss: 10.6615 - accuracy: 0.9794267/313 [========================>.....] - ETA: 1s - loss: 10.7021 - accuracy: 0.9793269/313 [========================>.....] - ETA: 1s - loss: 10.6226 - accuracy: 0.9794271/313 [========================>.....] - ETA: 1s - loss: 10.5442 - accuracy: 0.9796273/313 [=========================>....] - ETA: 1s - loss: 10.4669 - accuracy: 0.9797275/313 [=========================>....] - ETA: 1s - loss: 10.3908 - accuracy: 0.9799277/313 [=========================>....] - ETA: 1s - loss: 10.3158 - accuracy: 0.9800279/313 [=========================>....] - ETA: 1s - loss: 10.2418 - accuracy: 0.9802281/313 [=========================>....] - ETA: 1s - loss: 10.1689 - accuracy: 0.9803283/313 [==========================>...] - ETA: 1s - loss: 10.1823 - accuracy: 0.9801285/313 [==========================>...] - ETA: 1s - loss: 10.1109 - accuracy: 0.9803287/313 [==========================>...] - ETA: 1s - loss: 10.0404 - accuracy: 0.9804289/313 [==========================>...] - ETA: 0s - loss: 9.9710 - accuracy: 0.9805 291/313 [==========================>...] - ETA: 0s - loss: 9.9024 - accuracy: 0.9807293/313 [===========================>..] - ETA: 0s - loss: 9.8348 - accuracy: 0.9808295/313 [===========================>..] - ETA: 0s - loss: 9.7682 - accuracy: 0.9809297/313 [===========================>..] - ETA: 0s - loss: 9.7024 - accuracy: 0.9811299/313 [===========================>..] - ETA: 0s - loss: 9.6890 - accuracy: 0.9810301/313 [===========================>..] - ETA: 0s - loss: 9.6246 - accuracy: 0.9811303/313 [============================>.] - ETA: 0s - loss: 9.7131 - accuracy: 0.9810305/313 [============================>.] - ETA: 0s - loss: 9.8994 - accuracy: 0.9808307/313 [============================>.] - ETA: 0s - loss: 10.1373 - accuracy: 0.9805309/313 [============================>.] - ETA: 0s - loss: 10.2520 - accuracy: 0.9804311/313 [============================>.] - ETA: 0s - loss: 10.2605 - accuracy: 0.9801313/313 [==============================] - ETA: 0s - loss: 10.3340 - accuracy: 0.9801313/313 [==============================] - 13s 39ms/step - loss: 10.3340 - accuracy: 0.9801
Final model accuracy: 0.980100
Total training time: 160.653413
2021-12-03 20:20:56.078118: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 20:20:56.078215: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-12-03 20:20:56.138093: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 20:20:56.138334: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-12-03 20:21:00.424435: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 20:21:00.424600: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2021-12-03 20:21:00.424650: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (node-0005): /proc/driver/nvidia/version does not exist
2021-12-03 20:21:00.503235: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 20:21:00.503467: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2021-12-03 20:21:00.503537: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (node-0001): /proc/driver/nvidia/version does not exist
===========
rank: 0
Batch size: 8
Total number of batches: 4000
Total training examples: 32000
Num of workers: 2
===========
Step #0 (total examples = 0)	Loss: 2.304537
Step #10 (total examples = 80)	Loss: 1.857727
Step #20 (total examples = 160)	Loss: 1.158017
Step #30 (total examples = 240)	Loss: 0.685242
Step #40 (total examples = 320)	Loss: 0.625054
Step #50 (total examples = 400)	Loss: 0.482231
Step #60 (total examples = 480)	Loss: 1.248035
Step #70 (total examples = 560)	Loss: 0.057872
Step #80 (total examples = 640)	Loss: 0.904368
Step #90 (total examples = 720)	Loss: 0.499493
Step #100 (total examples = 800)	Loss: 0.441190
Step #110 (total examples = 880)	Loss: 0.804800
Step #120 (total examples = 960)	Loss: 0.282467
Step #130 (total examples = 1040)	Loss: 0.082813
Step #140 (total examples = 1120)	Loss: 1.051131
Step #150 (total examples = 1200)	Loss: 0.201584
Step #160 (total examples = 1280)	Loss: 0.363780
Step #170 (total examples = 1360)	Loss: 0.133295
Step #180 (total examples = 1440)	Loss: 0.174529
Step #190 (total examples = 1520)	Loss: 0.282496
Step #200 (total examples = 1600)	Loss: 0.058256
Step #210 (total examples = 1680)	Loss: 0.441266
Step #220 (total examples = 1760)	Loss: 0.426271
Step #230 (total examples = 1840)	Loss: 0.123437
Step #240 (total examples = 1920)	Loss: 0.002616
Step #250 (total examples = 2000)	Loss: 0.057408
Step #260 (total examples = 2080)	Loss: 0.066335
Step #270 (total examples = 2160)	Loss: 0.279829
Step #280 (total examples = 2240)	Loss: 0.283901
Step #290 (total examples = 2320)	Loss: 0.095928
Step #300 (total examples = 2400)	Loss: 0.233017
Step #310 (total examples = 2480)	Loss: 0.082407
Step #320 (total examples = 2560)	Loss: 0.118556
Step #330 (total examples = 2640)	Loss: 0.015179
Step #340 (total examples = 2720)	Loss: 0.057004
Step #350 (total examples = 2800)	Loss: 0.063905
Step #360 (total examples = 2880)	Loss: 0.070471
Step #370 (total examples = 2960)	Loss: 0.397020
Step #380 (total examples = 3040)	Loss: 0.529098
Step #390 (total examples = 3120)	Loss: 1.047626
Step #400 (total examples = 3200)	Loss: 0.308864
Step #410 (total examples = 3280)	Loss: 0.118672
Step #420 (total examples = 3360)	Loss: 0.011350
Step #430 (total examples = 3440)	Loss: 0.003948
Step #440 (total examples = 3520)	Loss: 0.077864
Step #450 (total examples = 3600)	Loss: 0.054715
Step #460 (total examples = 3680)	Loss: 0.035084
Step #470 (total examples = 3760)	Loss: 0.106801
Step #480 (total examples = 3840)	Loss: 0.035403
Step #490 (total examples = 3920)	Loss: 0.795274
Step #500 (total examples = 4000)	Loss: 0.066835
Step #510 (total examples = 4080)	Loss: 0.232829
Step #520 (total examples = 4160)	Loss: 0.196528
Step #530 (total examples = 4240)	Loss: 0.004070
Step #540 (total examples = 4320)	Loss: 0.014815
Step #550 (total examples = 4400)	Loss: 0.029715
Step #560 (total examples = 4480)	Loss: 0.000263
Step #570 (total examples = 4560)	Loss: 0.809133
Step #580 (total examples = 4640)	Loss: 0.011465
Step #590 (total examples = 4720)	Loss: 0.481073
Step #600 (total examples = 4800)	Loss: 0.080184
Step #610 (total examples = 4880)	Loss: 0.199554
Step #620 (total examples = 4960)	Loss: 0.012484
Step #630 (total examples = 5040)	Loss: 0.036817
Step #640 (total examples = 5120)	Loss: 0.096321
Step #650 (total examples = 5200)	Loss: 0.217154
Step #660 (total examples = 5280)	Loss: 0.150684
Step #670 (total examples = 5360)	Loss: 0.130961
Step #680 (total examples = 5440)	Loss: 0.148267
Step #690 (total examples = 5520)	Loss: 0.010436
Step #700 (total examples = 5600)	Loss: 0.505117
Step #710 (total examples = 5680)	Loss: 0.034720
Step #720 (total examples = 5760)	Loss: 0.042318
Step #730 (total examples = 5840)	Loss: 0.043959
Step #740 (total examples = 5920)	Loss: 0.181465
Step #750 (total examples = 6000)	Loss: 0.110599
Step #760 (total examples = 6080)	Loss: 0.010710
Step #770 (total examples = 6160)	Loss: 0.007672
Step #780 (total examples = 6240)	Loss: 0.014026
Step #790 (total examples = 6320)	Loss: 0.022491
Step #800 (total examples = 6400)	Loss: 0.082716
Step #810 (total examples = 6480)	Loss: 0.038196
Step #820 (total examples = 6560)	Loss: 0.110821
Step #830 (total examples = 6640)	Loss: 0.041103
Step #840 (total examples = 6720)	Loss: 0.001353
Step #850 (total examples = 6800)	Loss: 0.000324
Step #860 (total examples = 6880)	Loss: 0.285784
Step #870 (total examples = 6960)	Loss: 0.029253
Step #880 (total examples = 7040)	Loss: 0.224065
Step #890 (total examples = 7120)	Loss: 0.312192
Step #900 (total examples = 7200)	Loss: 0.023993
Step #910 (total examples = 7280)	Loss: 0.008860
Step #920 (total examples = 7360)	Loss: 0.010497
Step #930 (total examples = 7440)	Loss: 0.757072
Step #940 (total examples = 7520)	Loss: 0.390232
Step #950 (total examples = 7600)	Loss: 0.050335
Step #960 (total examples = 7680)	Loss: 0.114597
Step #970 (total examples = 7760)	Loss: 0.337459
Step #980 (total examples = 7840)	Loss: 0.003967
Step #990 (total examples = 7920)	Loss: 0.069022
Step #1000 (total examples = 8000)	Loss: 0.026017
Step #1010 (total examples = 8080)	Loss: 0.016317
Step #1020 (total examples = 8160)	Loss: 0.049017
Step #1030 (total examples = 8240)	Loss: 0.501012
Step #1040 (total examples = 8320)	Loss: 0.003365
Step #1050 (total examples = 8400)	Loss: 0.005857
Step #1060 (total examples = 8480)	Loss: 0.004428
Step #1070 (total examples = 8560)	Loss: 0.047433
Step #1080 (total examples = 8640)	Loss: 0.364218
Step #1090 (total examples = 8720)	Loss: 0.563778
Step #1100 (total examples = 8800)	Loss: 0.067399
Step #1110 (total examples = 8880)	Loss: 0.001216
Step #1120 (total examples = 8960)	Loss: 0.008894
Step #1130 (total examples = 9040)	Loss: 0.004001
Step #1140 (total examples = 9120)	Loss: 0.203765
Step #1150 (total examples = 9200)	Loss: 0.024890
Step #1160 (total examples = 9280)	Loss: 0.215609
Step #1170 (total examples = 9360)	Loss: 0.004240
Step #1180 (total examples = 9440)	Loss: 0.044443
Step #1190 (total examples = 9520)	Loss: 0.008767
Step #1200 (total examples = 9600)	Loss: 0.042642
Step #1210 (total examples = 9680)	Loss: 0.079248
Step #1220 (total examples = 9760)	Loss: 0.003609
Step #1230 (total examples = 9840)	Loss: 0.079570
Step #1240 (total examples = 9920)	Loss: 0.063784
Step #1250 (total examples = 10000)	Loss: 0.183884
Step #1260 (total examples = 10080)	Loss: 0.006696
Step #1270 (total examples = 10160)	Loss: 0.014658
Step #1280 (total examples = 10240)	Loss: 0.032135
Step #1290 (total examples = 10320)	Loss: 0.003152
Step #1300 (total examples = 10400)	Loss: 0.068824
Step #1310 (total examples = 10480)	Loss: 0.011256
Step #1320 (total examples = 10560)	Loss: 0.051419
Step #1330 (total examples = 10640)	Loss: 0.311240
Step #1340 (total examples = 10720)	Loss: 0.008073
Step #1350 (total examples = 10800)	Loss: 0.049632
Step #1360 (total examples = 10880)	Loss: 0.005511
Step #1370 (total examples = 10960)	Loss: 0.032883
Step #1380 (total examples = 11040)	Loss: 0.064703
Step #1390 (total examples = 11120)	Loss: 0.028686
Step #1400 (total examples = 11200)	Loss: 0.007416
Step #1410 (total examples = 11280)	Loss: 0.038896
Step #1420 (total examples = 11360)	Loss: 0.340486
Step #1430 (total examples = 11440)	Loss: 0.074160
Step #1440 (total examples = 11520)	Loss: 0.042112
Step #1450 (total examples = 11600)	Loss: 0.085680
Step #1460 (total examples = 11680)	Loss: 0.364821
Step #1470 (total examples = 11760)	Loss: 0.244705
Step #1480 (total examples = 11840)	Loss: 0.005256
Step #1490 (total examples = 11920)	Loss: 0.006076
Step #1500 (total examples = 12000)	Loss: 0.075542
Step #1510 (total examples = 12080)	Loss: 0.053659
Step #1520 (total examples = 12160)	Loss: 0.024928
Step #1530 (total examples = 12240)	Loss: 0.347189
Step #1540 (total examples = 12320)	Loss: 0.024284
Step #1550 (total examples = 12400)	Loss: 0.018454
Step #1560 (total examples = 12480)	Loss: 0.002535
Step #1570 (total examples = 12560)	Loss: 0.030664
Step #1580 (total examples = 12640)	Loss: 0.022396
Step #1590 (total examples = 12720)	Loss: 0.017847
Step #1600 (total examples = 12800)	Loss: 0.000565
Step #1610 (total examples = 12880)	Loss: 0.043377
Step #1620 (total examples = 12960)	Loss: 0.007976
Step #1630 (total examples = 13040)	Loss: 0.051005===========
rank: 1
===========
Step #0 (total examples = 0)	Loss: 2.290365
Step #10 (total examples = 80)	Loss: 2.321586
Step #20 (total examples = 160)	Loss: 1.518786
Step #30 (total examples = 240)	Loss: 0.372606
Step #40 (total examples = 320)	Loss: 0.376914
Step #50 (total examples = 400)	Loss: 0.857806
Step #60 (total examples = 480)	Loss: 0.193119
Step #70 (total examples = 560)	Loss: 0.422932
Step #80 (total examples = 640)	Loss: 0.828075
Step #90 (total examples = 720)	Loss: 0.290478
Step #100 (total examples = 800)	Loss: 0.083979
Step #110 (total examples = 880)	Loss: 0.519983
Step #120 (total examples = 960)	Loss: 0.522994
Step #130 (total examples = 1040)	Loss: 0.627365
Step #140 (total examples = 1120)	Loss: 0.220855
Step #150 (total examples = 1200)	Loss: 0.292178
Step #160 (total examples = 1280)	Loss: 0.172432
Step #170 (total examples = 1360)	Loss: 0.163454
Step #180 (total examples = 1440)	Loss: 0.031309
Step #190 (total examples = 1520)	Loss: 0.269889
Step #200 (total examples = 1600)	Loss: 0.398479
Step #210 (total examples = 1680)	Loss: 0.123653
Step #220 (total examples = 1760)	Loss: 0.065519
Step #230 (total examples = 1840)	Loss: 0.748392
Step #240 (total examples = 1920)	Loss: 0.137748
Step #250 (total examples = 2000)	Loss: 0.319270
Step #260 (total examples = 2080)	Loss: 0.127791
Step #270 (total examples = 2160)	Loss: 0.128736
Step #280 (total examples = 2240)	Loss: 1.009898
Step #290 (total examples = 2320)	Loss: 0.286240
Step #300 (total examples = 2400)	Loss: 0.360648
Step #310 (total examples = 2480)	Loss: 0.116547
Step #320 (total examples = 2560)	Loss: 0.029960
Step #330 (total examples = 2640)	Loss: 0.192965
Step #340 (total examples = 2720)	Loss: 0.006522
Step #350 (total examples = 2800)	Loss: 0.060036
Step #360 (total examples = 2880)	Loss: 0.209723
Step #370 (total examples = 2960)	Loss: 0.225045
Step #380 (total examples = 3040)	Loss: 0.013910
Step #390 (total examples = 3120)	Loss: 0.073922
Step #400 (total examples = 3200)	Loss: 0.934164
Step #410 (total examples = 3280)	Loss: 0.113126
Step #420 (total examples = 3360)	Loss: 0.014738
Step #430 (total examples = 3440)	Loss: 0.029422
Step #440 (total examples = 3520)	Loss: 0.078317
Step #450 (total examples = 3600)	Loss: 0.008686
Step #460 (total examples = 3680)	Loss: 0.071808
Step #470 (total examples = 3760)	Loss: 0.189198
Step #480 (total examples = 3840)	Loss: 0.272732
Step #490 (total examples = 3920)	Loss: 0.032183
Step #500 (total examples = 4000)	Loss: 0.717553
Step #510 (total examples = 4080)	Loss: 0.010356
Step #520 (total examples = 4160)	Loss: 0.128990
Step #530 (total examples = 4240)	Loss: 0.296628
Step #540 (total examples = 4320)	Loss: 0.098871
Step #550 (total examples = 4400)	Loss: 0.138259
Step #560 (total examples = 4480)	Loss: 0.053067
Step #570 (total examples = 4560)	Loss: 0.429443
Step #580 (total examples = 4640)	Loss: 0.098630
Step #590 (total examples = 4720)	Loss: 0.064865
Step #600 (total examples = 4800)	Loss: 0.081485
Step #610 (total examples = 4880)	Loss: 0.006536
Step #620 (total examples = 4960)	Loss: 0.089311
Step #630 (total examples = 5040)	Loss: 0.073933
Step #640 (total examples = 5120)	Loss: 0.125034
Step #650 (total examples = 5200)	Loss: 0.082104
Step #660 (total examples = 5280)	Loss: 0.178830
Step #670 (total examples = 5360)	Loss: 0.030756
Step #680 (total examples = 5440)	Loss: 0.046302
Step #690 (total examples = 5520)	Loss: 0.006096
Step #700 (total examples = 5600)	Loss: 0.598933
Step #710 (total examples = 5680)	Loss: 0.018156
Step #720 (total examples = 5760)	Loss: 0.001285
Step #730 (total examples = 5840)	Loss: 0.034025
Step #740 (total examples = 5920)	Loss: 0.001065
Step #750 (total examples = 6000)	Loss: 0.454789
Step #760 (total examples = 6080)	Loss: 0.076625
Step #770 (total examples = 6160)	Loss: 0.109415
Step #780 (total examples = 6240)	Loss: 0.296245
Step #790 (total examples = 6320)	Loss: 0.028602
Step #800 (total examples = 6400)	Loss: 0.915128
Step #810 (total examples = 6480)	Loss: 0.058133
Step #820 (total examples = 6560)	Loss: 0.156496
Step #830 (total examples = 6640)	Loss: 0.032855
Step #840 (total examples = 6720)	Loss: 0.074168
Step #850 (total examples = 6800)	Loss: 0.014714
Step #860 (total examples = 6880)	Loss: 0.028489
Step #870 (total examples = 6960)	Loss: 0.364499
Step #880 (total examples = 7040)	Loss: 0.043458
Step #890 (total examples = 7120)	Loss: 0.003690
Step #900 (total examples = 7200)	Loss: 0.074919
Step #910 (total examples = 7280)	Loss: 0.010711
Step #920 (total examples = 7360)	Loss: 0.853916
Step #930 (total examples = 7440)	Loss: 0.017406
Step #940 (total examples = 7520)	Loss: 0.064044
Step #950 (total examples = 7600)	Loss: 0.214839
Step #960 (total examples = 7680)	Loss: 0.220413
Step #970 (total examples = 7760)	Loss: 0.011267
Step #980 (total examples = 7840)	Loss: 0.125158
Step #990 (total examples = 7920)	Loss: 0.166373
Step #1000 (total examples = 8000)	Loss: 0.215120
Step #1010 (total examples = 8080)	Loss: 0.249758
Step #1020 (total examples = 8160)	Loss: 0.039374
Step #1030 (total examples = 8240)	Loss: 0.435878
Step #1040 (total examples = 8320)	Loss: 0.086424
Step #1050 (total examples = 8400)	Loss: 0.444728
Step #1060 (total examples = 8480)	Loss: 0.178452
Step #1070 (total examples = 8560)	Loss: 0.017683
Step #1080 (total examples = 8640)	Loss: 0.160146
Step #1090 (total examples = 8720)	Loss: 0.808152
Step #1100 (total examples = 8800)	Loss: 0.775983
Step #1110 (total examples = 8880)	Loss: 0.023502
Step #1120 (total examples = 8960)	Loss: 0.054318
Step #1130 (total examples = 9040)	Loss: 0.051398
Step #1140 (total examples = 9120)	Loss: 0.043550
Step #1150 (total examples = 9200)	Loss: 0.002890
Step #1160 (total examples = 9280)	Loss: 0.045814
Step #1170 (total examples = 9360)	Loss: 0.140089
Step #1180 (total examples = 9440)	Loss: 0.486623
Step #1190 (total examples = 9520)	Loss: 0.006070
Step #1200 (total examples = 9600)	Loss: 0.006682
Step #1210 (total examples = 9680)	Loss: 0.314176
Step #1220 (total examples = 9760)	Loss: 0.284795
Step #1230 (total examples = 9840)	Loss: 0.138862
Step #1240 (total examples = 9920)	Loss: 0.027337
Step #1250 (total examples = 10000)	Loss: 0.001998
Step #1260 (total examples = 10080)	Loss: 0.055603
Step #1270 (total examples = 10160)	Loss: 0.237982
Step #1280 (total examples = 10240)	Loss: 0.214685
Step #1290 (total examples = 10320)	Loss: 0.003877
Step #1300 (total examples = 10400)	Loss: 0.140932
Step #1310 (total examples = 10480)	Loss: 0.001653
Step #1320 (total examples = 10560)	Loss: 0.244362
Step #1330 (total examples = 10640)	Loss: 0.005190
Step #1340 (total examples = 10720)	Loss: 0.104988
Step #1350 (total examples = 10800)	Loss: 0.000810
Step #1360 (total examples = 10880)	Loss: 0.240104
Step #1370 (total examples = 10960)	Loss: 0.000721
Step #1380 (total examples = 11040)	Loss: 0.015969
Step #1390 (total examples = 11120)	Loss: 0.002345
Step #1400 (total examples = 11200)	Loss: 0.312389
Step #1410 (total examples = 11280)	Loss: 0.036972
Step #1420 (total examples = 11360)	Loss: 0.028399
Step #1430 (total examples = 11440)	Loss: 0.021199
Step #1440 (total examples = 11520)	Loss: 0.042026
Step #1450 (total examples = 11600)	Loss: 0.000301
Step #1460 (total examples = 11680)	Loss: 0.005633
Step #1470 (total examples = 11760)	Loss: 0.008927
Step #1480 (total examples = 11840)	Loss: 0.004442
Step #1490 (total examples = 11920)	Loss: 0.036344
Step #1500 (total examples = 12000)	Loss: 0.015253
Step #1510 (total examples = 12080)	Loss: 0.045184
Step #1520 (total examples = 12160)	Loss: 0.215246
Step #1530 (total examples = 12240)	Loss: 0.003289
Step #1540 (total examples = 12320)	Loss: 0.342657
Step #1550 (total examples = 12400)	Loss: 0.973257
Step #1560 (total examples = 12480)	Loss: 0.001082
Step #1570 (total examples = 12560)	Loss: 0.184220
Step #1580 (total examples = 12640)	Loss: 0.215474
Step #1590 (total examples = 12720)	Loss: 0.024179
Step #1600 (total examples = 12800)	Loss: 0.008733
Step #1610 (total examples = 12880)	Loss: 0.067811
Step #1620 (total examples = 12960)	Loss: 0.002587
Step #1630 (total examples = 13040)	Loss: 0.103319
Step #1640 (total examples = 13120)	Loss: 0.017184
Step #1650 (total examples = 13200)	Loss: 0.158914
Step #1660 (total examples = 13280)	Loss: 0.015234
Step #1670 (total examples = 13360)	Loss: 0.018257
Step #1680 (total examples = 13440)	Loss: 0.122808
Step #1690 (total examples = 13520)	Loss: 0.006571
Step #1700 (total examples = 13600)	Loss: 0.005621
Step #1710 (total examples = 13680)	Loss: 0.013320
Step #1720 (total examples = 13760)	Loss: 0.227595
Step #1730 (total examples = 13840)	Loss: 0.148302
Step #1740 (total examples = 13920)	Loss: 0.020604
Step #1750 (total examples = 14000)	Loss: 0.030020
Step #1760 (total examples = 14080)	Loss: 0.660114
Step #1770 (total examples = 14160)	Loss: 0.003559
Step #1780 (total examples = 14240)	Loss: 0.003994
Step #1790 (total examples = 14320)	Loss: 0.005780
Step #1800 (total examples = 14400)	Loss: 0.017158
Step #1810 (total examples = 14480)	Loss: 0.053211
Step #1820 (total examples = 14560)	Loss: 0.153206
Step #1830 (total examples = 14640)	Loss: 0.149481
Step #1840 (total examples = 14720)	Loss: 0.025694
Step #1850 (total examples = 14800)	Loss: 0.002637
Step #1860 (total examples = 14880)	Loss: 0.184963
Step #1870 (total examples = 14960)	Loss: 0.000634
Step #1880 (total examples = 15040)	Loss: 0.020735
Step #1890 (total examples = 15120)	Loss: 0.002909
Step #1900 (total examples = 15200)	Loss: 0.007159
Step #1910 (total examples = 15280)	Loss: 0.101291
Step #1920 (total examples = 15360)	Loss: 0.008610
Step #1930 (total examples = 15440)	Loss: 0.071842
Step #1940 (total examples = 15520)	Loss: 0.422901
Step #1950 (total examples = 15600)	Loss: 0.401774
Step #1960 (total examples = 15680)	Loss: 0.047827
Step #1970 (total examples = 15760)	Loss: 0.013470
Step #1980 (total examples = 15840)	Loss: 0.109379
Step #1990 (total examples = 15920)	Loss: 0.002920

Step #1640 (total examples = 13120)	Loss: 0.377887
Step #1650 (total examples = 13200)	Loss: 0.053201
Step #1660 (total examples = 13280)	Loss: 0.003404
Step #1670 (total examples = 13360)	Loss: 0.007627
Step #1680 (total examples = 13440)	Loss: 0.001412
Step #1690 (total examples = 13520)	Loss: 0.372995
Step #1700 (total examples = 13600)	Loss: 0.008349
Step #1710 (total examples = 13680)	Loss: 0.107744
Step #1720 (total examples = 13760)	Loss: 0.001785
Step #1730 (total examples = 13840)	Loss: 0.238210
Step #1740 (total examples = 13920)	Loss: 0.026584
Step #1750 (total examples = 14000)	Loss: 0.402308
Step #1760 (total examples = 14080)	Loss: 0.000813
Step #1770 (total examples = 14160)	Loss: 0.138050
Step #1780 (total examples = 14240)	Loss: 0.053233
Step #1790 (total examples = 14320)	Loss: 0.010668
Step #1800 (total examples = 14400)	Loss: 0.030631
Step #1810 (total examples = 14480)	Loss: 0.025511
Step #1820 (total examples = 14560)	Loss: 0.012266
Step #1830 (total examples = 14640)	Loss: 0.079872
Step #1840 (total examples = 14720)	Loss: 0.003029
Step #1850 (total examples = 14800)	Loss: 0.034107
Step #1860 (total examples = 14880)	Loss: 0.011371
Step #1870 (total examples = 14960)	Loss: 0.027335
Step #1880 (total examples = 15040)	Loss: 0.001896
Step #1890 (total examples = 15120)	Loss: 0.006794
Step #1900 (total examples = 15200)	Loss: 0.086982
Step #1910 (total examples = 15280)	Loss: 0.029199
Step #1920 (total examples = 15360)	Loss: 0.030380
Step #1930 (total examples = 15440)	Loss: 0.002971
Step #1940 (total examples = 15520)	Loss: 0.002479
Step #1950 (total examples = 15600)	Loss: 0.001675
Step #1960 (total examples = 15680)	Loss: 0.142152
Step #1970 (total examples = 15760)	Loss: 0.002322
Step #1980 (total examples = 15840)	Loss: 0.023548
Step #1990 (total examples = 15920)	Loss: 0.006916
  1/313 [..............................] - ETA: 2:19 - loss: 3.0952 - accuracy: 0.9375  3/313 [..............................] - ETA: 11s - loss: 2.0112 - accuracy: 0.9688   5/313 [..............................] - ETA: 11s - loss: 1.2970 - accuracy: 0.9750  7/313 [..............................] - ETA: 12s - loss: 0.9264 - accuracy: 0.9821  8/313 [..............................] - ETA: 13s - loss: 0.8106 - accuracy: 0.9844  9/313 [..............................] - ETA: 14s - loss: 0.7205 - accuracy: 0.9861 10/313 [..............................] - ETA: 15s - loss: 0.6485 - accuracy: 0.9875 11/313 [>.............................] - ETA: 16s - loss: 7.8438 - accuracy: 0.9830 12/313 [>.............................] - ETA: 16s - loss: 7.2332 - accuracy: 0.9818 13/313 [>.............................] - ETA: 16s - loss: 6.8205 - accuracy: 0.9808 14/313 [>.............................] - ETA: 17s - loss: 7.7839 - accuracy: 0.9799 15/313 [>.............................] - ETA: 17s - loss: 8.6483 - accuracy: 0.9792 16/313 [>.............................] - ETA: 17s - loss: 8.4172 - accuracy: 0.9785 17/313 [>.............................] - ETA: 17s - loss: 7.9220 - accuracy: 0.9798 18/313 [>.............................] - ETA: 17s - loss: 7.4819 - accuracy: 0.9809 19/313 [>.............................] - ETA: 18s - loss: 7.0881 - accuracy: 0.9819 20/313 [>.............................] - ETA: 18s - loss: 10.3466 - accuracy: 0.9797 22/313 [=>............................] - ETA: 17s - loss: 10.1473 - accuracy: 0.9787 24/313 [=>............................] - ETA: 16s - loss: 13.8962 - accuracy: 0.9753 26/313 [=>............................] - ETA: 16s - loss: 13.4574 - accuracy: 0.9760 28/313 [=>............................] - ETA: 15s - loss: 15.1020 - accuracy: 0.9732 30/313 [=>............................] - ETA: 15s - loss: 16.6164 - accuracy: 0.9740 32/313 [==>...........................] - ETA: 14s - loss: 16.8750 - accuracy: 0.9736 34/313 [==>...........................] - ETA: 14s - loss: 16.2101 - accuracy: 0.9743 36/313 [==>...........................] - ETA: 14s - loss: 16.9368 - accuracy: 0.9740 38/313 [==>...........................] - ETA: 13s - loss: 17.5605 - accuracy: 0.9729 40/313 [==>...........................] - ETA: 13s - loss: 17.6048 - accuracy: 0.9719 42/313 [===>..........................] - ETA: 13s - loss: 17.6415 - accuracy: 0.9717 44/313 [===>..........................] - ETA: 12s - loss: 16.8396 - accuracy: 0.9730 46/313 [===>..........................] - ETA: 12s - loss: 16.1075 - accuracy: 0.9742 48/313 [===>..........................] - ETA: 12s - loss: 16.1584 - accuracy: 0.9727 50/313 [===>..........................] - ETA: 12s - loss: 16.0466 - accuracy: 0.9719 52/313 [===>..........................] - ETA: 12s - loss: 16.0268 - accuracy: 0.9706 54/313 [====>.........................] - ETA: 11s - loss: 16.2498 - accuracy: 0.9699 56/313 [====>.........................] - ETA: 11s - loss: 16.0049 - accuracy: 0.9704 58/313 [====>.........................] - ETA: 11s - loss: 15.4530 - accuracy: 0.9714 60/313 [====>.........................] - ETA: 11s - loss: 15.6264 - accuracy: 0.9714 62/313 [====>.........................] - ETA: 11s - loss: 15.1920 - accuracy: 0.9718 64/313 [=====>........................] - ETA: 11s - loss: 16.4018 - accuracy: 0.9707 66/313 [=====>........................] - ETA: 10s - loss: 17.5950 - accuracy: 0.9692 68/313 [=====>........................] - ETA: 10s - loss: 18.6888 - accuracy: 0.9683 70/313 [=====>........................] - ETA: 10s - loss: 19.4704 - accuracy: 0.9670 72/313 [=====>........................] - ETA: 10s - loss: 19.8192 - accuracy: 0.9666 74/313 [======>.......................] - ETA: 10s - loss: 19.2836 - accuracy: 0.9675 76/313 [======>.......................] - ETA: 10s - loss: 18.9759 - accuracy: 0.9675 78/313 [======>.......................] - ETA: 10s - loss: 19.6980 - accuracy: 0.9655 80/313 [======>.......................] - ETA: 9s - loss: 19.5625 - accuracy: 0.9660  82/313 [======>.......................] - ETA: 9s - loss: 19.4133 - accuracy: 0.9661 84/313 [=======>......................] - ETA: 9s - loss: 19.6311 - accuracy: 0.9658 86/313 [=======>......................] - ETA: 9s - loss: 19.7140 - accuracy: 0.9655 88/313 [=======>......................] - ETA: 9s - loss: 19.3182 - accuracy: 0.9659 90/313 [=======>......................] - ETA: 9s - loss: 19.4360 - accuracy: 0.9660 92/313 [=======>......................] - ETA: 9s - loss: 19.1923 - accuracy: 0.9657 94/313 [========>.....................] - ETA: 9s - loss: 19.2822 - accuracy: 0.9658 96/313 [========>.....................] - ETA: 9s - loss: 18.8805 - accuracy: 0.9665 98/313 [========>.....................] - ETA: 8s - loss: 18.5187 - accuracy: 0.9665100/313 [========>.....................] - ETA: 8s - loss: 18.1483 - accuracy: 0.9672102/313 [========>.....................] - ETA: 8s - loss: 17.8567 - accuracy: 0.9675104/313 [========>.....................] - ETA: 8s - loss: 17.5212 - accuracy: 0.9678106/313 [=========>....................] - ETA: 8s - loss: 17.4704 - accuracy: 0.9679108/313 [=========>....................] - ETA: 8s - loss: 17.2657 - accuracy: 0.9682110/313 [=========>....................] - ETA: 8s - loss: 17.0853 - accuracy: 0.9682112/313 [=========>....................] - ETA: 8s - loss: 17.4753 - accuracy: 0.9679114/313 [=========>....................] - ETA: 8s - loss: 17.1687 - accuracy: 0.9685116/313 [==========>...................] - ETA: 8s - loss: 16.9626 - accuracy: 0.9688118/313 [==========>...................] - ETA: 7s - loss: 17.3381 - accuracy: 0.9680120/313 [==========>...................] - ETA: 7s - loss: 18.5081 - accuracy: 0.9669122/313 [==========>...................] - ETA: 7s - loss: 18.4976 - accuracy: 0.9672124/313 [==========>...................] - ETA: 7s - loss: 19.0854 - accuracy: 0.9670126/313 [===========>..................] - ETA: 7s - loss: 19.1400 - accuracy: 0.9665128/313 [===========>..................] - ETA: 7s - loss: 19.1547 - accuracy: 0.9666130/313 [===========>..................] - ETA: 7s - loss: 18.8624 - accuracy: 0.9668132/313 [===========>..................] - ETA: 7s - loss: 18.8767 - accuracy: 0.9666134/313 [===========>..................] - ETA: 7s - loss: 18.9864 - accuracy: 0.9667136/313 [============>.................] - ETA: 7s - loss: 19.1143 - accuracy: 0.9662138/313 [============>.................] - ETA: 7s - loss: 19.0027 - accuracy: 0.9663140/313 [============>.................] - ETA: 6s - loss: 18.8370 - accuracy: 0.9665142/313 [============>.................] - ETA: 6s - loss: 18.7284 - accuracy: 0.9665144/313 [============>.................] - ETA: 6s - loss: 18.5912 - accuracy: 0.9666146/313 [============>.................] - ETA: 6s - loss: 18.3365 - accuracy: 0.9670148/313 [=============>................] - ETA: 6s - loss: 18.0887 - accuracy: 0.9675150/313 [=============>................] - ETA: 6s - loss: 18.6023 - accuracy: 0.9669152/313 [=============>................] - ETA: 6s - loss: 18.7143 - accuracy: 0.9669154/313 [=============>................] - ETA: 6s - loss: 18.4712 - accuracy: 0.9673156/313 [=============>................] - ETA: 6s - loss: 18.3266 - accuracy: 0.9673158/313 [==============>...............] - ETA: 6s - loss: 18.0946 - accuracy: 0.9678160/313 [==============>...............] - ETA: 6s - loss: 17.8988 - accuracy: 0.9680162/313 [==============>...............] - ETA: 5s - loss: 17.7171 - accuracy: 0.9680164/313 [==============>...............] - ETA: 5s - loss: 17.5010 - accuracy: 0.9684166/313 [==============>...............] - ETA: 5s - loss: 17.2901 - accuracy: 0.9688168/313 [===============>..............] - ETA: 5s - loss: 17.0843 - accuracy: 0.9691170/313 [===============>..............] - ETA: 5s - loss: 16.8833 - accuracy: 0.9695172/313 [===============>..............] - ETA: 5s - loss: 16.6870 - accuracy: 0.9698174/313 [===============>..............] - ETA: 5s - loss: 16.4952 - accuracy: 0.9702176/313 [===============>..............] - ETA: 5s - loss: 16.3877 - accuracy: 0.9703178/313 [================>.............] - ETA: 5s - loss: 16.3936 - accuracy: 0.9702180/313 [================>.............] - ETA: 5s - loss: 16.2858 - accuracy: 0.9703182/313 [================>.............] - ETA: 5s - loss: 16.1451 - accuracy: 0.9705184/313 [================>.............] - ETA: 5s - loss: 15.9696 - accuracy: 0.9708186/313 [================>.............] - ETA: 4s - loss: 15.8751 - accuracy: 0.9709188/313 [=================>............] - ETA: 4s - loss: 16.7896 - accuracy: 0.9701190/313 [=================>............] - ETA: 4s - loss: 17.0357 - accuracy: 0.9699192/313 [=================>............] - ETA: 4s - loss: 17.0347 - accuracy: 0.9699194/313 [=================>............] - ETA: 4s - loss: 16.9221 - accuracy: 0.9700196/313 [=================>............] - ETA: 4s - loss: 16.7494 - accuracy: 0.9703198/313 [=================>............] - ETA: 4s - loss: 16.5802 - accuracy: 0.9706200/313 [==================>...........] - ETA: 4s - loss: 16.4144 - accuracy: 0.9709202/313 [==================>...........] - ETA: 4s - loss: 16.2519 - accuracy: 0.9712204/313 [==================>...........] - ETA: 4s - loss: 16.2972 - accuracy: 0.9712206/313 [==================>...........] - ETA: 4s - loss: 16.1390 - accuracy: 0.9715208/313 [==================>...........] - ETA: 4s - loss: 16.4057 - accuracy: 0.9713210/313 [===================>..........] - ETA: 3s - loss: 16.2495 - accuracy: 0.9716212/313 [===================>..........] - ETA: 3s - loss: 16.0962 - accuracy: 0.9718214/313 [===================>..........] - ETA: 3s - loss: 15.9457 - accuracy: 0.9721216/313 [===================>..........] - ETA: 3s - loss: 15.7981 - accuracy: 0.9724218/313 [===================>..........] - ETA: 3s - loss: 15.6531 - accuracy: 0.9726220/313 [====================>.........] - ETA: 3s - loss: 15.5108 - accuracy: 0.9729222/313 [====================>.........] - ETA: 3s - loss: 15.3711 - accuracy: 0.9731224/313 [====================>.........] - ETA: 3s - loss: 15.2339 - accuracy: 0.9734226/313 [====================>.........] - ETA: 3s - loss: 15.1141 - accuracy: 0.9735228/313 [====================>.........] - ETA: 3s - loss: 14.9816 - accuracy: 0.9737230/313 [=====================>........] - ETA: 3s - loss: 14.8513 - accuracy: 0.9739232/313 [=====================>........] - ETA: 3s - loss: 14.7233 - accuracy: 0.9741234/313 [=====================>........] - ETA: 3s - loss: 14.7260 - accuracy: 0.9742236/313 [=====================>........] - ETA: 2s - loss: 14.6012 - accuracy: 0.9744238/313 [=====================>........] - ETA: 2s - loss: 14.4785 - accuracy: 0.9747240/313 [======================>.......] - ETA: 2s - loss: 14.3579 - accuracy: 0.9749242/313 [======================>.......] - ETA: 2s - loss: 14.2392 - accuracy: 0.9751244/313 [======================>.......] - ETA: 2s - loss: 14.1757 - accuracy: 0.9752246/313 [======================>.......] - ETA: 2s - loss: 14.2304 - accuracy: 0.9750248/313 [======================>.......] - ETA: 2s - loss: 14.2651 - accuracy: 0.9747250/313 [======================>.......] - ETA: 2s - loss: 14.1904 - accuracy: 0.9747252/313 [=======================>......] - ETA: 2s - loss: 14.1869 - accuracy: 0.9748254/313 [=======================>......] - ETA: 2s - loss: 14.3661 - accuracy: 0.9747256/313 [=======================>......] - ETA: 2s - loss: 14.2538 - accuracy: 0.9749258/313 [=======================>......] - ETA: 2s - loss: 14.1639 - accuracy: 0.9749260/313 [=======================>......] - ETA: 2s - loss: 14.0549 - accuracy: 0.9751262/313 [========================>.....] - ETA: 1s - loss: 13.9477 - accuracy: 0.9753264/313 [========================>.....] - ETA: 1s - loss: 13.8420 - accuracy: 0.9755266/313 [========================>.....] - ETA: 1s - loss: 13.8428 - accuracy: 0.9754268/313 [========================>.....] - ETA: 1s - loss: 13.7835 - accuracy: 0.9754270/313 [========================>.....] - ETA: 1s - loss: 13.8003 - accuracy: 0.9755272/313 [=========================>....] - ETA: 1s - loss: 13.6989 - accuracy: 0.9756274/313 [=========================>....] - ETA: 1s - loss: 13.5989 - accuracy: 0.9758276/313 [=========================>....] - ETA: 1s - loss: 13.5003 - accuracy: 0.9760278/313 [=========================>....] - ETA: 1s - loss: 13.4032 - accuracy: 0.9762280/313 [=========================>....] - ETA: 1s - loss: 13.3075 - accuracy: 0.9763282/313 [==========================>...] - ETA: 1s - loss: 13.3986 - accuracy: 0.9762284/313 [==========================>...] - ETA: 1s - loss: 13.4237 - accuracy: 0.9762286/313 [==========================>...] - ETA: 1s - loss: 13.3299 - accuracy: 0.9764288/313 [==========================>...] - ETA: 0s - loss: 13.2373 - accuracy: 0.9766290/313 [==========================>...] - ETA: 0s - loss: 13.1460 - accuracy: 0.9767292/313 [==========================>...] - ETA: 0s - loss: 13.0560 - accuracy: 0.9769294/313 [===========================>..] - ETA: 0s - loss: 12.9672 - accuracy: 0.9770296/313 [===========================>..] - ETA: 0s - loss: 12.8831 - accuracy: 0.9771298/313 [===========================>..] - ETA: 0s - loss: 12.9112 - accuracy: 0.9770300/313 [===========================>..] - ETA: 0s - loss: 12.9246 - accuracy: 0.9767302/313 [===========================>..] - ETA: 0s - loss: 12.9025 - accuracy: 0.9767304/313 [============================>.] - ETA: 0s - loss: 12.9571 - accuracy: 0.9768306/313 [============================>.] - ETA: 0s - loss: 13.5017 - accuracy: 0.9765308/313 [============================>.] - ETA: 0s - loss: 13.6811 - accuracy: 0.9762310/313 [============================>.] - ETA: 0s - loss: 13.7691 - accuracy: 0.9757312/313 [============================>.] - ETA: 0s - loss: 13.8109 - accuracy: 0.9756313/313 [==============================] - 12s 38ms/step - loss: 13.7888 - accuracy: 0.9756
Final model accuracy: 0.975600
Total training time: 157.181481
srun: Job 30899852 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Step created for job 30899852
2021-12-03 20:23:52.367719: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 20:23:52.367810: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-12-03 20:23:52.402292: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 20:23:52.402508: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-12-03 20:23:56.498799: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 20:23:56.498900: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2021-12-03 20:23:56.498952: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (node-0005): /proc/driver/nvidia/version does not exist
2021-12-03 20:23:56.574461: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/lang/Python/3.7.2-intel-2018.5.274/lib:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib64:/opt/apps/software/lib/libffi/3.2.1-GCCcore-6.3.0/lib:/opt/apps/software/math/GMP/6.1.2-GCCcore-6.3.0/lib:/opt/apps/software/tools/XZ/5.2.4-GCCcore-6.3.0/lib:/opt/apps/software/devel/SQLite/3.27.2-GCCcore-6.3.0/lib:/opt/apps/software/lang/Tcl/8.6.9-GCCcore-6.3.0/lib:/opt/apps/software/devel/ncurses/6.1-intel-2018.5.274/lib:/opt/apps/software/lib/libreadline/7.0-GCCcore-6.3.0/lib:/opt/apps/software/lib/zlib/1.2.11-GCCcore-6.3.0/lib:/opt/apps/software/tools/bzip2/1.0.6/lib:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/mkl/lib/intel64:/opt/apps/software/numlib/imkl/2018.4.274-iimpi-2018.4.274/lib/intel64:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/lib64:/opt/apps/software/lib/libfabric/1.9.1/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64:/opt/apps/software/compiler/ifort/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/debugger_2018/libipt/intel64/lib:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4:/opt/apps/software/compiler/icc/2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64:/opt/apps/software/tools/binutils/2.26-GCCcore-6.3.0/lib:/opt/apps/software/compiler/GCCcore/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0:/opt/apps/software/compiler/GCCcore/6.3.0/lib64:/opt/apps/software/compiler/GCCcore/6.3.0/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib:/opt/apps/software/mpi/impi/2018.4.274-iccifort-2018.5.274-GCC-6.3.0-2.26/compilers_and_libraries_2018.5.274/linux/mpi/mic/lib
2021-12-03 20:23:56.594064: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2021-12-03 20:23:56.594172: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (node-0001): /proc/driver/nvidia/version does not exist
===========
rank: 0
Batch size: 8
Total number of batches: 4000
Total training examples: 32000
Num of workers: 2
===========
Step #0 (total examples = 0)	Loss: 2.324334
Step #10 (total examples = 80)	Loss: 1.981234
Step #20 (total examples = 160)	Loss: 0.934990
Step #30 (total examples = 240)	Loss: 0.478292
Step #40 (total examples = 320)	Loss: 1.142076
Step #50 (total examples = 400)	Loss: 0.826525
Step #60 (total examples = 480)	Loss: 0.226861
Step #70 (total examples = 560)	Loss: 0.061539
Step #80 (total examples = 640)	Loss: 0.415300
Step #90 (total examples = 720)	Loss: 0.760280
Step #100 (total examples = 800)	Loss: 0.557250
Step #110 (total examples = 880)	Loss: 0.523831
Step #120 (total examples = 960)	Loss: 0.285017
Step #130 (total examples = 1040)	Loss: 0.044892
Step #140 (total examples = 1120)	Loss: 1.067125
Step #150 (total examples = 1200)	Loss: 0.219778
Step #160 (total examples = 1280)	Loss: 0.171516
Step #170 (total examples = 1360)	Loss: 0.087176
Step #180 (total examples = 1440)	Loss: 0.414006
Step #190 (total examples = 1520)	Loss: 0.202945
Step #200 (total examples = 1600)	Loss: 0.030295
Step #210 (total examples = 1680)	Loss: 0.438456
Step #220 (total examples = 1760)	Loss: 0.696418
Step #230 (total examples = 1840)	Loss: 0.497806
Step #240 (total examples = 1920)	Loss: 0.010068
Step #250 (total examples = 2000)	Loss: 0.037744
Step #260 (total examples = 2080)	Loss: 0.577059
Step #270 (total examples = 2160)	Loss: 0.164678
Step #280 (total examples = 2240)	Loss: 0.338375
Step #290 (total examples = 2320)	Loss: 0.211698
Step #300 (total examples = 2400)	Loss: 0.221754
Step #310 (total examples = 2480)	Loss: 0.024293
Step #320 (total examples = 2560)	Loss: 0.074592
Step #330 (total examples = 2640)	Loss: 0.048779
Step #340 (total examples = 2720)	Loss: 0.186652
Step #350 (total examples = 2800)	Loss: 0.435496
Step #360 (total examples = 2880)	Loss: 0.274694
Step #370 (total examples = 2960)	Loss: 0.276581
Step #380 (total examples = 3040)	Loss: 0.076469
Step #390 (total examples = 3120)	Loss: 0.825918
Step #400 (total examples = 3200)	Loss: 0.050691
Step #410 (total examples = 3280)	Loss: 0.098393
Step #420 (total examples = 3360)	Loss: 0.011343
Step #430 (total examples = 3440)	Loss: 0.019137
Step #440 (total examples = 3520)	Loss: 0.086763
Step #450 (total examples = 3600)	Loss: 0.007411
Step #460 (total examples = 3680)	Loss: 0.013610
Step #470 (total examples = 3760)	Loss: 0.111028
Step #480 (total examples = 3840)	Loss: 0.204736
Step #490 (total examples = 3920)	Loss: 0.476729
Step #500 (total examples = 4000)	Loss: 0.023887
Step #510 (total examples = 4080)	Loss: 0.028449
Step #520 (total examples = 4160)	Loss: 0.436986
Step #530 (total examples = 4240)	Loss: 0.029254
Step #540 (total examples = 4320)	Loss: 0.028633
Step #550 (total examples = 4400)	Loss: 0.024310
Step #560 (total examples = 4480)	Loss: 0.086588
Step #570 (total examples = 4560)	Loss: 0.298466
Step #580 (total examples = 4640)	Loss: 0.006834
Step #590 (total examples = 4720)	Loss: 0.094041
Step #600 (total examples = 4800)	Loss: 0.050658
Step #610 (total examples = 4880)	Loss: 0.016065
Step #620 (total examples = 4960)	Loss: 0.146339
Step #630 (total examples = 5040)	Loss: 0.015821
Step #640 (total examples = 5120)	Loss: 0.102452
Step #650 (total examples = 5200)	Loss: 0.028541
Step #660 (total examples = 5280)	Loss: 0.154039
Step #670 (total examples = 5360)	Loss: 0.193174
Step #680 (total examples = 5440)	Loss: 0.017286
Step #690 (total examples = 5520)	Loss: 0.002090
Step #700 (total examples = 5600)	Loss: 0.290424
Step #710 (total examples = 5680)	Loss: 0.004494
Step #720 (total examples = 5760)	Loss: 0.014022
Step #730 (total examples = 5840)	Loss: 0.086352
Step #740 (total examples = 5920)	Loss: 0.188974
Step #750 (total examples = 6000)	Loss: 0.242527
Step #760 (total examples = 6080)	Loss: 0.018484
Step #770 (total examples = 6160)	Loss: 0.114467
Step #780 (total examples = 6240)	Loss: 0.059609
Step #790 (total examples = 6320)	Loss: 0.097490
Step #800 (total examples = 6400)	Loss: 0.089299
Step #810 (total examples = 6480)	Loss: 0.231623
Step #820 (total examples = 6560)	Loss: 0.009618
Step #830 (total examples = 6640)	Loss: 0.033064
Step #840 (total examples = 6720)	Loss: 0.003048
Step #850 (total examples = 6800)	Loss: 0.000147
Step #860 (total examples = 6880)	Loss: 0.661182
Step #870 (total examples = 6960)	Loss: 0.002103
Step #880 (total examples = 7040)	Loss: 0.207723
Step #890 (total examples = 7120)	Loss: 0.413806
Step #900 (total examples = 7200)	Loss: 0.001674
Step #910 (total examples = 7280)	Loss: 0.006571
Step #920 (total examples = 7360)	Loss: 0.010588
Step #930 (total examples = 7440)	Loss: 0.268960
Step #940 (total examples = 7520)	Loss: 0.311111
Step #950 (total examples = 7600)	Loss: 0.021509
Step #960 (total examples = 7680)	Loss: 0.012350
Step #970 (total examples = 7760)	Loss: 1.612082
Step #980 (total examples = 7840)	Loss: 0.013363
Step #990 (total examples = 7920)	Loss: 0.012296
Step #1000 (total examples = 8000)	Loss: 0.002994
Step #1010 (total examples = 8080)	Loss: 0.017237
Step #1020 (total examples = 8160)	Loss: 0.020558
Step #1030 (total examples = 8240)	Loss: 0.008476
Step #1040 (total examples = 8320)	Loss: 0.012629
Step #1050 (total examples = 8400)	Loss: 0.008388
Step #1060 (total examples = 8480)	Loss: 0.026590
Step #1070 (total examples = 8560)	Loss: 0.003852
Step #1080 (total examples = 8640)	Loss: 0.701673
Step #1090 (total examples = 8720)	Loss: 0.171315
Step #1100 (total examples = 8800)	Loss: 0.027518
Step #1110 (total examples = 8880)	Loss: 0.006219
Step #1120 (total examples = 8960)	Loss: 0.007703
Step #1130 (total examples = 9040)	Loss: 0.025899
Step #1140 (total examples = 9120)	Loss: 0.292668
Step #1150 (total examples = 9200)	Loss: 0.014592
Step #1160 (total examples = 9280)	Loss: 0.405476
Step #1170 (total examples = 9360)	Loss: 0.024112
Step #1180 (total examples = 9440)	Loss: 0.049236
Step #1190 (total examples = 9520)	Loss: 0.007443
Step #1200 (total examples = 9600)	Loss: 0.007020
Step #1210 (total examples = 9680)	Loss: 0.012582
Step #1220 (total examples = 9760)	Loss: 0.044152
Step #1230 (total examples = 9840)	Loss: 0.047393
Step #1240 (total examples = 9920)	Loss: 0.020717
Step #1250 (total examples = 10000)	Loss: 0.081767
Step #1260 (total examples = 10080)	Loss: 0.007784
Step #1270 (total examples = 10160)	Loss: 0.089640
Step #1280 (total examples = 10240)	Loss: 0.014630
Step #1290 (total examples = 10320)	Loss: 0.078524
Step #1300 (total examples = 10400)	Loss: 0.171368
Step #1310 (total examples = 10480)	Loss: 0.002942
Step #1320 (total examples = 10560)	Loss: 0.311357
Step #1330 (total examples = 10640)	Loss: 0.263835
Step #1340 (total examples = 10720)	Loss: 0.036513
Step #1350 (total examples = 10800)	Loss: 0.119125
Step #1360 (total examples = 10880)	Loss: 0.048306
Step #1370 (total examples = 10960)	Loss: 0.010586
Step #1380 (total examples = 11040)	Loss: 0.036348
Step #1390 (total examples = 11120)	Loss: 0.088840
Step #1400 (total examples = 11200)	Loss: 0.005814
Step #1410 (total examples = 11280)	Loss: 0.081811
Step #1420 (total examples = 11360)	Loss: 0.263705
Step #1430 (total examples = 11440)	Loss: 0.020179
Step #1440 (total examples = 11520)	Loss: 0.029142
Step #1450 (total examples = 11600)	Loss: 0.227206
Step #1460 (total examples = 11680)	Loss: 1.253342
Step #1470 (total examples = 11760)	Loss: 0.064999
Step #1480 (total examples = 11840)	Loss: 0.130373
Step #1490 (total examples = 11920)	Loss: 0.260313
Step #1500 (total examples = 12000)	Loss: 0.248731
Step #1510 (total examples = 12080)	Loss: 0.010058
Step #1520 (total examples = 12160)	Loss: 0.442241
Step #1530 (total examples = 12240)	Loss: 0.124785
Step #1540 (total examples = 12320)	Loss: 0.074041
Step #1550 (total examples = 12400)	Loss: 0.023543
Step #1560 (total examples = 12480)	Loss: 0.005032
Step #1570 (total examples = 12560)	Loss: 0.181650
Step #1580 (total examples = 12640)	Loss: 0.028999
Step #1590 (total examples = 12720)	Loss: 0.007074
Step #1600 (total examples = 12800)	Loss: 0.000958
Step #1610 (total examples = 12880)	Loss: 0.181430
Step #1620 (total examples = 12960)	Loss: 0.043266
Step #1630 (total examples = 13040)	Loss: 0.108362===========
rank: 1
===========
Step #0 (total examples = 0)	Loss: 2.281121
Step #10 (total examples = 80)	Loss: 2.036259
Step #20 (total examples = 160)	Loss: 1.195760
Step #30 (total examples = 240)	Loss: 0.546931
Step #40 (total examples = 320)	Loss: 0.729879
Step #50 (total examples = 400)	Loss: 0.345939
Step #60 (total examples = 480)	Loss: 0.598496
Step #70 (total examples = 560)	Loss: 0.277959
Step #80 (total examples = 640)	Loss: 0.936953
Step #90 (total examples = 720)	Loss: 0.184488
Step #100 (total examples = 800)	Loss: 0.227359
Step #110 (total examples = 880)	Loss: 0.597669
Step #120 (total examples = 960)	Loss: 0.553093
Step #130 (total examples = 1040)	Loss: 0.090920
Step #140 (total examples = 1120)	Loss: 0.400457
Step #150 (total examples = 1200)	Loss: 0.072171
Step #160 (total examples = 1280)	Loss: 0.141117
Step #170 (total examples = 1360)	Loss: 0.231840
Step #180 (total examples = 1440)	Loss: 0.099007
Step #190 (total examples = 1520)	Loss: 0.127063
Step #200 (total examples = 1600)	Loss: 0.197682
Step #210 (total examples = 1680)	Loss: 0.166775
Step #220 (total examples = 1760)	Loss: 0.159549
Step #230 (total examples = 1840)	Loss: 0.306164
Step #240 (total examples = 1920)	Loss: 0.025449
Step #250 (total examples = 2000)	Loss: 0.118794
Step #260 (total examples = 2080)	Loss: 0.565174
Step #270 (total examples = 2160)	Loss: 0.241330
Step #280 (total examples = 2240)	Loss: 0.627359
Step #290 (total examples = 2320)	Loss: 0.349378
Step #300 (total examples = 2400)	Loss: 0.140539
Step #310 (total examples = 2480)	Loss: 0.056632
Step #320 (total examples = 2560)	Loss: 0.043587
Step #330 (total examples = 2640)	Loss: 0.080357
Step #340 (total examples = 2720)	Loss: 0.058205
Step #350 (total examples = 2800)	Loss: 0.281771
Step #360 (total examples = 2880)	Loss: 0.215097
Step #370 (total examples = 2960)	Loss: 0.085158
Step #380 (total examples = 3040)	Loss: 0.007764
Step #390 (total examples = 3120)	Loss: 0.005919
Step #400 (total examples = 3200)	Loss: 0.408077
Step #410 (total examples = 3280)	Loss: 0.200740
Step #420 (total examples = 3360)	Loss: 0.079916
Step #430 (total examples = 3440)	Loss: 0.064087
Step #440 (total examples = 3520)	Loss: 0.075972
Step #450 (total examples = 3600)	Loss: 0.011027
Step #460 (total examples = 3680)	Loss: 0.034381
Step #470 (total examples = 3760)	Loss: 0.232535
Step #480 (total examples = 3840)	Loss: 0.101277
Step #490 (total examples = 3920)	Loss: 0.004399
Step #500 (total examples = 4000)	Loss: 0.637379
Step #510 (total examples = 4080)	Loss: 0.000541
Step #520 (total examples = 4160)	Loss: 0.505579
Step #530 (total examples = 4240)	Loss: 0.479975
Step #540 (total examples = 4320)	Loss: 0.152455
Step #550 (total examples = 4400)	Loss: 0.003345
Step #560 (total examples = 4480)	Loss: 0.194101
Step #570 (total examples = 4560)	Loss: 0.072288
Step #580 (total examples = 4640)	Loss: 0.035303
Step #590 (total examples = 4720)	Loss: 0.046734
Step #600 (total examples = 4800)	Loss: 0.056645
Step #610 (total examples = 4880)	Loss: 0.045207
Step #620 (total examples = 4960)	Loss: 0.004254
Step #630 (total examples = 5040)	Loss: 0.586273
Step #640 (total examples = 5120)	Loss: 0.414456
Step #650 (total examples = 5200)	Loss: 0.068331
Step #660 (total examples = 5280)	Loss: 0.011229
Step #670 (total examples = 5360)	Loss: 0.082964
Step #680 (total examples = 5440)	Loss: 0.023912
Step #690 (total examples = 5520)	Loss: 0.043732
Step #700 (total examples = 5600)	Loss: 0.075439
Step #710 (total examples = 5680)	Loss: 0.013219
Step #720 (total examples = 5760)	Loss: 0.007650
Step #730 (total examples = 5840)	Loss: 0.058470
Step #740 (total examples = 5920)	Loss: 0.101809
Step #750 (total examples = 6000)	Loss: 0.391929
Step #760 (total examples = 6080)	Loss: 0.030170
Step #770 (total examples = 6160)	Loss: 0.271181
Step #780 (total examples = 6240)	Loss: 0.624198
Step #790 (total examples = 6320)	Loss: 0.017372
Step #800 (total examples = 6400)	Loss: 0.527198
Step #810 (total examples = 6480)	Loss: 0.047416
Step #820 (total examples = 6560)	Loss: 0.060072
Step #830 (total examples = 6640)	Loss: 0.033121
Step #840 (total examples = 6720)	Loss: 0.013677
Step #850 (total examples = 6800)	Loss: 0.001995
Step #860 (total examples = 6880)	Loss: 0.032439
Step #870 (total examples = 6960)	Loss: 0.237005
Step #880 (total examples = 7040)	Loss: 0.214327
Step #890 (total examples = 7120)	Loss: 0.019820
Step #900 (total examples = 7200)	Loss: 0.069575
Step #910 (total examples = 7280)	Loss: 0.015326
Step #920 (total examples = 7360)	Loss: 0.736190
Step #930 (total examples = 7440)	Loss: 0.108462
Step #940 (total examples = 7520)	Loss: 0.017386
Step #950 (total examples = 7600)	Loss: 0.021210
Step #960 (total examples = 7680)	Loss: 0.277516
Step #970 (total examples = 7760)	Loss: 0.120387
Step #980 (total examples = 7840)	Loss: 0.143500
Step #990 (total examples = 7920)	Loss: 0.098012
Step #1000 (total examples = 8000)	Loss: 0.058261
Step #1010 (total examples = 8080)	Loss: 0.003455
Step #1020 (total examples = 8160)	Loss: 0.003945
Step #1030 (total examples = 8240)	Loss: 0.010603
Step #1040 (total examples = 8320)	Loss: 0.025501
Step #1050 (total examples = 8400)	Loss: 0.325471
Step #1060 (total examples = 8480)	Loss: 0.093234
Step #1070 (total examples = 8560)	Loss: 0.079489
Step #1080 (total examples = 8640)	Loss: 0.218158
Step #1090 (total examples = 8720)	Loss: 0.083517
Step #1100 (total examples = 8800)	Loss: 0.161757
Step #1110 (total examples = 8880)	Loss: 0.031860
Step #1120 (total examples = 8960)	Loss: 0.001071
Step #1130 (total examples = 9040)	Loss: 0.032425
Step #1140 (total examples = 9120)	Loss: 0.085311
Step #1150 (total examples = 9200)	Loss: 0.019204
Step #1160 (total examples = 9280)	Loss: 0.076915
Step #1170 (total examples = 9360)	Loss: 0.388207
Step #1180 (total examples = 9440)	Loss: 0.184907
Step #1190 (total examples = 9520)	Loss: 0.019249
Step #1200 (total examples = 9600)	Loss: 0.077321
Step #1210 (total examples = 9680)	Loss: 0.133377
Step #1220 (total examples = 9760)	Loss: 0.013805
Step #1230 (total examples = 9840)	Loss: 0.418740
Step #1240 (total examples = 9920)	Loss: 0.462246
Step #1250 (total examples = 10000)	Loss: 0.000441
Step #1260 (total examples = 10080)	Loss: 0.693976
Step #1270 (total examples = 10160)	Loss: 0.480454
Step #1280 (total examples = 10240)	Loss: 0.030505
Step #1290 (total examples = 10320)	Loss: 0.001330
Step #1300 (total examples = 10400)	Loss: 0.001132
Step #1310 (total examples = 10480)	Loss: 0.118662
Step #1320 (total examples = 10560)	Loss: 0.726813
Step #1330 (total examples = 10640)	Loss: 0.009544
Step #1340 (total examples = 10720)	Loss: 0.071315
Step #1350 (total examples = 10800)	Loss: 0.013032
Step #1360 (total examples = 10880)	Loss: 0.104854
Step #1370 (total examples = 10960)	Loss: 0.007267
Step #1380 (total examples = 11040)	Loss: 0.070783
Step #1390 (total examples = 11120)	Loss: 0.018553
Step #1400 (total examples = 11200)	Loss: 0.011611
Step #1410 (total examples = 11280)	Loss: 0.000904
Step #1420 (total examples = 11360)	Loss: 0.004690
Step #1430 (total examples = 11440)	Loss: 0.046054
Step #1440 (total examples = 11520)	Loss: 0.004055
Step #1450 (total examples = 11600)	Loss: 0.001410
Step #1460 (total examples = 11680)	Loss: 0.409672
Step #1470 (total examples = 11760)	Loss: 0.037034
Step #1480 (total examples = 11840)	Loss: 0.068625
Step #1490 (total examples = 11920)	Loss: 0.040086
Step #1500 (total examples = 12000)	Loss: 0.116277
Step #1510 (total examples = 12080)	Loss: 0.021034
Step #1520 (total examples = 12160)	Loss: 0.172889
Step #1530 (total examples = 12240)	Loss: 0.029991
Step #1540 (total examples = 12320)	Loss: 1.044193
Step #1550 (total examples = 12400)	Loss: 0.238773
Step #1560 (total examples = 12480)	Loss: 0.003705
Step #1570 (total examples = 12560)	Loss: 0.117839
Step #1580 (total examples = 12640)	Loss: 0.164878
Step #1590 (total examples = 12720)	Loss: 0.007424
Step #1600 (total examples = 12800)	Loss: 0.002731
Step #1610 (total examples = 12880)	Loss: 0.041515
Step #1620 (total examples = 12960)	Loss: 0.103237
Step #1630 (total examples = 13040)	Loss: 0.088304
Step #1640 (total examples = 13120)	Loss: 0.043069
Step #1650 (total examples = 13200)	Loss: 0.029110
Step #1660 (total examples = 13280)	Loss: 0.091951
Step #1670 (total examples = 13360)	Loss: 0.025312
Step #1680 (total examples = 13440)	Loss: 0.204727
Step #1690 (total examples = 13520)	Loss: 0.014455
Step #1700 (total examples = 13600)	Loss: 0.045818
Step #1710 (total examples = 13680)	Loss: 0.006823
Step #1720 (total examples = 13760)	Loss: 0.034754
Step #1730 (total examples = 13840)	Loss: 0.199629
Step #1740 (total examples = 13920)	Loss: 0.007256
Step #1750 (total examples = 14000)	Loss: 0.114000
Step #1760 (total examples = 14080)	Loss: 0.037324
Step #1770 (total examples = 14160)	Loss: 0.001815
Step #1780 (total examples = 14240)	Loss: 0.006195
Step #1790 (total examples = 14320)	Loss: 0.072749
Step #1800 (total examples = 14400)	Loss: 0.001763
Step #1810 (total examples = 14480)	Loss: 0.146776
Step #1820 (total examples = 14560)	Loss: 0.112236
Step #1830 (total examples = 14640)	Loss: 0.056632
Step #1840 (total examples = 14720)	Loss: 0.005525
Step #1850 (total examples = 14800)	Loss: 0.129956
Step #1860 (total examples = 14880)	Loss: 0.018278
Step #1870 (total examples = 14960)	Loss: 0.021948
Step #1880 (total examples = 15040)	Loss: 0.071903
Step #1890 (total examples = 15120)	Loss: 0.005390
Step #1900 (total examples = 15200)	Loss: 0.000694
Step #1910 (total examples = 15280)	Loss: 0.058742
Step #1920 (total examples = 15360)	Loss: 0.006175
Step #1930 (total examples = 15440)	Loss: 0.462476
Step #1940 (total examples = 15520)	Loss: 0.366937
Step #1950 (total examples = 15600)	Loss: 0.000320
Step #1960 (total examples = 15680)	Loss: 0.002993
Step #1970 (total examples = 15760)	Loss: 0.002190
Step #1980 (total examples = 15840)	Loss: 0.067249
Step #1990 (total examples = 15920)	Loss: 0.008414

Step #1640 (total examples = 13120)	Loss: 0.524266
Step #1650 (total examples = 13200)	Loss: 0.007593
Step #1660 (total examples = 13280)	Loss: 0.313753
Step #1670 (total examples = 13360)	Loss: 0.004554
Step #1680 (total examples = 13440)	Loss: 0.017562
Step #1690 (total examples = 13520)	Loss: 0.005227
Step #1700 (total examples = 13600)	Loss: 0.077756
Step #1710 (total examples = 13680)	Loss: 0.147576
Step #1720 (total examples = 13760)	Loss: 0.065308
Step #1730 (total examples = 13840)	Loss: 0.299402
Step #1740 (total examples = 13920)	Loss: 0.002790
Step #1750 (total examples = 14000)	Loss: 0.190954
Step #1760 (total examples = 14080)	Loss: 0.002274
Step #1770 (total examples = 14160)	Loss: 0.017385
Step #1780 (total examples = 14240)	Loss: 0.059113
Step #1790 (total examples = 14320)	Loss: 0.001506
Step #1800 (total examples = 14400)	Loss: 0.246400
Step #1810 (total examples = 14480)	Loss: 0.254421
Step #1820 (total examples = 14560)	Loss: 0.012591
Step #1830 (total examples = 14640)	Loss: 0.495440
Step #1840 (total examples = 14720)	Loss: 0.007839
Step #1850 (total examples = 14800)	Loss: 0.025485
Step #1860 (total examples = 14880)	Loss: 0.001071
Step #1870 (total examples = 14960)	Loss: 0.042425
Step #1880 (total examples = 15040)	Loss: 0.009955
Step #1890 (total examples = 15120)	Loss: 0.001384
Step #1900 (total examples = 15200)	Loss: 0.005915
Step #1910 (total examples = 15280)	Loss: 0.032215
Step #1920 (total examples = 15360)	Loss: 0.159826
Step #1930 (total examples = 15440)	Loss: 0.001729
Step #1940 (total examples = 15520)	Loss: 0.003014
Step #1950 (total examples = 15600)	Loss: 0.002632
Step #1960 (total examples = 15680)	Loss: 0.163891
Step #1970 (total examples = 15760)	Loss: 0.011747
Step #1980 (total examples = 15840)	Loss: 0.010875
Step #1990 (total examples = 15920)	Loss: 0.006535
  1/313 [..............................] - ETA: 2:22 - loss: 1.9659 - accuracy: 0.9688  3/313 [..............................] - ETA: 11s - loss: 22.6751 - accuracy: 0.9375  5/313 [..............................] - ETA: 11s - loss: 18.7696 - accuracy: 0.9500  6/313 [..............................] - ETA: 12s - loss: 15.6413 - accuracy: 0.9583  7/313 [..............................] - ETA: 13s - loss: 13.9560 - accuracy: 0.9554  8/313 [..............................] - ETA: 15s - loss: 15.4714 - accuracy: 0.9531  9/313 [..............................] - ETA: 15s - loss: 17.3950 - accuracy: 0.9549 10/313 [..............................] - ETA: 16s - loss: 15.6555 - accuracy: 0.9594 11/313 [>.............................] - ETA: 17s - loss: 25.3615 - accuracy: 0.9517 12/313 [>.............................] - ETA: 17s - loss: 24.7067 - accuracy: 0.9531 13/313 [>.............................] - ETA: 17s - loss: 22.8062 - accuracy: 0.9567 14/313 [>.............................] - ETA: 17s - loss: 22.9469 - accuracy: 0.9576 15/313 [>.............................] - ETA: 18s - loss: 26.1707 - accuracy: 0.9563 16/313 [>.............................] - ETA: 18s - loss: 24.5350 - accuracy: 0.9590 17/313 [>.............................] - ETA: 18s - loss: 23.0918 - accuracy: 0.9614 18/313 [>.............................] - ETA: 18s - loss: 22.1767 - accuracy: 0.9618 19/313 [>.............................] - ETA: 18s - loss: 21.2586 - accuracy: 0.9605 20/313 [>.............................] - ETA: 18s - loss: 21.0224 - accuracy: 0.9609 22/313 [=>............................] - ETA: 17s - loss: 20.7273 - accuracy: 0.9588 24/313 [=>............................] - ETA: 16s - loss: 21.0194 - accuracy: 0.9570 26/313 [=>............................] - ETA: 16s - loss: 20.3302 - accuracy: 0.9579 28/313 [=>............................] - ETA: 15s - loss: 19.0889 - accuracy: 0.9587 30/313 [=>............................] - ETA: 15s - loss: 19.0829 - accuracy: 0.9604 32/313 [==>...........................] - ETA: 14s - loss: 19.9342 - accuracy: 0.9590 34/313 [==>...........................] - ETA: 14s - loss: 19.1621 - accuracy: 0.9586 36/313 [==>...........................] - ETA: 14s - loss: 20.1146 - accuracy: 0.9592 38/313 [==>...........................] - ETA: 13s - loss: 21.7146 - accuracy: 0.9564 40/313 [==>...........................] - ETA: 13s - loss: 24.5097 - accuracy: 0.9531 42/313 [===>..........................] - ETA: 13s - loss: 24.2840 - accuracy: 0.9524 44/313 [===>..........................] - ETA: 13s - loss: 23.4574 - accuracy: 0.9531 46/313 [===>..........................] - ETA: 12s - loss: 22.6282 - accuracy: 0.9545 48/313 [===>..........................] - ETA: 12s - loss: 21.9290 - accuracy: 0.9551 50/313 [===>..........................] - ETA: 12s - loss: 22.7426 - accuracy: 0.9550 52/313 [===>..........................] - ETA: 12s - loss: 22.7968 - accuracy: 0.9549 54/313 [====>.........................] - ETA: 12s - loss: 23.2700 - accuracy: 0.9549 56/313 [====>.........................] - ETA: 11s - loss: 23.8240 - accuracy: 0.9542 58/313 [====>.........................] - ETA: 11s - loss: 23.0209 - accuracy: 0.9553 60/313 [====>.........................] - ETA: 11s - loss: 23.1210 - accuracy: 0.9557 62/313 [====>.........................] - ETA: 11s - loss: 22.5724 - accuracy: 0.9567 64/313 [=====>........................] - ETA: 11s - loss: 23.7667 - accuracy: 0.9556 66/313 [=====>........................] - ETA: 11s - loss: 24.7042 - accuracy: 0.9545 68/313 [=====>........................] - ETA: 10s - loss: 26.5224 - accuracy: 0.9540 70/313 [=====>........................] - ETA: 10s - loss: 27.5894 - accuracy: 0.9531 72/313 [=====>........................] - ETA: 10s - loss: 27.2031 - accuracy: 0.9536 74/313 [======>.......................] - ETA: 10s - loss: 26.6648 - accuracy: 0.9544 76/313 [======>.......................] - ETA: 10s - loss: 27.0383 - accuracy: 0.9539 78/313 [======>.......................] - ETA: 10s - loss: 26.8497 - accuracy: 0.9539 80/313 [======>.......................] - ETA: 10s - loss: 26.2716 - accuracy: 0.9539 82/313 [======>.......................] - ETA: 10s - loss: 25.9293 - accuracy: 0.9543 84/313 [=======>......................] - ETA: 9s - loss: 25.9932 - accuracy: 0.9546  86/313 [=======>......................] - ETA: 9s - loss: 25.7033 - accuracy: 0.9546 88/313 [=======>......................] - ETA: 9s - loss: 25.4686 - accuracy: 0.9545 90/313 [=======>......................] - ETA: 9s - loss: 25.3075 - accuracy: 0.9549 92/313 [=======>......................] - ETA: 9s - loss: 25.5898 - accuracy: 0.9548 94/313 [========>.....................] - ETA: 9s - loss: 26.1664 - accuracy: 0.9541 96/313 [========>.....................] - ETA: 9s - loss: 25.7512 - accuracy: 0.9548 98/313 [========>.....................] - ETA: 9s - loss: 25.5553 - accuracy: 0.9550100/313 [========>.....................] - ETA: 8s - loss: 25.0442 - accuracy: 0.9559102/313 [========>.....................] - ETA: 8s - loss: 24.6526 - accuracy: 0.9565104/313 [========>.....................] - ETA: 8s - loss: 24.6862 - accuracy: 0.9567106/313 [=========>....................] - ETA: 8s - loss: 24.8702 - accuracy: 0.9564108/313 [=========>....................] - ETA: 8s - loss: 24.4307 - accuracy: 0.9569110/313 [=========>....................] - ETA: 8s - loss: 24.5333 - accuracy: 0.9568112/313 [=========>....................] - ETA: 8s - loss: 24.2975 - accuracy: 0.9573114/313 [=========>....................] - ETA: 8s - loss: 23.9020 - accuracy: 0.9578116/313 [==========>...................] - ETA: 8s - loss: 23.4899 - accuracy: 0.9585118/313 [==========>...................] - ETA: 8s - loss: 23.9341 - accuracy: 0.9582120/313 [==========>...................] - ETA: 7s - loss: 25.1519 - accuracy: 0.9573122/313 [==========>...................] - ETA: 7s - loss: 25.2469 - accuracy: 0.9567124/313 [==========>...................] - ETA: 7s - loss: 25.5198 - accuracy: 0.9564126/313 [===========>..................] - ETA: 7s - loss: 25.4506 - accuracy: 0.9561128/313 [===========>..................] - ETA: 7s - loss: 25.3330 - accuracy: 0.9558130/313 [===========>..................] - ETA: 7s - loss: 24.9511 - accuracy: 0.9563132/313 [===========>..................] - ETA: 7s - loss: 25.1307 - accuracy: 0.9560134/313 [===========>..................] - ETA: 7s - loss: 25.3667 - accuracy: 0.9557136/313 [============>.................] - ETA: 7s - loss: 25.1479 - accuracy: 0.9557138/313 [============>.................] - ETA: 7s - loss: 24.8632 - accuracy: 0.9561140/313 [============>.................] - ETA: 7s - loss: 24.5736 - accuracy: 0.9560142/313 [============>.................] - ETA: 6s - loss: 24.4613 - accuracy: 0.9560144/313 [============>.................] - ETA: 6s - loss: 24.3047 - accuracy: 0.9562146/313 [============>.................] - ETA: 6s - loss: 23.9717 - accuracy: 0.9568148/313 [=============>................] - ETA: 6s - loss: 23.6478 - accuracy: 0.9573150/313 [=============>................] - ETA: 6s - loss: 24.2963 - accuracy: 0.9573152/313 [=============>................] - ETA: 6s - loss: 24.5909 - accuracy: 0.9572154/313 [=============>................] - ETA: 6s - loss: 24.2985 - accuracy: 0.9576156/313 [=============>................] - ETA: 6s - loss: 24.2011 - accuracy: 0.9577158/313 [==============>...............] - ETA: 6s - loss: 23.8947 - accuracy: 0.9583160/313 [==============>...............] - ETA: 6s - loss: 23.6950 - accuracy: 0.9586162/313 [==============>...............] - ETA: 6s - loss: 23.4566 - accuracy: 0.9589164/313 [==============>...............] - ETA: 5s - loss: 23.1706 - accuracy: 0.9594166/313 [==============>...............] - ETA: 5s - loss: 23.1064 - accuracy: 0.9597168/313 [===============>..............] - ETA: 5s - loss: 22.9477 - accuracy: 0.9600170/313 [===============>..............] - ETA: 5s - loss: 22.6777 - accuracy: 0.9605172/313 [===============>..............] - ETA: 5s - loss: 22.4140 - accuracy: 0.9609174/313 [===============>..............] - ETA: 5s - loss: 22.1564 - accuracy: 0.9614176/313 [===============>..............] - ETA: 5s - loss: 21.9317 - accuracy: 0.9616178/313 [================>.............] - ETA: 5s - loss: 21.8006 - accuracy: 0.9619180/313 [================>.............] - ETA: 5s - loss: 21.6141 - accuracy: 0.9622182/313 [================>.............] - ETA: 5s - loss: 21.3766 - accuracy: 0.9626184/313 [================>.............] - ETA: 5s - loss: 21.1442 - accuracy: 0.9630186/313 [================>.............] - ETA: 5s - loss: 20.9907 - accuracy: 0.9632188/313 [=================>............] - ETA: 4s - loss: 21.5053 - accuracy: 0.9633190/313 [=================>............] - ETA: 4s - loss: 21.7099 - accuracy: 0.9632192/313 [=================>............] - ETA: 4s - loss: 21.9876 - accuracy: 0.9629194/313 [=================>............] - ETA: 4s - loss: 22.2075 - accuracy: 0.9625196/313 [=================>............] - ETA: 4s - loss: 21.9809 - accuracy: 0.9629198/313 [=================>............] - ETA: 4s - loss: 21.7589 - accuracy: 0.9632200/313 [==================>...........] - ETA: 4s - loss: 21.5413 - accuracy: 0.9636202/313 [==================>...........] - ETA: 4s - loss: 21.3280 - accuracy: 0.9640204/313 [==================>...........] - ETA: 4s - loss: 21.5107 - accuracy: 0.9640206/313 [==================>...........] - ETA: 4s - loss: 21.3852 - accuracy: 0.9639208/313 [==================>...........] - ETA: 4s - loss: 21.5008 - accuracy: 0.9638210/313 [===================>..........] - ETA: 4s - loss: 21.3335 - accuracy: 0.9640212/313 [===================>..........] - ETA: 3s - loss: 21.3879 - accuracy: 0.9637214/313 [===================>..........] - ETA: 3s - loss: 21.2459 - accuracy: 0.9636216/313 [===================>..........] - ETA: 3s - loss: 21.0492 - accuracy: 0.9640218/313 [===================>..........] - ETA: 3s - loss: 20.8561 - accuracy: 0.9643220/313 [====================>.........] - ETA: 3s - loss: 20.6665 - accuracy: 0.9646222/313 [====================>.........] - ETA: 3s - loss: 20.4803 - accuracy: 0.9649224/313 [====================>.........] - ETA: 3s - loss: 20.2974 - accuracy: 0.9653226/313 [====================>.........] - ETA: 3s - loss: 20.1178 - accuracy: 0.9656228/313 [====================>.........] - ETA: 3s - loss: 19.9413 - accuracy: 0.9659230/313 [=====================>........] - ETA: 3s - loss: 19.7679 - accuracy: 0.9662232/313 [=====================>........] - ETA: 3s - loss: 19.5975 - accuracy: 0.9665234/313 [=====================>........] - ETA: 3s - loss: 19.6918 - accuracy: 0.9663236/313 [=====================>........] - ETA: 3s - loss: 19.5249 - accuracy: 0.9666238/313 [=====================>........] - ETA: 2s - loss: 19.3609 - accuracy: 0.9669240/313 [======================>.......] - ETA: 2s - loss: 19.2030 - accuracy: 0.9671242/313 [======================>.......] - ETA: 2s - loss: 19.2321 - accuracy: 0.9672244/313 [======================>.......] - ETA: 2s - loss: 19.3338 - accuracy: 0.9671246/313 [======================>.......] - ETA: 2s - loss: 19.7269 - accuracy: 0.9662248/313 [======================>.......] - ETA: 2s - loss: 20.0009 - accuracy: 0.9659250/313 [======================>.......] - ETA: 2s - loss: 20.0200 - accuracy: 0.9657252/313 [=======================>......] - ETA: 2s - loss: 19.9079 - accuracy: 0.9659254/313 [=======================>......] - ETA: 2s - loss: 20.2933 - accuracy: 0.9654256/313 [=======================>......] - ETA: 2s - loss: 20.1572 - accuracy: 0.9656258/313 [=======================>......] - ETA: 2s - loss: 20.0009 - accuracy: 0.9658260/313 [=======================>......] - ETA: 2s - loss: 19.9196 - accuracy: 0.9657262/313 [========================>.....] - ETA: 1s - loss: 19.8241 - accuracy: 0.9658264/313 [========================>.....] - ETA: 1s - loss: 19.6739 - accuracy: 0.9660266/313 [========================>.....] - ETA: 1s - loss: 19.5426 - accuracy: 0.9662268/313 [========================>.....] - ETA: 1s - loss: 19.3967 - accuracy: 0.9664270/313 [========================>.....] - ETA: 1s - loss: 19.3122 - accuracy: 0.9664272/313 [=========================>....] - ETA: 1s - loss: 19.1702 - accuracy: 0.9667274/313 [=========================>....] - ETA: 1s - loss: 19.0302 - accuracy: 0.9669276/313 [=========================>....] - ETA: 1s - loss: 18.8923 - accuracy: 0.9672278/313 [=========================>....] - ETA: 1s - loss: 18.7564 - accuracy: 0.9674280/313 [=========================>....] - ETA: 1s - loss: 18.6225 - accuracy: 0.9676282/313 [==========================>...] - ETA: 1s - loss: 18.7978 - accuracy: 0.9675284/313 [==========================>...] - ETA: 1s - loss: 18.7104 - accuracy: 0.9676286/313 [==========================>...] - ETA: 1s - loss: 18.5795 - accuracy: 0.9679288/313 [==========================>...] - ETA: 0s - loss: 18.4560 - accuracy: 0.9680290/313 [==========================>...] - ETA: 0s - loss: 18.3287 - accuracy: 0.9682292/313 [==========================>...] - ETA: 0s - loss: 18.2031 - accuracy: 0.9684294/313 [===========================>..] - ETA: 0s - loss: 18.1671 - accuracy: 0.9685296/313 [===========================>..] - ETA: 0s - loss: 18.0444 - accuracy: 0.9688298/313 [===========================>..] - ETA: 0s - loss: 18.2053 - accuracy: 0.9688300/313 [===========================>..] - ETA: 0s - loss: 18.1081 - accuracy: 0.9686302/313 [===========================>..] - ETA: 0s - loss: 18.0399 - accuracy: 0.9686304/313 [============================>.] - ETA: 0s - loss: 17.9875 - accuracy: 0.9688306/313 [============================>.] - ETA: 0s - loss: 18.6149 - accuracy: 0.9684308/313 [============================>.] - ETA: 0s - loss: 18.6163 - accuracy: 0.9682310/313 [============================>.] - ETA: 0s - loss: 18.6139 - accuracy: 0.9681312/313 [============================>.] - ETA: 0s - loss: 18.7397 - accuracy: 0.9681313/313 [==============================] - 13s 39ms/step - loss: 18.7252 - accuracy: 0.9681
Final model accuracy: 0.968100
Total training time: 162.003990
